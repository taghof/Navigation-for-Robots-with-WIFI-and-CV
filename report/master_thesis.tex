%\documentclass[12pt,a4paper,twoside,openright,fleqn]{memoir}
%\documentclass[10pt]{article}
\documentclass[12pt]{report}
%\documentclass[12pt,twoside,openright]{report}
%\documentclass[10pt,twoside]{report}
 
\usepackage[english, danish]{babel}
\usepackage[ansinew]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{float}
% \floatstyle{boxed} 
% \restylefloat{figure}
\usepackage{listings}

\usepackage{natbib}
\usepackage{titleref}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tocbibind}

%\usepackage{caption}
%\usepackage{subcaption}
\usepackage[hyphens]{url}

%\newcounter{description}
%\renewcommand{\descriptionlabel}[1]{%
%  \refstepcounter{description}%
%  \hspace\labelsep
%  \normalfont\bfseries \thedescription. #1:}

%Define a new 'leo' style for the package that will use a smaller font.
\makeatletter
\def\url@leostyle{
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}

\usepackage{hyperref}

\newenvironment{framefig}
   {\begin{figure}[H]\begin{framed}}
   {\end{framed}\end{figure}}

\newcommand{\uri}[1]{ #1 }
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textbf{todo:}(#1)}
\newcommand{\dronen}[1]{#1}
\newcommand{\drone}[0]{AR.Drone} 
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\begin{document}
\begin{titlepage}
\begin{center}


% Upper part of the page
\includegraphics[width=0.50\textwidth]{./grafik/segl.jpg}\\[1cm]    

\textsc{\LARGE Aarhus Universitet}\\[0.2cm]
\textsc{Datalogisk Institut}\\[1.0cm]
\textsc{\Large Master's Thesis}\\[1.0cm]


% Title
\HRule \\[0.4cm]
{ \huge \bfseries Semi-autonom indendørs navigation for luftbåren robot}\\[0.4cm]

\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.5\textwidth}
\begin{flushleft} \large
Morten Daugaard, 20051715
Thomas Thyregod, 20051688
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Vejleder:} \\
Ole Caprani
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large \today}

\end{center}

\end{titlepage}
\pagestyle{headings}
\begin{otherlanguage}{english}
\abstract{
This thesis investigates the possibility of making a quadrokopter,
AR.Drone, fly semi autonomously in an indoor environment. Firstly an
easy to use software platform that utilizes the AR.Drone sensing
apparatus is contructed. The platform is then used for further
experiments. These experiments explores how the sensor apparatus
aboard the AR.Drone can be exploited to derive more abstract
information about the near environment. This information is used to
make the AR.Drone navigate through respectively corridors and open
spaces with visual markers on the floor.
}
\end{otherlanguage}
\abstract{
%Indeholder og gennemgår i det følgende (på dansk).
%-Kort intro hvad vil vi gøre...
Dette speciale handler om at klarlægge mulighederne, for at gøre en
quadrokopter, AR.Drone, delvis selvflyvende i et indendørsmiljø. Mere
specifikt implementeres i første omgang en let anvendelig
softwareplatform, der kan udnytte AR.Dronens indbyggede
sensorapparat. Platformen anvendes herefter til de videre
eksperimenter. Det undersøges hvorledes sensorapparatet ombord på
AR.Dronen kan udnyttes, til at udlede mere abstrakt information om
miljøet i den umiddelbare nærhed. Denne information anvendes til at
lade AR.Dronen navigere i henholdsvis korridorer og åbne rum med
visuelle markører på gulvet. 
\todo{Kort uddybning af, hvad har vi gjort, hvad var resultatet.}
}

\tableofcontents

\pagebreak

\chapter{Introduktion}  %Thomas
\label{sec:introduktion}
AR.Drone,~\cite{electronic:ardrone}, er en quadrokopter,~\cite{electronic:wiki_quadrotor}, der er udviklet af
Parrot,~\cite{electronic:parrot}. AR.Dronen blev i 2010 introduceret som ``the~Flying~Video~Game'' og
promoveres stadig af producenten som et stykke legetøj, der fjernstyres fra
brugerens iPhone eller iPad. Siden har AR.Dronen dog fundet stor
anvendelse, ikke blot som et avanceret stykke legetøj. Forskere,~\cite{single_img_pespective_cues},
studerende~\cite{electronic:ludep}, og hobbyister har taget AR.Dronen
til %den fjernstyrede quadrokopter til
sig som en platform og selv udviklet videre ovenpå den.%cite

\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/start3.png}
  \caption{\emph{the Flying Video Game} med to spillere med hver deres AR.Drone.}
  \label{fig:devisen}
\end{framefig}

% hobby selvbyg (ombygge fysisk LED http://www.youtube.com/user/dikale
%/GPS/WIFI) /
%flashe ny styreprogram ombord 
%Sousveillance vs. surveillance, noget multimedieæstetik, occupy wallstreet movement thing
%http://www.alternet.org/occupywallst/153542/ows\_fights\_back\_against\_police\_surveillance\_by\_launching\_\%22occucopter\%22\_citizen\_drone/ 
% også cyborgen Steve Mann

En stor del af forskningen i quadrokoptere har hidtil gået på at
optimere mekanikken,~\cite{quteprints33767}, stabiliseringen og
motorstyringen. I mange eksperimenter har quadrokopteren været
fjernstyret gennem ekstern observation. Eksemplevis i forskningen af
små agile Mikro Arial Vehicle (MAV) i flok,~\cite{turpin:swarm}
(publiseret 2012) er der
fastmonteret kameraer i lokalet rundt om MAV til ekstern udregning og feedback
af MAV absolut position.
En anden tilgang og en som ligner dette speciales tilgang er at alle
beslutninger omkring navigation skal træffes udelukkende på baggrund
af sensorinput fra sensorerne på quadrokopteren. Eksempler på
denne tilgang kan findes i brugen af quadrokoptere i inspektionen af
højspændingsledninger og broer,~\cite{uavbridgeinspection2007}, og i
eksperimenterne af Chen et al omkring navigation af AR.Dronen i
forskellige indendørs miljøer,~\cite{single_img_pespective_cues}.
Ideen om at sensorinput og navigation skal komme fra quadrokopteren selv
ses stadig så sent som 2011 hos Shen et al. i deres forsøg med
``Simultaneous localization and mapping''
(SLAM),~\cite{shen2011kinect}, hvor quadrokopteren er påmonteret en 
kinect,~\cite{electronic:kinect} til selv at opfatte omgivelserne i 3
dimensioner og derigennem selv orientere sig og konstruere modeller af
omgivelserne.

\section{Motivation} % Thomas & Morten, Hvorfor
\label{sec:motivation}
At specialet skulle beskæftige sig med AR.Dronen var en beslutning der
blev truffet helt fra projektstarten. Argumentet for valget er at vi
ønskede at lære mere om og få mere erfaring med modeller af den
fysiske verden og brug af modeller i beregninger på AR.Dronen. Vi
havde en forventning og håb om at den fysiske verden ville stritte
imod vores modelrepræsentationer af verdenen. 

Specialet er i løbet af projektet vokset ud af, og omkring
AR.Dronen, gennem et ønske fra vores side om at vide hvad den
fjernstyrede quadrokopter ville være istand til som robotplatform.

Igennem vores forundersøgelser af AR.Dronen er vi blevet overrasket
over hvor stor interessen er og hvor stor aktivitet der er i diverse
brugergrupper på internettet omkring quadrokoptere. %DIYDrones etc
Udfra denne erfaring mener vi at konstruktionen af en softwareplatform
udover at skulle støtte vores eget videre arbejde i specialet også bør
have et fokus på at lave et let tilgængeligt interface for almindelige
brugere, som ikke nødvendigvis er programmører. Et perspektiv kunne
være at AR.dronen og den konstruerede platform vil kunne bruges til at
lære om programmering (fordi det er sjovt).

Hvis platformen laves modulopbygget, vil man kunne genbruge
komponenter og derigennem evt kunne effektivisere udviklingen af
dedikerede styringer til nye arbejdsopgaver for AR.Dronen.

Langt størstedelen af arbejdet i projektet handler om at klarlægge og
forstå AR.Dronen og dens muligheder. Vi vil gerne forstå
konsekvenserne af de forskellige designvalg vi har mulighed for at
træffe, inden en eventuel senere reel ibrugtagning og produktudvikling.

%+Hvilke bestanddele skal en platform have, hvilke muligheder,
%arkitektur/opbygning.
%<- hvad skal der til?
\pagebreak
\section{Teseformulering} % Thomas & Morten, Hvad
\label{sec:tese}
%\todo{skal de 3 linier væk?}
%Et blødt mål i specialet er at undersøge hvilke bestandele en platform
%skal indeholde, hvad kan AR.Dronen suppleres med og hvilken struktur
%bør softwaren have. 
%HVAD er det præcis vi vil vise eller afvise?!
%Målet med dette speciale er at 

Specialet har fire mål som formuleres og uddybes således: 
\begin{enumerate}
\item \textbf{Konstruktion af klientplatform:} \label{tese1} Der skal konstrueres en softwareplatform til at
  lette fjernkontrol af AR.Dronen fra PC. Den konstruerede platform skal:
  \begin{enumerate}
  \item \textbf{Facilitere eksperimenter:} \label{tese1a} Understøtte de
    videre eksperimenter i specialet.
  %Være let anvendelig:
  \item \textbf{Tilgængeliggøre et interface til AR.Dronen:} \label{tese3} Konstrueres så den
    tilgængeliggør et let anvendeligt interface til AR.Dronen.
  \end{enumerate}
\item \textbf{Semiautonom navigation:} \label{tese2} 
 Det skal undersøges hvorvidt en quadrokopter, der er designet % 2
  til kontrol via fjernstyring, kan navigere semi-autonomt 
  i et indendørs miljø.
\item \textbf{Udledning af yderligere navigationsdata:} \label{tese4}
  Mulighederne for at bruge AR.Dronens ombordværende sensorsystem
  (udvidede sensorsystem) til at udlede højere ordens viden om
  navigationsmiljøet skal undersøges, samt hvorledes den ny viden kan bruges som støtte for den
  semi-autonome styring af AR.Dronen. Herunder undersøges konkret
  mulighederne for at bruge:
  \begin{enumerate}
  \item \label{tese4a} Lokalisering via WIFI-signalstyrker.
  \item \label{tese4b} Lokalisering via visuelle markøre.
  \item \label{tese4c} Detektering af navigationsmiljøtype.
  \item \label{tese4d} Odometrisk afstandsmåling.
  \item \label{tese4e} Optisk triangulering af afstande.
  \end{enumerate}
\item \textbf{Vurdering af AR.Dronen som robotplatform:} \label{tese5} Som en del
  af specialets metodeevaluering skal det undersøges om AR.Dronen
  faktisk er anvendelig som robotplatform.
\end{enumerate}

%label til tese4: \ref{tese4}, og til tese4c: \ref{tese4c}.

%% \begin{enumerate}
%% \item \label{tese1} Der skal konstruere en softwareplatform til at lette de videre % 1
%%   eksperiementer i specialet.
%% %(, 3) mediere kontrollen med AR.Dronen, 
%% \item \label{tese2} Det skal undersøges hvorvidt en quadrokopter der er designet % 2
%%   til kontrol via fjernstyring kan navigere semi-autonomt 
%%   i et indendørs miljø.
%% \end{enumerate}
%% Derudover skal der undersøges om
%% \begin{enumerate}
%% \item \label{tese3} Platformen kan konstrueres til at tilgængeliggøre et let % 3
%%   anvendelig, medierende interface til AR.Dronen.
%% \item \label{tese4} Mulighederne for at bruge AR.Dronens ombordværende % 4
%%   sensorsystem (udvidede sensorsystem) til at udlede højere ordens
%%   viden om navigationsmiljø, og om den ny viden kan
%%   bruges som støtte i den autonome styring af quadrokopteren. Herunder
%%   skal det konkret undersøges mulighederne for at bruge:
%%   \begin{enumerate}
%%   \item \label{tese4a} Lokalisering via WIFI
%%   \item \label{tese4b} Lokalisering via visuelle markøre
%%   \item \label{tese4c} Detektering af navigationsmiljøtype
%%   \item \label{tese4d} Odometrisk afstandsmåling
%%   \item \label{tese4e} Optisk triangulering af afstande
%%   \end{enumerate}
%% \item \label{tese5} AR.Dronen faktisk er anvendelighed som robotplatform. %5
%% \end{enumerate}

%Dette speciale anvender AR.Dronen til at opbygge en
%robotplatform omkring for at undersøge hvilke krav der nødvendigvis
%skal opfyldes for at konstruere en modulopbygget kontrolplatform til
%semiautonom indendørs fjernstyring af en quadrokopter til afvikling på
%en PC.
\pagebreak
\section{Metode} % Thomas,
\label{sec:metode}
%\paragraph{Domæne afgrænsning}
AR.Dronen siges at kunne flyve både indendørs og udendørs. Specialet
er afgrænset til at behandle navigation indendørs. For at 
skabe en god basis at arbejde videre udfra, defineres afgrænsningen
yderligere gennem følgende brugsscenarie. 
%
Der er fundet inspiration i indendørsområder som de beskrevet i
arbejdet med den selvkørende museumsrobotguide Minerva fra 1999,
\cite{Thrun99minerva:a}. 

\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/minerva_article_3fig.png}
  \caption{Minerva i hel figur, med ansigtstræk igang med at lave guidede tours på Smithsonian museum.}
  \label{fig:devisen}
\end{framefig}

Minerva er udstyret med kamera og afstandsmåler til at se publikum og
til at navigere i omgivelserne og i forhold til puplikum. 
%
Minerva handler også om menneske-maskin interaktion som kan være svært
at finde løsninger på, 
%
men i spørgsmålet om at finde løsninger til naviagtionsproblemet alene
findes allerede mange svar på lignende problemer i litteraturen.
%
Spørgsmålet er så hvad der sker hvis den kørende robot i et museumsmiljø
udskiftes med en flyvende AR.Drone. Med AR.Dronens tilgængelige
sensore og muligheder for manøvrering.

\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/ml_fetisism.png}
  \caption{Kan du ikke komme tæt nok på til at tage et billede, bryder du dig ikke om stor folkemængder, så brug AR.Dronen! \todo{anden caption}}
  \label{fig:devisen}
\end{framefig}

Hvilke muligheder og begrænsninger ville det give at bruge AR.Dronen
som robotplatform. Et scenarie er at lave et webinterface så gæster
kan besøge museet fra internettet. Spørgsmålet er hvad 
det vil medføre. Det er nemt at forestille sig at den meget fjerne
kontrol vil give en klar oplevelse af disembodyment. Der er også et sikkerhedsaspekt i det at lade en quadrokopter næsten
selvstyrende flyve rundt blandt mennesker. Et andet scenarie er at
lave film af en personaliseret flyvning, eller en forprogrammeret
rutesekvens og dele den med andre som et postkort eller souvenir.
%
%% Det er noget med en [semi autonom] [navigation] [indendørs] [luftbåren]
%% fra semiautonom og navigation er der noget, 1) bevægelse i forskellige miljøer, og 2) avoid opførsel, møde med silhuet eller lavt batteri.
%% at det er indendørs giver nogle begrænsninger, kan bestemme et bestemt miljø ala Hopper-gangene / Ada-gangene eller Zuse.
%% Luftbåren at vi ligepludselig har en 3. dimension at forholde os til
%
Af hensyn til at lave praktiske forsøg, bliver de konkrete
navigationsløsninger designet til at virke i IT-byens kontorer og
gangarealer samt Zuse-bygningen på Katrinebjerg i Århus.



%% - hvilke værktøjer vil vi tage i brug AR.Dronen, PC, joystick/controller, Python, => hvorfor, argumentation.
%%    tage AR.Dronen gå fra at bruge den som en fjernstyrbar enhed til at gøre den til en næsten selvstændig semi autonom enhed.

%% -Hvordan vil vi faktisk teste vores tese? - arbejde går imod at lave en testopstilling, hvad skal der til for at opstillingen virker? framework, processor-kraft båndbredde
%% Højniveau?:
%% - er der noget højt,... abstraktion?
%hvordan teori
%Domæne/navigationsmiljø % Thomas,
%Nogle bygninger på Katrinebjerg, indendørs...
%hvorfor / uddybende beskrivelse af kontor, gang, openspace
%% - hvad er miljøet, Vi begrænser anvendelses-/test-området, hvorfor argumentation er simplificering for os selv, fokusere på det relevante, hvad er det (relevante)?! - pege tilbage på tesenformuleringen sektion \ref{sec:tese}.

\subsection{AR.Dronen} % Thomas, 
AR.Dronen, som beskrives i detaljer i kapitel~\ref{sec:AR.Drone},
tilfredstiller vores behov for en quadrokopter-platform. AR.Dronen
stiller en række sensorer tilrådighed og præsenterer et veldefineret
interface til fjernstyring. Alternativt kunne man have bygget en
platform fra bunden, som i \cite{quteprints33767}.

Anvendelsen af AR.Dronen besværliggøres af den korte batteritid (10-12
minutters flyvetid). Vi har dog kunnet minimere problemet, ved at
anvende seks batterier og fire opladere. Desuden har vi, under
eksperimenter der ikke krævede flyvning, anvendt en strømforsyning fra
en PC i stedet for et batteri.

Med AR.Dronen er vi hurtigt i luften og der kan hurtigt bygges software
der anvender det offentliggjorte interface. Til gengæld har vi ingen
indflydelse på det styreprogram der kører på AR.Dronen og kan kun
styre AR.Dronen gennem interfacet. Det er muligt at implementere et
alternativt styreprogram til at AR.Dronen, men det har ikke været et
mål med dette speciale.

Der gives en detaljeret vurdering af AR.Dronen som robotplatform i
sektion~\ref{ssec:dronerecap}.

%%%% Introduction - Metode
\subsection{Klientplatform og klientværktøjer} % Thomas & Morten 
Der er implementeret en klientplatform til indsamling af AR.Dronens
sensorstrømme og fjernstyring af denne. Klientplatformen har
fasciliteret de eksperimenter som beskrives i
sektion~\ref{chap:Eksperimenter} og dermed også udviklingen af de
forskellige tasks i sektion~\ref{sec:tasks}.

Klientværktøjerne: Sensordisplay, Mapcreator og Taskcreator, som alle
beskrives i sektion~\ref{chap:clienttools}, er udviklet for at lette
arbejdet med klientplatformen. Blandt andet ved at give brugeren
adgang til en grafisk repræsentation af AR.Dronens
sensorstrømme. Klientværktøjerne har vist sig meget anvendelige i
forbindelse med forberedelse og udførelse af eksperimenter med
klientplatformen.

\subsection{Python}
Vi valgt at skrive klientplatformen i
Python. Grunden til dette er primært Pythons læsbarhed og simple
syntaks. Læsbar kode fordrer ikke bare hurtig udvikling fra vores
side, men også at andre udviklere hurtigere kan sætte sig ind i koden
og videreudvikle på den. Pythons simple syntaks betyder, at
klientplatformen også vil henvende sig til mindre erfarne
udviklere. Da det har været et mål, at skabe en let anvendelig og let
udbyggelig klientplatform, er Python et oplagt valg.
 
Eksemplerne i sektion~\ref{sec:platformsadgang} viser hvor lidt kode
der behøves, for at anvende klientplatformen. Desuden henvises til
implementationen af vores klientværktøjer,~\cite{github:sensordisplay},~\cite{github:mapcreator},~\cite{github:taskcreator},
for eksempler der anvender klientplatformen.

Udfordingen ved valget af implementationssprog er, at Python som et
dynamisk typet, fortolket sprog, ikke performance-mæssigt kan
sammenlignes med mere maskinnære sprog som C og C++. For at overkomme
denne udfordring, har vi gjort anvendelse af OpenCVs
billedalgoritmer (beskrevet nedenunder) og
Psyco,~\cite{electronic:psyco}, i forbindelse med afkodning af
billeder fra AR.Dronen (se desuden sektion~\ref{subsec:receivers}). En
fremtidig forbedring kunne være, at implementere afkodningen af
AR.Dronens billeder i et C-bibliotek og bygge en Pythonwrapper til
dette.

\subsection{OpenCV}
\label{opencv} OpenCV er et bibliotek af optimerede algoritmer,
indenfor området
computer-vision,~\cite{opencv},~\cite{opencv:wiki}. OpenCV blev
oprindeligt startet af Intel (med bidrag fra optimeringseksperter i Intel Russia og
Intels Performance Library Team), men udvikles nu af
robotforskningslaboratoriet Willow Garage,
\cite{opencv:willow:wiki}. Det har fra starten i 1999 været et mål, at
skabe ikke bare åben, men også optimeret kode, til en grundlæggende
computer vision infrastruktur.

OpenCV blev i starten udviklet i C, men den seneste udvikling er
udelukkende foregået i C++. Udover C og C++, findes der en række
interfaces til biblioteket, bland andre et Python
interface,~\cite{opencv:wiki}. Python interfacet fungerer som en
wrapper om OpenCVs C/C++ algoritmer. Ved anvendelse af dette
interface, får vi altså kombineret Pythons brugervenlighed og OpenCVs
hastighed.

OpenCVs optimerede algoritmer har, på grund af deres effektivitet,
været uundværlige, blandt andet i implementationen af vores
PositionSilhouetDetector (sektion~\ref{sec:sub:virtuel}). Forskellen i
afviklingshastighed på en blobdetector implementeret i ren Python og
en der anvender OpenCVs algoritmer, var tydelig under vores
eksperimenter i sektion~\ref{ssec:visuellokalisering}.

\subsection{Eksperimenter} 
For at teste AR.Dronen og vores
klientplatform, har vi udført en række eksperimenter. Eksperimenterne
omhandler generelt udledning af kontekstuel information fra AR.Dronens
sensorstrømme. Mere specifikt tester vi metoder til lokalisering,
afstandsbedømmelse, måling af tilbagelagt afstand (Odometri) og
korridordetektering.
\todo{ref?}

\paragraph{Lokalisering via WIFI-fingerprinting:} Ved hjælp af en
WIFI-fingerprinting algoritme, har vi undersøgt muligheden for at
lokalisere AR.Dronen i forhold til et antal fastlagte
positioner. Eksperimenternes opstilling og resultater kan ses i
sektion~\ref{ssec:wifilokalisering}.

\paragraph{Lokalisering via visuelle markører:} Vi har designet et
antal forskellige visuelle markører. Markørerne er blevet testet for
hvor godt de kunne genkendes af AR.Dronen, både i statiske
opstillinger og i forbindelse med bevægelse. Disse eksperimenter har
også fungeret som test af vores tasksystem og de udviklede
tasks. Eksperimenternes opstilling og resultater kan ses i
sektion~\ref{ssec:visuellokalisering}.

\paragraph{Afstandsbedømmelse via optical flow:} Vi har undersøgt
muligheden for at udlede afstande, til objekter synlige for AR.Dronens
frontkamera.  Ved at lade AR.Dronen flyve sidelæns, trianguleres
afstande ud til genstande foran frontkameraet. Dette gøres ved brug af
den tilbagelagte afstand og målte vinkler til genstanden.
\todo{ref}

\paragraph{Korridor detektering:} Der er eksperimenteret med muligheden
for at detektere et korridormiljø. Med en sådan detektion, kan
AR.Dronen bevæge sig i forhold til denne specifikke
miljøtype. Algoritmen analyserer på indholdet af diagonale linier i
billedet fra frontkameraet.
\todo{ref}

\paragraph{DistanceTracker:} En del af den navigationsdata vi modtager
fra AR.Dronen er hastighedsestimater. Vi har anvendt disse
hastighedsestimater til at implementere en odometrisk
afstandsmåler (DistanceTracker). Implementationen integrerer over de
modtagne hastighedsestimater. Vores eksperimenter med DistanceTracker
involverer manuelle og automatiserede flyvninger over mere eller
mindre ensartede overflader.
\todo{ref}

\section{Medfølgende CD}
På den medfølgende CD findes README.txt som forklarer hvordan de
medfølgende programmer afvikles, samt følgende materiale:
\begin{itemize}
\item /klientplatform/\\
  Kildekoden til den udviklede klientplatform inklusiv de udviklede tasks samt
  klientværktøjerne.
\item /AR.Drone addons/\\ 
  Kernemodulerne, bibliotekerne, driver-kildekoden og
  loadscriptet som er anvendt i forbindelse med udvikling på AR.Dronen.
\item /report/\\ 
  Pdf-version af denne specialerapport. 
\item /webcache/\\
  Off-line kopier af følsomme webkilder.
\item /video/\\
  Videomateriale af AR.Dronen og klientplatformen i funktion.
\end{itemize}

\section{Projekt Blog}
\label{sec:blog} Under specialets udvikling, er der anvendt en blog
til at beskrive tekniske fremgangsmåder og til at fungere som
adgangspunkt til det offentlige kode-repository. Bloggen hostes på
Github og adressen angives i~\cite{blog}. Bloggen indeholder udover en
projektbeskrivelse, især informationer om vores tidlige forsøg med
AR.Dronen. Blandt andet beskrives proceduren for at aktivere
AR.Dronens USB-port i detaljer. Et andet emne som behandles på bloggen
er afvikling af kode på AR.Dronen, herunder proceduren for
cross-compiling til ARM arkitekturen. Procedurerne er stykket sammen
fra adskillige kilder og formålet med bloggen er blandt andet at
kunne tilbyde en mere komplet kilde for andre i vores situation.

%\section{Raport struktur} % Thomas, %/læsevejledning Introduktion til
%rapportens arbejde, hvilke afsnit der gennemgåes og hvad afsnit
%indeholder.  intro til medfølgende CD og blogpost, med kode på Github

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Quadrokopter AR.Drone}
\label{sec:AR.Drone}
AR.Dronen er en quadrokopter fremstillet og udviklet af det franske
firma Parrot, \cite{electronic:parrot},
\cite{electronic:wiki_parrot}. AR står Augmented Reality,
\cite{electronic:nbc}. En quadrokopter benytter fire faste rotorer til
at generere opdrift, samt til at styre sin bevægelse i rummet.
Parrots quadrokopter kommer med et sæt sensorer bestående af: To
kameraer, et vertikalt nedadrettet og et horisontalt fremadrettet, en
nedadrettet afstandssensor, et gyroskop, samt et accelerometer. I 2012
ventes en opdateret udgave af AR.Dronen. Den nye version 
tilføjer yderligere et kamera i høj opløsning (HD), en trykmåler
(digitalt barometer) og et kompas til platformen, 
\cite{electronic:ar2}.AR.Drone er opbygget som en letvægtskonstruktion
af kulfiber, plast og skummateriale og kommer med to forskellige
skumskjold til henholdsvis indendørs
(figur~\ref{fig:indendoersskjold}) og udendørs flyvning
(figur~\ref{fig:udendoersskjold}).

I det følgende gennemgåes i afsnit \emph{\nameref{ssec:hardware}} de
fysiske enheder som quadrokopteren består af, i afsnittet
\emph{\nameref{ssec:fysik}} gives en introduktion til hvorledes
quadrokopteren bruger rotorerne til at skabe opdrift og bevægelse i
luften, i afsnittet \nameref{ssec:software} og afsnittet
\emph{\nameref{ssec:kommunikation}} beskrives kommunikationen mellem
AR.Dronens styreprogram og forskeliige typer klienter, mens afsnittet
\emph{\nameref{ssec:dronerecap}} opregner de observationer der er gjort
under specialearbejdet og som kan hjælpe andre med at træffe en
beslutning om hvorvidt AR.Dronen er den rigtige platform at bygge
videre på.

\begin{framefig}
\centering
  \subfigure[Indendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone.jpg}
    \label{fig:indendoersskjold}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Udendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone-outdoor.jpg}
    \label{fig:udendoersskjold}}
  \caption{AR.Drone med indendørs- og udendørsskjold. Figurene er fra
    AR.Drone Developer Guide, \cite{techreport:ardroneDevGuide}}
  \label{fig:ardrone}
\end{framefig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Hardware - subsection
 
\section{AR.Dronens Hardware}
\label{ssec:hardware}
Det er nærmest umuligt for et menneske at styre en helikopter ved
direkte kontrol med mindre man har trænet i mange år (hvilket er det
profesionelle piloter gør, \cite{electronic:heli}). Brugerens styring af AR.Dronen er således
ikke direkte men støttet af en feedback kontrol hvortil der
er sensorinput fra omverdenen. Sensorerne, aktuatorerne der gør det
muligt at kontrollere rotorerne, strømforsyningen og computeren
ombord beskrives herunder.

%og må siges at være en af de grundlæggende egenskaber for en robot. At der så er mulighed for at udbygge en stillingstagen iforhold til 
%
%og en processorenhed. Herunder beskrives sensorene og aktuatore samt den ombordværende computer og trømforsyningen.

\subsection{Sensorer}
\label{sssec:sensorer}
\begin{framefig}
  \subfigure[AR.Dronens frontkamera]{\includegraphics[width=0.45\textwidth]{grafik/frontcamera.jpg}
    \label{fig:frontkamera}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[AR.Dronens bundkamera]{\includegraphics[width=0.45\textwidth]{grafik/bundkamera2.png}
    \label{fig:bundkamera}}

  \caption{AR.Dronens to kameraer}
  \label{fig:kameraer}
\end{framefig}

\paragraph{Kameraer}
AR.Drone er som sagt udstyret med to kameraer: Et
bundkamera(nedadrettet) og et frontkamera(fremadrettet). Frontkameraet
har en synsvinkel på $93 ^{\circ}$ og består af en
CMOS\footnote{Complimentary-Symmetry Metal Oxide Semiconductor, på
dansk: en sensor af komplimentær metaloxid halvleder
teknologi.}-billedsensor som kan levere $640 \times 480$ pixel billeder.
AR.Dronen transmitterer dog kun med en opløsning på $320 \times 240$, pixels Quarter Video
Graphics Array (QVGA)  med en framerate på 15 FPS.
Bundkameraet er en CMOS sensor med $64 ^{\circ}$ synsvinkel,
der tager billeder i $176 \times 144$ pixels opløsning, Quarter Common
Intermediate Format (QCIF), med en framerate på 60 FPS. 
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/navboard.png}
  \caption{Over- og undersidebilleder af AR.Dronens
    navboard. Navboardet indeholder bl.a. den nedadrettede
    afstandssensor. Figurerne er fra to internetbutikker, 
    \cite{electronic:arct} og, \cite{electronic:dparts},  hvorfra der kan bestilles AR.Drone reservedele.}
  \label{fig:navboard}
\end{framefig}

\subsubsection{Afstandssensor} Til at bestemme AR.Dronens afstand til gulvet
anvendes en ultrasonisk afstandssensor. Denne opererer ved at udsende
en ultralydspuls og herefter måle tidsintervallet der går før ekkoet
måles. Ultralydsmåleren er angivet til at virke op til 6 meter og er
rent fysisk placeret på navboardet (se figur \ref{fig:navboard}) på
AR.Dronens underside.

\subsubsection{Acceleration og inerti} AR.Drone er udstyret med to
gyroskoper, et 2-akses gyroskop til at måle rotationsvinklerne
$\theta$ og $\phi$ (pitch og roll) samt et 1-akses piezoelektrisk gyroskop til at
måle rotationsvinklen $\psi$ (yaw). Gyroskoperne kombineres med et
3-akses accelerometer i en samlet enhed til at måle inerti
(IMU)\footnote{Inertial Measurement Unit}. For en grafisk
repræsentation se figur \ref{fig:drone_akser}.
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/drone_akser.png}
  \caption{AR.Dronens lokale koordinatsystem.}
  \label{fig:drone_akser}
\end{framefig}

\subsection{Aktuatorer}
\label{sssec:aktuatorer}
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/motor_gear.png}
  \caption{a) Motorreservedel, b) samlet motor og gear på kulfiberkryds og c) gear-reservedel.}
  \label{fig:motorgear}
\end{framefig}
AR.Dronens aktuatorer er de fire børsteløse 15 Watt
motorer, \cite{electronic:engines}. Motorerne
arbejder i intervallet 10350 til 41400 omdrejninger per minut(RPM) og når AR.Dronen står stille i
luften arbejder motorerne med 28000 RPM. Dette svarer, grundet gearingen,
til 3300 RPM for propellerne, \cite{electronic:engines}. Motorerne har uden sammenligning det
største strømforbrug af alle AR.Dronens enheder.

\subsubsection{Den indlejrede computer}
\label{sssec:processor}
Den centrale computer på AR.Dronen er en ARM9 468 MHz
processor (Parrot 6 ARM926EJ) med 128 MB DDR RAM til 
arbejdshukommelse og 128 MB NAND flash til persistent (vedvarende) hukommelse.

Tilkoblet den indlejrede computer er der et integreret trådløst netværkskort (Atheros AR6102G-BM2D) til WIFI. 

På undersiden af quadrokopteren er der et 7 benet molex-stik til
ekstern serialkommunikation, \cite{electronic:molex}. Porten virker
bl.a. som en On-The-Go USB-port, \cite{electronic:otg}, der primært
anvendes ved softwareopdateringer.%otg beskrives nærmere i \ref{sec:udvidelse}

\subsection{Strømforsyning}
\label{sssec:forsyning}
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/batteries.jpg}
  \caption{Batterier til AR.Dronen (Figuren er fra \cite{electronic:techpower}.}
  \label{fig:batterier}
\end{framefig}
Batteriet der driver AR.Dronens enheder er et 3 celle lithium-polymer
batteri med en open-circuit\footnote{Open-circuit spændingen er den
spænding, der er over batteriet, når det er uden belastning og ikke
oplades.} spænding på 11,1 Volt, med ladning på 1000
milli-ampere-timer(mAh) og en afladnings hastighed på 10 Coulomb(C),
\cite{electronic:ar.drone_parrot_technologies}. Opladning af batteriet
kan foregå i løbet af 90 minutter. Efter sigende skulle batteriet give
omking 12 minutters flyvetid, \cite{electronic:wiki_ar.drone}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%      - Hardware -
%%%%      + Fysik - subsection
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{AR.Dronens fysik}
\label{ssec:fysik}
AR.Dronen fåes som nævnt med to skrog eller skjold
(figur~\ref{fig:indendoersskjold}), og vejer 420 gram med det
indendørs skrog monteret. 420 gram er en relativ lille vægt i forhold
til andre quadrokoptere, \cite{quteprints33767}, som har 4-5 kg de skal
løfte. 

På en AR.Drone sidder de fire propeller i hver sit hjørne af en
kulfiberkryds. Når en propel roterer om sin akse flytter den 
luften i et areal rundt om propellen. Lufttrykket på undersiden af
propellen bliver herved større end lufttrykket på oversiden af
propellen. Hvis AR.Dronen ligger vandret vil forskellen i lufttryk
giver en Bernoullieffekt,\cite{nla.cat-vn856146}, som påvirker
AR.Dronen med en kraft modsat tyngdekraften (thrust), 
\cite{electronic:wiki_quadrotor}. Udover thrust påvirker den 
roterende bevægelse også propellens base med et moment (torque)
modsat propellens retning. Eksempelvis vil en helikopter uden haleror
dreje ukontrollerbart rundt om sig selv, men fordi AR.Dronen har et par
rotorer som drejer med uret og et andet par som drejer modsat mod
uret, \cite{techreport:ardroneDevGuide}, bliver summen af
momentpåvirkningen nul, såfremt de fire rotorer drejer med samme hastighed. 
Denne egenskab bruges også til at styre AR.Dronens flyvning og bevægelser i
rummet, som kan beskrives med tre bevægelsesakser: x, y og z, og de tilhørende
rotationsvinkler $\phi$, $\theta$  og $\psi$ (Tait-Bryan-vinklerne, \cite{wiki:euler}). På figur~\ref{fig:drone_movements} forklares hvordan
en quadrokopter i det generelle tilfælde kan bevæges omkring de tre
akser. Det bemærkes at x og y-akserne på
denne figur er forskudt 45$^{\circ}$ så de flugter med motorerne (for at
lette forklaringen), hvorimod AR.Dronens akser er forskudt så y-aksens positive retning peger i
samme retning som dennes frontkamera, se figur~\ref{fig:drone_akser}.   

AR.Dronens bevægelser styres ved at kontrollere dens vinkel i forhold
til de 3 akser, ligesom på figur~\ref{fig:drone_movements}. Skal
AR.Dronen f.eks. flyve fremad, ændres $\theta$ ved at sænke
omdrejningshastigheden på de to forreste propeller, samtidig med at
omdrejningshastigheden på de to bagerste propeller øges. 
%spørgsmålet er, hvad er moment påvirkningen af $(2*omega_H + delta_A - delta_B) - (2*omega_H)$ 
Dette resulterer i at AR.Dronen hælder lidt fremad. Samme princip gør sig
gældende når AR.Dronen skal flytte sig sideværts. Når AR.Dronen skal
bevæges om z-aksen foregår det som på figur~\ref{fig:drone_movements}d.
Det vil sige, at man enten øger eller sænker omdrejningshastigheden på
de propelpar som drejer samme vej, således at det ene pars
momentpåvirkning overstiger det andets og dermed får AR.Dronen til at
rotere. 

\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/drone_movements.png}
  \caption{(a) vinkelhastigheden $\Omega_H$ øges med $\Delta_A$ på
    alle 4 propeller, dermed øges quadrokopterens løftekraft
    (acceleration i Z-aksen), (b) Vinkelhastigheden øges på venstre og
    mindskes på højre propel, quadrokopteren laver et ``roll'' til højre
    (ændring i $\phi$), (c) samme som b, men i pitch
    (ændring i $\theta$) (d) En større vinkelhastighed på de
    med-uret-drejende propeller gør at torque bliver ulige og at
    quadrokopteren roterer(yaw) mod venstre (ændring i $\psi$). Ved bevægelser frem og tilbage udledes en positiv
    eller negativ hastighed ($V_x$), ligesom ved bevægelser sideværts
    ($V_y$). Figuren er fra ~\cite{techreport:ardroneDevGuide}.}
  \label{fig:drone_movements}
\end{framefig}

Når AR.Dronen går fra at ligge vandret til istedet at hælde, så
flyttes noget thrust fra at virke lodret imod tyngdekraften til
også at virke sideværts. Det betyder, at propellernes samlede
omdrejningshastighed skal øges for at opretholde den samme afstand til
gulvet. Ifølge SDK DevGuide,~\cite{techreport:ardroneDevGuide}, bør
$\phi$ og $\theta$ ikke overstige 0.52 rad (30$^{\circ}$)
ellers kan flyvehøjden ikke opretholdes. 

AR.Dronens styreprogram har en parameter der beskriver den øvre grænse for hvor
meget quadrokopteren maximalt må krænge. Denne parameter,
\emph{Euler\_angle\_max }, virker som en cut-offvinkel, dvs. at
AR.Dronen under flyvning løbende tester at hverken pitch ($\theta$) eller roll
($\phi$) overskrider grænsen. Sker dette lukkes systemet ned, kraften
til rotorene stoppes og AR.Dronen falder til jorden.Den maximale
hældning kan sættes til f.eks. 0.25 rad via AT-kommandoen vist i
figur~\ref{fig:ateulerangle}. 

\begin{framefig}
\begin{verbatim}
 AT*CONFIG=605,"control:euler_angle_max","0.25"
\end{verbatim}
  \caption{Eksempel på indholdet af en AT-kommandobesked. Beskeden
    overføres fra klient-enheden til AR.Dronen på UDP-port 5556. Lige
    netop denne besked sætter en parameter i AR.Dronens
    styreprogram, der bestemmer den maksimalt tilladte hældning på $\phi$
    og $\theta$.}   
  \label{fig:ateulerangle}
\end{framefig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Software - subsection

\section{AR.Dronens Software}
\label{ssec:software}
AR.Dronens computer kommer med en Linux 2.6.27 kerne
installeret. Det bemærkes at 2.6.27 er den første kerne med indbygget
support for UBIFS, \cite{electronic:wiki_ubifs}, som er det filsystem der
anvendes på AR.Dronens NAND flash-hukommelse.

\subsection{Busybox Linux platformen på AR.Dronen}
\label{sssec:busybox}
Udover Linux kernen leveres AR.Drone også med
Busybox installeret, \cite{electronic:busybox}.  Busybox implementerer
en række Unix værktøjer (\code{cd}, \code{cp}, \code{ls}, \code{mv}, osv.) 
optimeret til brug i indlejrede systemer. Busybox konfigureres så kun
de ønskede værktøjer medtages og disse pakkes til en enkelt
eksekverbar fil.  Dette betyder, at størrelsen holdes på et
minimum. Busybox på AR.Drone fylder 483 kbyte og udstyrer AR.Dronen
med de brugerkommandoer man forventer på et standard Linux
system. AR.Dronens Busybox leverer desuden en Dynamic Host Configuration
Protocol(DHCP), \cite{rfc2131}, service, som er den DHCP-server AR.Dronen
anvender til at oprette %DHCP cite?  IP\cite{rfc791}-adresser og lave
et netværk mellem en ekstern klient og AR.Dronen selv, samt en
telnet-server, \cite{rfc854}, der giver adgang til AR.Dronens terminal
og en texteditor(Vi) der gør det muligt at redigere filer direkte på
AR.Dronen. 

\subsection{AR.Dronens styreprogram fra Parrot}
\label{sssec:program.elf}
Det styreprogram der står for selve motorkontrollen, transmitteringen af data og
bearbejdning af sensordata i forbindelse med flyvning er implementeret
i \uri{/bin/program.elf}. Da der er tale om proprietært software
betragtes programmet som en lukket kasse uden hensyntagen til
implementeringen. Vi kan dog udlede forskellige elementer efter at
have observeret AR.Dronen under flyvning. Program.elf indeholder blandt andet:
\begin{itemize}
\item en PID controller, \cite{electronic:pid}, \cite{electronic:pid2}, til at stabilisere AR.Dronen når der svæves.
\item en algoritme til at udlede AR.Dronens hastighed ved hjælp af
  bundkameraet, \cite{velocity_estimation_inside_the_drone}.
\item og en algoritme til at detektere tags i billeder i forbindelse
  med augmented reality spil, se figur~\ref{fig:tags}.
\end{itemize}
AR.Dronens styreprogram modtager styringskommandoer fra brugeren gennem en User
Datagram Protocol(UDP) port, \cite{rfc768}. Kommunikationsinterfacet
mellem klientenhed og quadro\-kopteren beskrives i afsnit~\ref{ssec:kommunikation}.

\begin{framefig}
  \subfigure[Roundel]{\includegraphics[width=0.30\textwidth]{grafik/tag1.png}
    \label{fig:roundel}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Hull]{\includegraphics[width=0.30\textwidth]{grafik/tag2.png}
    \label{fig:hull}}
 ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[stripes]{\includegraphics[width=0.30\textwidth]{grafik/tag3.png}
    \label{fig:stripes}}

  \caption{Visuelle tags der kan genkendes af AR.Dronen, billeder fra
    \cite{electronic:roundel} og \cite{techreport:ardroneDevGuide}}
  \label{fig:tags}
\end{framefig}

\begin{framefig}
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/telnetscreenshot.png}
  \caption{Forbindelse til AR.Dronens Telnetserver på AR.Dronens
    standard IP addresse $192.186.1.1$.}
  \label{fig:telnetterminal}
\end{framefig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Klient/server kommunikation - subsection


\section{Kommunikation mellem AR.Dronen og klienter} %mellem AR.Dronen %og en klient-enhed
AR.Dronen fjernstyres oftest via en iPhone, men dette er blot en af
flere interaktionsmuligheder. Herunder listes nogle af de muligheder
AR.Dronens Linux-platform og AR.Dronens styreprogram åbner op for,
både i form af fjernstyring og intern kommunikation på AR.Dronen.
\label{ssec:kommunikation}
\begin{framefig}
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/ar_drone_nokia900.png}
  \caption{Fjernstyring af AR.Dronen med en Smartphone, her en
    Nokia-telefon. Figuren er oprindeligt fra \cite{electronic:nokia}.}
  \label{fig:nokiastyring}
\end{framefig}
\subsection{Ekstern klientkommunikation}
En ekstern klients (PC, smartphone eller lignende) interaktion med
AR.Dronens Linux-platform og AR.Dronens styreprogram kan kategoriseres i 2 grupper:
\begin{itemize}
  \item Fra en ekstern klientapplikation gennem FTP, \cite{rfc959},
    eller Telnet (figur~\ref{fig:telnetterminal}) over WIFI til
    services kørende i userspace på AR.Dronen. 
  \item En ekstern klientapplikation kan sende AT-kommandoer\footnote{AT(tention)-kommandoer over
      trådløst netværk (gennemgåes nærmere i sektion
      \ref{sssec:atinterface}}) via UDP protokollen til AR.Dronens
    styreprogram og kan modtage sensordata fra AR.Dronens styreprogram
    over trådløs UDP.
\end{itemize}
%
\begin{framefig}
  \centering
  \includegraphics[width=0.48\textwidth]{grafik/klient_server.png}
  \caption{Klienter kommunikerer med AR.Dronen over netværket gennem UDP og TCP.}
  \label{fig:klientserver}
\end{framefig}
%
Interaktion mellem AR.Dronen og en ekstern klient benævnes som værende
fjernstyring, mens AR.Dronen opfattes som værende autonom
hvis kontrolsignaler til AR.Dronens styreprogram kommer fra en intern
process der kører parallelt med AR.Dronens styreprogram på AR.Dronen.

\subsection{Intern klientkommunikation}
%Når interaktionen foregår mellem 2 processor
%internt på AR.Drone-platformen, vil det altid grupperes under den
%første kategori herunder og sommetider som værende i begge kategorier
%herunder: 
Interne processer på AR.Drone-platformen har mulighed for at
kommunikere med AR.Dronens styreprogram gennem UDP, på lige fod med de eksterne
klienter til fjernstyring. En intern process vil afvikles i userspace,
en nærmere beskrivelse herom gennemgåes i sektion~\ref{sec:udvidelse}.
%
\begin{framefig}
  \centering
  \includegraphics[width=0.48\textwidth]{grafik/userspace.png}
  \caption{En intern klient på AR.Dronen, kommunikerer med AR.Dronens styreprogram på 3 UDP-porte, men har også adgang til platformen som ethvert andet userspaceprogram.}
  \label{fig:userspaceklient}
\end{framefig}
%
%
\subsection{FTP og Telnet baseret kommunikation}
\label{sssec:tcp}
Fra en PC eller smartphone er det muligt at få adgang til AR.Dronens 
Linuxinstallation ved at oprette en Telnetforbindelse til AR.Dronens 
faste IP-adresse: $192.168.1.1$. Efter at forbindelsen er etableret 
får brugeren her adgang til en root-terminal (se figur~\ref{fig:telnetterminal}).

Ved at oprette en FTP-forbindelse til AR.Dronens FTP-server, fra en
FTP-klient, kan man overføre filer mellem AR.Dronen og en PC med 
FTP-protokollens PUT og GET kommandoer. Hvis der ikke angives en sti lægges 
overførte filer som standard i \uri{~/data/video} mappen på AR.Dronen.

\subsection{UDP baseret kommunikation}
\label{sssec:udp}
I modsætning til FTP- og Telnet-klienterne, som er TCP baserede, anvendes
UDP-protokollen istedet til kommunikationen imellem intern og eksterne
klienter og AR.Dronens styreprogram.

AR.Dronens styreprogram sender og modtager pakker på tre UDP-porte:
\begin{itemize}
  \item På port 5556 (command port) lytter styreprogrammet efter
    AT-kommandoer afsendt fra klienten, AT-kommandoer gennemgåes i \ref{sssec:atinterface}.
  \item på port 5555 (video port) sender AR.Dronen en videostrøm,
    indeholdende billeder fra èn af fire videokanaler til klienten.
  \item på port 5554 (navdata port) sender AR.Dronen navdatastrøm
    indeholdende statusinformation om f.eks. hastighed, eulervinkler,
    afstand til gulvet og generel information om navigationstilstanden.
\end{itemize}

\subsection{AT kommando interface}
\label{sssec:atinterface}
En klient som skal kommunikere med AR.Dronens styreprogram, gør dette ved hjælp af
såkaldte attention-kommandoer(AT-kommando). En AT-kommando er basalt set blot en
tekststreng repræsenteret som 8 bits karakterer. AT-kommandoer sendes til AR.Dronens
kommandoport(5556) som UDP-pakker. Formatet for AT-kommandoer ses på
figur \ref{fig:atsyntaks}.

\begin{framefig}
\begin{verbatim}
 AT*[kommandonavn]=[sekvensnummer],[arg1, arg2 ... argN]<LF>
\end{verbatim}
  \caption{Syntaksen for en AT-kommando: [kommandonavn] kan f.eks. være
    \code{PCMD}, se \ref{tab:kommandoer}. [sekvensnummer]-heltallet skal være stigende for hver
    ny afsendt AT-besked. Listen med argumenter: [arg1, arg2 ... argN],
    varierer i længde afhængig af konteksten. Ved afsending af en
    \code{PCMD}-kommando skal der medsendes 5 talværdier for hhv. flag,
    roll, pitch, gas og yaw.} 
  \label{fig:atsyntaks}
\end{framefig}
%
Ved at afsende en kommando som i figur~\ref{fig:ateksempel} bestemmer
man hvorledes AR.Dronen skal bevæge sig ved translation og rotation,
som beskrevet i \nameref{ssec:fysik}, sektion \ref{ssec:fysik}. 
%
%
\begin{framefig}
\begin{verbatim}
 AT*PCMD=21625,1,0,0,0,0<LF>
\end{verbatim}
  \caption{Eksempel på en \code{PCMD} AT-kommando. Denne specifikke
    kommando bringer AR.Dronen i hovertilstand. [sekvensnummer]'et er 21625. Listen med argumenter er hhv. flag=1, roll=0, pitch=0, gas=0 og yaw=0.}
  \label{fig:ateksempel}   
\end{framefig}
%    [kommandonavn] er navnet på den ønskede kommando
%    [sekvensnummer] er et globalt, altid stigende sekvensnummer
%    [arg1, ... argN] er et variende antal parametre

Brugen, syntaksen og betydningen af 7 forskellige AT-kommando navne
beskrives i detaljer i afsnit 6 af AR.Drone Development
Guide, \cite{techreport:ardroneDevGuide}. Det skal dog nævnes, at
man i reglen skal sikre sig, at man vedbliver at sende beskeder
periodisk for at AR.Dronen ikke skal tolke forbindelsen til
klientenheden som tabt. En tabel med AT-kommandoerne kan ses i tabel
\ref{tab:kommandoer} i appendiks \ref{sec:at}. 

%\section{Userspace process på AR.Dronen}
\section{Udvidelse af AR.Dronen med USB-\-moduler}
\label{sec:udvidelse}
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/userspace_n_module.png}
  \caption{Platformstrukturen på AR.Dronen gør det muligt at tilføje
    kernemoduler, biblioteker og at køre programmer i userspace.}
  \label{fig:userspace15}
\end{framefig}
En robotplatform der kan udvides med nye sensorer efter behov, er
klart mere anvendelig end en helt lukket platform. Derfor blev det
undersøgt om det er muligt at anvende AR.Dronens OTG-USB-port, \cite{electronic:otg}, til andet end at lave softwareopdateringer.
Dette har blandt andet indbefattet at kompilere kernemoduler til AR.Dronens linuxkerne,
samt at omgå AR.Dronens software for ikke at miste strømmen til
USB-porten, \cite{blog:usb}. Den indledende proof-of-concept test var, at få mounted en almindelig USB-memorystick
og skrive til og læse fra den. Indstallationsproceduren er forholdsvis
simpel, idet Linuxdriverne til dette formål er meget generiske. Proceduren
til at aktivere AR.Dronens USB-port og læse fra en USB-memorystick kan læses
i en af specialets tilhørende blogposts: "Enabling the Drone USB
Port",~\cite{blog:usb}. 
\begin{framefig}
  \centering
  \subfigure[Hjemmelavet USB-kabel, en USB memorystick og en WIFI-USB-adapter til tilslutning på AR.Dronen.]{
    \includegraphics[width=0.45\textwidth]{grafik/usbdevices.png}
    \label{fig:cableandusbdevices}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[AR.Dronen med USB memorystick tilsluttet.]{\includegraphics[width=0.45\textwidth]{grafik/usbattached.png}
    \label{fig:usbattached}}
  \label{fig:usbdevices}
\end{framefig}

\subsection{WIFI-sensor på AR.Dronen}
\label{sec:wifisensorudvidelse}
Efter de indledende erfaringer med AR.Dronens USB-port, blev der
implementeret en WIFI-sensor, for på sigt at kunne anvende denne som 
lokaliseringsenhed. WIFI-sensoren består af en USB-WIFI-adapter
og et (\emph{WIFI-sensor-program}) til at aflæse og videresende
signalstyrkerne fra AR.Dronens omkringliggende accesspoints og andre
WIFI-kilder.  

Signalstyrken fra et accesspoint siger generelt noget om hvor
langt modtageren er fra accesspointet. Hvis man ved på hvilken etage
og i hvilket lokale et accesspunkt er placeret, og man modtager et
signal med svag signalstyrke, så ved man at man er langt væk fra den
placering. Hvis man istedet modtager et kraftigt signal fra samme
accespoint, så ved man at man er tæt på placeringen. 
Lokalisering via WIFI-fingerprinting bruger radiosignalstyrker til at
give specifikke lokationer unikke karakteristika,
WIFI-fingerprints. Et WIFI-fingerprint kommer i form af en  
mængde af accesspoint-identifikationer med tilhørende målte
signalstyrkeværdier på en given lokation, se
sektion~\ref{ssec:wifilokalisering} for en detaljeret beskrivelse af
WIFI-fingerprinting. 

For at teknikken skal fungere bedst, skal WIFI-sensor-programmet
modtage og behandle så mange WIFI-pakker som muligt. Derfor sættes
WIFI-adapteren i monitor og  promiscous mode\footnote{Normalt vil et
  netværksinterface frasortere og   ikke videresende datapakker der
  ikke er adresseret til processoren,   men i promiscous mode sendes
  al trafik videre.} så den modtager alle pakker der sendes fra de 
omkringliggende WIFI-kilder. Netværksinterfacet indstilles gennem et
script (\uri{load.sh}, \cite{github:loadsh}) som kaldes i forlængelse
af AR.Dronens  oprindelige bootup-sekvens. Scriptet sørger også for at
indlæse alle de nødvendige kernemoduler og starte
WIFI-sensor-programmet. 

\subsubsection{WIFI-sensor hardware}
AR.Dronen er blevet udstyret med en Dlink DWL-G122 WIFI-adapter,
\cite{electronic:dwlg122}. Denne adapter blev valgt fordi
den bygger på et Ralink chipset, \cite{electronic:wikiralink} og
understøtter monitor mode. Desuden udgiver Ralink linuxdrivere til
deres chipsets. Efter længere tids søgen og eksperimenteren, blev den
korrekte driver fundet og kompileret til ARM-arkitekturen så den var
kompatibel med linuxkernen der anvendes på AR.Dronen. Når driveren er
indlæst, genkendes adapteren og interfacet \emph{ra0} oprettes i
\code{/sys/class/net/}. 

\subsubsection{WIFI-sensor software}
\label{sec:sub:sub:wifisensor}
De første eksperimenter med hensyn til at kompilere og afvikle kode
på AR.Dronen, kan ses bloggen under "Compiling code for the
AR.Drone", \cite{blog:compiling}. Det var simple 'hello 
world' eksempler, men de gav indsigt i proceduren omkring
crosscompiling til AR.Drone-arkitekturen og den anvendte toolchain. 

For at skrive et program der kan modtage pakker fra et
netværksinterface anvender man ofte linux' pcap (packet capture)
bibliotek\cite{electronic:pcap}. Dette er dog ikke installeret på
AR.Dronen som standard og skulle først kompileres til ARM-arkitekturen,
før WIFI-sensor-programmet kunne linke til det, se blogposten
\cite{blog:compiling}.  

WIFI-sensor-programmet kører på AR.Dronen. Det henter WIFI-pakker som
det installerede netværksinterface, gennem WIFI-adapteren, henter fra
luften. De modtagne WIFI-pakker indlæses og MAC\footnote{Media Access
Control}-adressen på pakkens afsender, samt signalstyrken puttes i en
ny besked, som afsendes fra AR.Dronen. De afsendte pakker er del af en
sensorstrøm, WIFI-strømmen, og ligner det der ellers kommer fra
AR.Dronen(navdata og video). Strømmen fra WIFI-sensoren går over UDP-port
5551. Kommunikationen anvender denne port for at holde
klientens interface til AR.Dronen nogenlunde konsistent.

WIFI-sensor-programmet kan opstartes med et interfacenavn som
inputargument, men som standard anvendes \emph{ra0}-interfacet. Man
kan desuden vælge at angive en fast kanal, eller man kan vælge at
anvende channelhopping til at modtage pakker fra forskellige
kanaler. Derudover er det muligt at få tekstuel output til terminalen.
For ikke unødigt at spilde processorkrafter med at modtage og behandle
pakker, hvis de alligevel ikke skal bruges, så venter
WIFI-sensor-programmet efter opstart på et start-signal fra klienten
via en UDP-initialiseringspakke. Modtages startsignalet begynder
WIFI-sensor-programmet at sende MAC-adresser og signalstyrker videre
til klienten.

\section{Vurdering af AR.Dronen som robotplatform} % Thomas,
\label{ssec:dronerecap}
Under den indledende informationssøgning til dette speciale og gennem
diverse forsøg og eksperimenter undervejs, er der indsamlet en del erfaringer med
AR.Dronen. Nogle af de mest markante observationer er opregnet her.

\subsection{Hastighed og inerti}
Det giver mening at beskrive AR.Dronens bevægelser i 3
dimensioner for derved nemmere direkte at kunne bruge accelerometer- og gyro-målinger
fra sensorene om bord. Under arbejdet har vi kunnet konstatere, at $\theta$
og $\phi$-værdierne ikke afviger fra det forventede, hvorimod
$\psi$-værdien har en tendens til at blive mere og mere upræcis under
flyvning, hvilket også bekræftes i \cite{electronic:parrotforum3}.

En observeret egenskab er at man ikke kan sige noget udfra
accelerometer og gyroer (de tilstedeværende sensore) om præcis hvor
langt AR.Dronen har fløjet eller med hvilken hastighed AR.Dronen
bevæger sig. Dog kan der udledes estimater for hastigheder ved at integrere over
accelerationen, men hastigheden i det horisontale plan er i praksis
afhængig af miljøet og kan ikke umiddelbart bestemmes. Roll og pitch
værdierne kan være aflæst til 0 og AR.Dronen vil ligge horisontal, men
kan stadig godt bevæge sig sideværts i luften\footnote{AR.Dronen siges
  at være ``trimmet'' (med en konstant hastighed, evt. $0m/S$, men også forskellig fra $0m/S$) hvis den
  ikke påvirkes af en resulterende kraft og acceleration dermed er 0}
på grund af indebåren inerti eller f.eks. en stille konstant
vind. Bevæger AR.Dronen sig med en konstant hastighed kan det ikke
aflæses af accelerometeret. Afstanden AR.Dronen har flyttet sig kan
således heller ikke udledes af accelerometer og gyro alene. Parrot har
derfor underbygget hastighedsestimatet med en optical-flow algoritme i
styreprogrammet, der bearbejder data fra bundkameraets billedstrøm,
således at hastighedsestimatet bliver mere troværdigt, \cite{velocity_estimation_inside_the_drone}. 

Dette speciales egne eksperimenter med hastighedsudledning er beskrevet i
afsnit~\ref{sec:distance-mesu}. Det viser sig at under de rette 
forhold er hastighedsudledningen udmærket brugbar. Udledningen af
hastighed besværliggøres ved forhold med dårlig belysning og ved
jordoverflader uden forskelligartet tekstur, hvor det ikke er muligt for
bundkameraet at detektere gode features at følge.

\subsection{Afstandsmåleren}
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/sonarzone2.png}
  \caption{a) Udbredelsesområde for ultralydssensorsignal, b) man ser
    at afstanden ikke nødvendigvis er vinkelret på sensoren idet der kan
    forekomme ekko fra et større område under sensoren (figuren er fra
    \cite{electronic:brown})}
  \label{fig:sonicbeam2}
\end{framefig}
Den nedadrettede afstandsmåling varetages som nævnt af en ultralyds-sender og
-modtager. Teknologien gør at afstanden man får ud ikke nødvendigvis er
fra det vinkelrette punkt udfor sensoren, men derimod returnerer den
mindste afstand indenfor et område som det på figur
\ref{fig:sonicbeam2}. Det vil sige højden er udledt af den tid der går
fra en lydpuls afsendes, til der modtages et ekko. Ekkoet behøver ikke
komme fra en stor overflade som en væg eller et gulv, men kan være en
kasse inde i sensorområdet som figur~\ref{fig:sonicbeam2}a. Det kan
specielt være et problem hvis der flyves i et trangt lokale med
kasser, stole og borde på gulvet, hvor de mange legemer vil give en
falsk opfattelse af højden til gulvet.  Fordi teknologien netop er lyd
vil der også være problemer med ekko på skrånende og bløde
overflader.

De ovennævnte problematikker kan have indflydelse ved udvikling med
AR.Dronen. Specielt i de tilfælde quadrokopteren hælder meget, f.eks. ved
flyvning i høj hastighed kan vinklen til gulvet indføre fejl i
højdemålingen.

\subsection{Realtidssystem} 
På forummet AR Drone Flyers, \cite{electronic:ardroneflyers80}, har en bruger skrevet,
at Linuxinstallationen, styreprogrammet og de eksterne processer der kører på
AR.Dronen tilsammen bruger op mod $80\%$ af CPU-kraften.  Om det lige
akkurat er $80\%$ kan formentlig diskuteres. Under alle omstændigheder
skal man tage højde for den begrænsede CPU-ressource, hvis man vælger at afvikle
yderligere processer på AR.Dronen, som f.eks. USB-understøttelse og en
WIFI-sensor, som beskrevet i afsnit~\ref{ssec:wifilokalisering}. Pointen er at man ikke kan bruge
mere end de resterende f.eks. $20\%$ uden at det vil gå ud over
kontrollen med flyvningen.

\subsection{Trådløs båndbredde} 
På grund af den manglende transmissionskontrol operer UDP-protokollen
med noget et forholdsvist lille overhead. Det gør, at der i
princippet kan sendes flere f.eks. videopakker end med en
synkroniseret kommunikationsprotokol som TCP. Oveni at der
kontinuerligt sendes friske datapakker, så giver det mest mening at
anvende en asynkron protokol i realtidskommunikationen med
AR.Dronen, idet det ikke kan betale sig at bruge resourcer på at
generhverve det fåtal af billedframes der går tabt pga. fejl i
transmissionen. Det er bedre at indlæse den næste datapakke
istedet. Information i den gamle datapakke vil alligevel være forældet
% Kan den måles?, hvad er den teoretisk effektive overførselshastighed?
og irrelevant. Vores erfaring har også været, at vi har modtaget både
video og navdatastrømmen med en passende framerate til vores formål.
\todo{kontrol af manual merge morten?}

\subsection{Billedkvalitet} 
AR.Dronen er et produkt der er designet til
at skulle være billig i produktion, for dermed let at kunne erhverves af menigmand. I
deres valg af design har Parrot afvejet et tradeoff mellem
videokvalitet og framerate. Det
betyder, at opdateringshastigheden af billeder og data fra AR.Dronen er
tilstrækkelig i de fleste tilfælde, men, som det fremgår afafsnit~\ref{sec:QRdecoding}, er billedkvaliteten ikke tilstrækkelig
til f.eks. afkodning af QR-koder, \cite{electronic:wikiqr}.

\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/2frames.png}
  \caption{Et billede fra AR.Dronens bundkamera (med opløsning på
    $88\times72$ pixels) i øverste venstre hjørne af et billede fra
    frontkameraet ($320 \times 240$ pixels).}
  \label{fig:2frames}
\end{framefig}

\subsection{Zap imellem 2 kameraer} 
I videostrømmen mellem en klient og AR.Dronen sendes kun et billede ad
gangen. Man kan modtage fire forskellige slags billeder: fra
frontkameraet, fra bundkameraet, fra frontkameraet med bundkameraets
billede overlagt i øverste venstre hjørne og fra bundkameraet med
frontkameraets billede overlagt i øverste ventre hjørne(de fire videostrømme). Skal
der bruges billeder i fuld størrelse fra begge kameraer, er det
nødvendigt at skifte imellem to videostrømme (zappe) og skiftevis få
et billede fra front- og bundkamera. Dette er ikke praktisk, idet det
vil nedsætte opdateringshastigheden i eksempelvis en feedback-kontrol
løkke. Desuden er der et lille delay forbundet med at skifte fra den
ene til det anden kamerastrøm. Anvender man de kombinerede billeder,
se figur~\ref{fig:2frames}, skal man være opmærksom på, at der billede
der er indeholdt i det andet, er af noget mindre opløsning end det
originale($88 \times 72$ pixels for bundkameraets billede når det er
lagt ovenpå frontkameraets, mod $176 \times 144$ pixels
originalt). Denne forskel i kvalitet har tydeligt en indflydelse på
resultatet af diverse billedanalysealgoritmer, se
sektion~\ref{ssec:smallmarks}.
%
%% sjovt at vi i begyndelsen af arbejdet hele tiden har sagt at nu
%% skal vi også huske at teste præcist hvor meget den drifter i yaw
%% piezo-måleren, men enten aldrig rigtig er kommet til det punkt
%% eller også kommet forbi det - ved at acceptere det og så styre udenom
%
%Derefter finde ud af at jo mindre des mindre inerti kan være indeholdt, mere agile agil/adræt
%gå fra centraliseret styring til fosøg med decentraliseret, swarm anonymitet, største proble er sanse apparatet, førhen flere ekserne kameraer til observation og feedback af position til flokken, nu forsøg med lokal dataindsamling fra bl.a. Kinect {http://www.ros.org/wiki/kinect}, {pelican quadrotor}

%strømforsyning, det er godt med 9 batterier, og at have dem opladt inden dagens arbejde, der er lige knap 12 minutters flyvetid - ``så erder i alle fald heller ikke mere''...

\todo{afsluttende bemærkning... evt. reference yderligere
  behandling/opsummering i Resultater/Konklusion...}
 
\chapter{Klient platform} % Morten,
Den grundlæggende platform er udviklet med det formål, at
understøtte det videre arbejde med AR.Dronen. Platformen skal sikre
let og effektiv adgang til AR.Dronens sensor output, samt give
mulighed for at styre AR.Dronen, både manuelt med joystick og tastatur
gennem en PC (altså ikke iPhone, iPad el. lign.) og ved hjælp af
forprogrammerede sekvenser af bevægelser(tasks, beskrives nærmere i
sektion~\ref{sec:tasks}). 

\section{Platformens anvendelse}
\label{sec:platformsadgang}
Python blev, som tidligere nævnt, valgt for at lette tilgængeligheden for
brugeren. Et af specialets mål er, at stille en let anvendelig og
let udbyggelig platform til rådighed for udviklermiljøet omkring
AR.Dronen. Herunder vises simple eksempler på hvordan dele af klientplatformen
kan anvendes. 

\subsection{Nem fjenstyring af AR.Dronen i Python}
\label{sec:sub:fjernstyrpy}
Et simpelt eksempel på anvendelse af det implementerede
kontrolinterface gives på figur~\ref{fig:pythoninterpex1}. Først
startes pythons fortolker, herefter importeres examples-modulet og
metoden square() afvikles. Det antages, at computeren der afvikler
eksemplet er forbundet med AR.Dronens trådløse netværk.

\begin{framefig}
    \begin{verbatim}
>>> import examples as e
>>> e.square()
    \end{verbatim}
\caption{Import og afvikling af simpelt kontrolinterface-eksempel i pythonfortolkeren}
\label{fig:pythoninterpex1}
\end{framefig}

Metoden \code{square()} der kaldes i eksemplet i figur~\ref{fig:pythoninterpex1},
lader AR.Dronen lette, flyve rundt i en firkant og for herefter at
lande igen . Bevægelsesmønstret opnåes ved at der kaldes en
metode (\code{take\_off}, \code{move} eller \code{land}) på kontrolinterfacet(sektion~\ref{sec:sub:controlinterface}), hvorefter der er en pause
før den næste metode kaldes. Metodekaldende oversættes af kontrolinterfacet til
AT-kommandoer og sendes så som UDP-pakker til AR.Dronens
kommandoport. Den konkrete implementation af \code{square()} ses i
figur~\ref{fig:pythonimpl1}. Det bemærkes, at kontrolinterfacet skal
startes (ved kald af \code{c.start()}) før kommandoerne har nogen effekt. 

\begin{framefig}
  \lstset{language=Python, frame=none}
  \begin{lstlisting}
def square():
    import controllers
    import time

    c = controllers.ControllerInterface()
    c.start()

    print 'taking off'
    c.take_off()
    time.sleep(5.0)
    print 'moving Forward'
    c.move(0.0, 0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving right'
    c.move(0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving back'
    c.move(0.0, -0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving left'
    c.move(-0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'landing'
    c.land()

    c.stop()
\end{lstlisting}
\caption{Python implementation der importerer og anvender vores kontrolinterface til en
  simpel flyvning. De fire første numeriske parametre der gives til
  move-metoden repræsenterer henholdsvis roll, pitch, thust og
  yaw. Den boolske parameter indentificerer overfor kontrolinterfacet,
  om der tale om værdier fra en fysisk kontrolenhed(XBox360 controller)}
\label{fig:pythonimpl1}
\end{framefig}

\subsection{Modtagelse af sensordata fra AR.Dronen} % Introduktion til modtagelse af video, navigationsog wifidata fra AR. Dronen
I det følgende gennemgåes mulighederne for at modtage datapakker med video, navigation- og WIFI-data fra AR.Dronen.
For at kunne anvende AR.Dronen som en robot er det nødvendigt at kunne
indsamle data fra de ombordværende sensorer, samt at kunne stille
disse data til rådighed for andre dele af systemet. Til dette formål
er der designet og implementeret en receiver-struktur bestående af tre
receiver-moduler. Disse moduler eksekveres i hver deres process og
sørger selv for at initiere forbindelsen med AR.Dronen. Efter
initialiseringen lytter hver receiver efter enten video-, navigations-
eller WIFI-pakker og når en sådan modtages udføres en passende
afkodning af pakken og den afkodede data deles med processen der
startede receiveren. De tre receivere er ikke
afhængige af hinanden og kan afvikles hver for sig, således kan
klientplatformen konfigureres til at afvikles med en eller flere receivere
tilkoblet. Sektion~\ref{subsection:receiver} giver et eksempel
på hvordan videoreceiveren lettest anvendes, til at modtage og vise
et enkelt billede fra AR.Dronens videostrøm. Anvendelsen af de to
andre receivere foregår på fuldstændig samme måde som videoreceiveren,
den eneste forskel er typen af data der modtages. 

\subsection{Eksempel på modtagelse af videostream}
\label{subsection:receiver}
Eftersom videoreceiveren kan anvendes for sig selv, vises her et
eksempel på hvor få linier kode man behøver for at modtage og vise et
billede fra AR.Dronens kamera. Eksemplet antager at der
allerede er oprettet en virkende forbindelse med AR.Dronens trådløse
netværk. Eksemplet startes fra Pythons kommandopromt.  
\begin{framefig}
\begin{verbatim}
>>> import examples as e
>>> e.receive_and_show_picture()
\end{verbatim}
\caption{Import og afvikling af simpelt videoreceiver-eksempel i Pythonfortolkeren}
\label{fig:pythoninterpex2}
\end{framefig}

Som det ses på figur~\ref{fig:pythonimpl2} kræver det tre liniers kode
for at modtage et billede (og endnu fire for at vise det). Dette
illustrerer fint, hvordan dette abstraktionslag kan
tilbyde et let anvendeligt interface til AR.Dronens forskellige
sensoroutput.   
\begin{framefig}
  \lstset{language=Python, frame=none}
  \begin{lstlisting}
def receive_and_show_picture():
    import receivers, settings, time
    import cv2.cv as cv
   
    video_sensor = receivers.VideoReceiver(VIDEO_PORT)
    video_sensor.start()
    time.sleep(1)
    pic = video_sensor.get_data()
      
    cv.StartWindowThread()
    win = cv.NamedWindow('win')
    cv.ShowImage('win', cv.fromarray(pic))
    cv.WaitKey()
    cv.DestroyWindow('win')

    video_sensor.stop()

  \end{lstlisting}
  \caption{Python implementation der importerer og anvender
    videoreceiver til at modtage og vise et billede}
  \label{fig:pythonimpl2}
\end{framefig}


\pagebreak

\section{Platformens opbygning}
Platformen består af et antal klasser placeret i et antal
Python-moduler som beskrives nærmere i det følgende. Her er
indledningsvis en kort introduktion for overblikkets skyld. Der er
udviklet tre forskellige receiverklasser til at modtage henholdvis video, 
navigationsdata(navdata) og WIFI-data. Disse beskrives i sektion \ref{subsec:receivers}. Der er udviklet to forskellige
controllerklasser til joystick og keyboard som beskrives i sektion \ref{subsec:controller}. Der er
kontrueret en taskmanager (sektion~\ref{subsec:taskmanager}) med ansvar for at opstarte autonome
delopgaver kaldet tasks (tasks beskrives i
sektion~\ref{sec:tasks}). For at styre AR.Dronen anvendes et kontrolinterface. Et simpelt eksempel herpå er allerede
vist i figur~\ref{fig:pythonimpl1}. Kontrolinterfacet
gennemgåes i sektion~\ref{sec:sub:controlinterface}).
Med inspiration fra~\cite{lenser:behaviorbasedarchitecture} og
\cite{Dey01:ctoolkit} er der, udover de allerede nævnte grundreceivere,
implementeret to virtuelle sensorer (sektion~\ref{sec:sub:virtuel}) som
bruges til at fortolke datastrømmene fra de almindelige receivere. 

Platformens klasser instantieres, startes og tilgåes som udgangspunkt
gennem droneklassen (dronemodulet). På denne måde kan udviklere der ønsker at
anvende platformen blot importere dronemodulet og instantiere
droneklassen, hvorefter der er let adgang til diverse sensordata, se
sekvensdiagrammet i figur~\ref{fig:initialiseringssekvens}.
\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/initialisering.png}
  \caption{Klientapplikationens entrypoint til platformen er gennem
    instantiering af drone-klassen, som sørger for at instantiere
    resten af objekterne og processerne i platformen.}
  \label{fig:initialiseringssekvens}
\end{framefig}
Udover ovenstående er der lavet et testdevice, som anvendes i forbindelse
med udvikling, når det ikke er muligt at have en AR.Drone fysisk
tilstede. Sektion~\ref{sec:sub:testdevice} beskriver muligheden for at
simulere output fra AR.Dronen gennem testmodulet.
% De enkelte klasser kan også anvendes hver for sig, se*.

%klasse diagram eller lignende


\subsection{Datastrømme-modtagere på klienten}
\label{subsec:receivers}
Subklasser af \emph{Receiver} er abstraktioner over de fysiske hardwaresensorer på
AR.Dronen. Receivernes opgaver er at initialisere UDP-kommunikationen
med AR.Dronen ved at sende en startbesked og herefter modtage de
datapakker AR.Dronen sender. Desuden skal datastrømmen holdes i live
ved at sende et watchdog-signal i faste intervaller. 
\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/receiveruml.png}
  \caption{Klassehierakiet for receiverklasser. Nedarvende klasser kan
    specialisere sig ved at overskrive de to metoder
    \code{on\_receive\_data} og \code{on\_request\_data}.}
  \label{fig:receiveruml}
\end{framefig}
Receiverklasser nedarver fra en baseklasse: Receiver (se figur \ref{fig:receiveruml}), der indkapsler
den basale receiverfunktionalitet. For bedre at kunne udnytte en
multicore processor er denne baseklasse implementeret som en
selvstændig proces ved hjælp af Pythons
multiprocessing-modul. Receiverprocessen kommunikerer ved hjælp af en
delt liste, som anvendes både til at overføre signaler og data.  For
at tilpasse de enkelte receivere implementerer disse specifikke
metoder (for eksempel til afkodning af den specifikke type data) der
kaldes på bestemte tidspunkter i det generelle receiverloop.
 
\subsubsection{Navdata-modtager}
\emph{NavdataReceiveren} modtager AR.Dronens navdatastrøm (AR.Dronens afstand til
jorden, flyvehastighed i x- og y-retning m.m.) via UDP fra
AR.Dronen og decoder denne, i forhold til de structs der findes
beskrevet i headerfilen navdata\_common.h,~\cite{ardroneorg:navdatacommonh}, fra Parrots SDK.

\subsubsection{Video-modtager og billedafkoding via Psyco}
\emph{VideoReceiveren} modtager og afkoder AR.Dronens videostrøm. Afkodningen
af billederne foregår på baggrund af den process der nævnes i sektion 7.2 side 43 i
\cite{techreport:ardroneDevGuide}. 

\paragraph{JIT-kompileret afkodning af billeder}
%%% % % %
Da Python er et fortolket højniveau sprog er det ikke muligt at opnå
samme afviklingshastighed som ved afvikling af C eller andre
maskinnære sprog. I forbindelse med afkodningen af video 
fra AR.Dronen anvendes biblioteket Psyco,~\cite{electronic:psyco},
til netop at øge afviklingshastigheden. Psyco er en form for JIT
compiler hvormed man kan afvikle umodificeret Pythonkode hurtigere. Til sammenligning vil
almindelig afviklet Pythonkode til afkodning af 100 billeder fra
AR.Dronens billedstrøm tage omkring 37.569 sekunder men hvis det samme
Python-kode afvikles med hjælp fra Psyco er afviklingstiden kun 7.075
sekunder. Ved brug af Psyco kan man altså hente og afkode 5 gange så mange
billeder. Psyco er lavet til 32 bit maskiner og virker fint der, men
desværre har udvikleren af Psyco valgt ikke at lave en 64-bit version
af biblioteket.  

Videoformatet der sendes fra AR.Dronen minder meget om JPEG formatet og 
forfatteren til den algoritme vi har tilpasset, spekulerer
på forumsiden, \cite{electronic:parrotforum2}, om det ville være muligt at anvende
en decideret JPEG decoder til afkodningen. Dette kunne gøre det muligt
at anvende et optimeret C bibliotek til formålet for derved at øge
afkodningshastigheden og samtidig eliminere afhængigheden af Psyco, som
kun fungerer på en 32-bit platform.

\subsubsection{WIFI-modtager}
Som de andre receivere modtager \emph{WIFIReceiveren} en datastrøm fra
AR.Dronen. WIFI-strømmen er dog en tilføjelse vi har lavet og formatet
der skal afkodes er blot en tekststreng bestående af en MAC-adresse og
en signalstyrke. Der modtages således en pakke for hver pakke
det indlejrede WIFI-sensor-program (beskrevet i
sektion~\ref{sec:sub:sub:wifisensor}) modtager. WIFI-receiveren
skiller sig ud fra de 
andre receivere, ved at placere de modtagne adresse/signal-par i en
liste og holde denne liste opdateret med de nyeste signalværdier for
hver adresse. For hver adresse gemmes desuden de sidste 20 modtagne værdier,
deres gennemsnit, deres varians og deres middelværdi. Når
WIFIReceiverens get\_data() metode kaldes, er det disse værdier der
returneres.

\subsubsection{Virtuelle sensorer}
\label{sec:sub:virtuel}
%\cite{Dey01:ctoolkit}\cite{lenser:behaviorbasedarchitecture} #Bardram jcaf?
De virtuelle sensorer er en sen tilføjelse til platformen og er som
tidligere nævnt inspireret af sensorhierakierne beskrevet i
\cite{lenser:behaviorbasedarchitecture} og \cite{Dey01:ctoolkit}, hvor
sensorer opererer på forskellige niveauer og derfor leverer sensor
output på forskellige abstraktionsniveauer. I modsætning til receiverne modtager den
virtuelle sensor ikke data direkte fra AR.Dronen, men derimod netop
fra receiverne. De virtuelle sensorer leverer således information om
AR.Dronens omgivelser på et højere abstraktionsniveau, end de rå
værdier fra receiverstrømmene.  

Den ene virtuelle sensor, PositionSilhouetDetector,~\cite{github:virtuel},
bearbejder kontinuerligt videodata for at genkende henholdsvis
markører på jorden og silhuetter umiddelbart foran AR.Dronen. For at kunne detektere både
markører på gulvet og silhuetter er det nødvendigt for sensoren hele
tiden at skifte mellem bund- og frontkamera. Dette giver imidlertid
problemer for AR.Dronens styreprogram der ved konstante kameraskift
løbende øger sit hukommelsesforbrug indtil Linuxkernen lukker
styreprogrammet ned. Vores målinger viser, at AR.Dronens program kun kan
køre mellem ti og tolv minutter, såfremt der samtidig skiftes kamera
med cirka 5 millisekunders interval. Selvom om dette problem ikke
beskrives direkte nogen steder, antydes det dog af en udvikler på
Parrots forum, at denne funktionalitet ikke er ment anvendt til
hurtige kameraskift i software og at der derfor er stor
sandsynlighed for, at metoden ikke virker pålideligt,
\cite{electronic:parrotforum1}.

Den anden virtuelle sensor, DistanceTracker,~\cite{github:virtuel}, benytter
hastighedsværdierne fra navdatastrømmen. Sensoren udfører løbende
integration over hastighederne i x og y-retning. Dette giver os et
brugbart estimat over den distance AR.Dronen har tilbagelagt. Se
sektion~\ref{sec:distance-mesu}, for resultaterne af de eksperimenter vi har udført med
DistanceTrackeren. 

\subsection{Kontrolinterface til fjernstyring af AR.Dronen}
\label{sec:sub:controlinterface}
Ved hjælp af AT-kommandoerne definerer AR.Dronen et AT-interface til
enheder der er indenfor rækkevidde af dens trådløse netværk. For at
gøre dette AT-interface mere praktisk anvendeligt har vi videreudviklet
på en Python-wrapper, \cite{electronic:venthur}, til AT-interfacet. Vi har implementeret
kontrolinterface som en Pythontråd der løbende sender kommandoer med de
aktuelle kontrolværdier til AR.Dronen. 
Dette betyder, at platformen ikke afhænger af at alle
kontrolpakker når deres destination, hvilket er praktisk da
UDP-protokollen netop ikke garanterer at en pakke når sin destination. %think I understand but humor me and elaborate...
Ved at sende al kontrolkommunikation gennem kontrolinterfacet, skal der ikke tænkes på detaljer som sekvensnumre og
watchdog-beskeder, da disse håndteres transparent af systemet.
  
Kontrolinterfacet( implementeret i ControllerInterface-klassen, \cite{github:controllers}) definerer en række metoder til at
fjernstyre AR.Dronens opførsel og bevægelser. Alt styring af AR.Dronen
fra platformens tråde går altid forbi den samme ene instans af
Python-interfacet se figur~\ref{fig:ciagregateuml}. Udover at virke i platformen, kan interfacet til
i simple tilfælde også anvendes alene, direkte fra
Pythons interpreter (som beskrevet i eksemplet
sektion~\ref{sec:sub:fjernstyrpy}).

\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/controllerinterface_agregate_uml.png}
  \caption{Singeltonklassen ControllerInterface til fjernstyring af
    AR.Dronen.}
  \label{fig:ciagregateuml}
\end{framefig}

\subsection{Controllers}
\label{subsec:controller}
Selvom det er muligt, er det ikke praktisk anvendeligt at brugeren skal
afvikle scripts, der anvender kontrolinterfacet, for hver ønsket
bevægelse (som det der er beskrevet i eksemplet i
figur~\ref{fig:pythonimpl1}). I stedet er der implementeret to
controllere til at tage mod input fra brugeren, en keyboardcontroller der lytter efter input
fra tastaturet og en joystickcontroller der er tilknyttet en fysisk
Xbox360-controller (se figur~\ref{fig:xboxlayout}).

Platformen kan startes med begge controllere på samme tid, men det er
også muligt at eller begge. Under udviklingen af platformen er begge
controllere oftest anvendt sideløbende, da de komplimenterer hinanden.
Man kan som sagt anvende platformen helt uden controllere og udelukkende anvende
taskmanageren til at kontrollere AR.Dronen, men skulle der i det
tilfælde ske noget uforudset under udførslen af en task, vil der
ikke være nogen mulighed for at bringe AR.Dronen manuelt (og sikkert) ned. 

\begin{framefig}
  \centering
  \includegraphics[width=0.6\textwidth]{grafik/controlleruml.png}
  \caption{Klassehieraki for klasser der nedarver fra
    Controller-klassen. Subklasser af Controller er kendetegnet ved
    at have adgang til styring af AR.Dronen gennem controlinterfacet.}
  \label{fig:controlleruml}
\end{framefig}

Begge controllerklasser nedarver fra en grundlæggende
controllerklasse: \code{Controller} (se
figur~\ref{fig:controlleruml}). Denne er implementeret som en tråd der
med et givent tidsinterval kalder en kontrolmetode (\code{process\_events()}). Den simple
opbygning betyder, at hvis man ønsker at implementere en anden type
controller, er det nok at nedarve Controller og implementere
\code{process\_events()} metoden. Da Python behandler metoder som
førsteklasses objekter, er det let at lade en controller skifte mellem
forskellige tilstande. Dette gøres ved at kalde \code{set\_control\_method} med
en ny kontrolmetode som argument. 

\subsubsection{Joystickcontroller}
platformen understøtter XBox360 controlleren gennem den implementerede
Joystickcontroller. XBox360 controlleren anvendes istedet for en iPhone eller iPad, til at
fjernstyre AR.Drone manuelt i forbindelse med testflyvninger. Der var
flere praktiske grunde til at implementere en alternativ
fjernstyring. Blandt andet havde var der et behov for at kunne interagere
både med platformen og AR.Dronen på samme tid, mens der til
stadighed var øjne på AR.Dronen. Desuden blev iPhonestyringen hverken
fundet tilstrækkelig præcis eller særlig intuitiv. Xbox360
controlleren derimod omtales som en af de bedst designede controllere
og har endda fundet anvendelse af militære styrker i USA og
storbritannien, \cite{electronic:zdnet}, \cite{electronic:joystiq},
\cite{electronic:youtube1}.  Fjernstyringsdelen er meget simpel og
fungerer ved at oversætte værdier fra Xbox360 controlleren (som gives af pygames joystickmodul, \cite{electronic:pygame}) og så sende
disse til kontrolinterfacet. Xbox360
Joystickcontrolleren bruges udover manuel fjernstyring af AR.Dronen
også til at starte tasks (sektion~\ref{sec:tasks}). Således kan
piloten manuelt bringe AR.Dronen i en ønsket position og så herefter
starte afvikling af et forudbestemt bevægemønster. På
figur~\ref{fig:xboxlayout} ses vores standardopsætning af Xbox360
controlleren.
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/xbox360layout.png}
  \caption{Standardopsætning af Xbox360 controlleren}
  \label{fig:xboxlayout}
\end{framefig}

\subsubsection{Keyboardcontroller}
Hvis man ønsker at starte platformen uden grafisk brugerflade (og
dermed uden tilhørende funktionalitet til at håndtere input fra brugeren), er det
praktisk stadig at kunne anvende tastaturet som input (evt. i kombination med Xbox360
controlleren). Der er derfor implementeret en simpel
keyboardcontroller, der læser input fra terminalen. Keyboardet
anvendes ikke til direkte styring af AR.Dronen (da Xbox360
controlleren er mere intuitiv og ligeledes tilgængelig), men
i stedet til ting såsom: At få udskrevet nuværende batteriniveau
('b'), at skifte kamera ('z'), at få udskrevet aktive tasks ('c'), at
få udskrevet en oversigt over tråde i programmet ('t'), samt at 
starte tasks ('1', '2' og '3').

\subsection{Testdevice}
\label{sec:sub:testdevice}
Dette modul muliggør gennemførelsen af simple simuleringseksperimenter i et
test miljø. Tanken bag modulet er, at man skal kunne optage, ikke
bare video, men alle sensoroutput fra en flyvning med AR.Dronen, for så
senere at kunne afspille hele flyvningen igen. Selve afspilningen
foregår næsten transparent for resten af platformen. Receiverne skal
stadig initiere kommunikationen (nu blot med testdevicet og ikke den
reelle AR.Drone). Forbindelsen skal stadig holdes i live og data der
modtages skal stadig afkodes. De eneste synlige forskelle der er mellem test og
live-kørsler er timing og hastighed, den første fordi vi ikke har
prioriteret at afsende testdataen i de intervaller de blev modtaget i
og den sidste fordi samme computer nu står både for afsendelse,
modtagelse og afkodning.  Testdevicemodulet har ikke noget at gøre med
optagelsen af sensoroutputtet. Dette foregår ved hjælp af
sensordisplay-værktøjet, beskrevet i sektion~\ref{sec:sensordisplay}.

\subsection{Taskmanager}
\label{subsec:taskmanager}
Taskmanagerklassen startede ud som en almindelig controller, men er
endt som en mere selvstændig del af platformen. Taskmanageren skiller
sig ud ved ikke, i modsætning til joystick- og keyboardcontrollerne,
at arbejde direkte med kontrolinterfacet eller med output fra
receiverne. Taskmanagerens opgave er at starte task (tasks som gennemgåes i
sektion~\ref{sec:tasks}) og stoppe dem igen, samt at facilitere
beslutningen, om hvilken task der har lov til at bevæge AR.Dronen.
\pagebreak
\section{Tasks} % Morten
\label{sec:tasks}
For at indkapsle forskellige bevægemønstre og for at kunne koordinere
en parallel afvikling af disse, er der implementeret et hierakisk
opbygget tasksystem. Enhederne i dette system kaldes tasks og ikke
behaviors, der ellers både bruges i artiklen 'A Modular 
Hierarchical Behavior-Based Architecture',
~\cite{lenser:behaviorbasedarchitecture}, og i Lejos'
subsumption-pakke, \cite{lejossub}. Grunden til dette er, at flere af 
de implementerede tasks er forholdsvis kortlivede (for eksempel take-off og
landtasks) og derfor mere minder om en opgave der skal forberedes,
udføres og afsluttes. Dette ses i modsætning til en behavior, som kan
opfattes som en længerevarende opførsel. Der er også implementeret
højniveau bevægemønstre der minder mere om en traditionelle behaviors
(for eksempel FollowTourTask som gennemgåes i sektion~\ref{sssec:followtourtask}). Vi mener, at det også giver mening, at
kalde disse tasks, da de også implementeres efter vores generelle
taskstruktur.

De første udgaver af tasksystemet indbefattede tasks der indsamlede
og behandlede data fra AR.Dronens sensorer. De behandlede data blev
herefter stillet til rådighed for andre tasks, gennem en delt
kontekstliste. I de endelige udgaver er disse tasks udskilt som
virtuelle sensorer. Tasks skal således ikke behandle sensordata, men
kun reagere og formidle bevægelse på baggrund af højniveau-sensordata.

\subsection{Task typer} 
Der er to hoved-tasktyper, simple og compoundtasks. Simple tasks nedarver
fra basetaskklassen Task. De repræsenterer oftest en simpel bevægelse,
men kan være vilkårligt komplekse. For eksempel har vi en task der,
blandt andet, implementerer en PID-controller.
  
Compoundtasks nedarver fra CompoundTask-klassen og indeholder en liste
af subtasks. Vi opdeler compoundtasks yderligere i to typer, 
sekventielle og parallelle (SeqCompoundTask og
ParCompoundTask). Sekventielle compoundtasks starter deres 
subtasks en efter en og venter med at starte en ny subtask, indtil den
forrige er afsluttet. En sekventiel compoundtask stopper, når den
sidste subtask er afsluttet. Parallelle compoundtasks starter alle
deres subtasks på en gang og stopper, når den sidste subtask er
afsluttet. Herunder gives eksempler på henholdsvis èn compoundtask og
to simple tasks. 

\subsubsection{FollowTourTask}
\label{sssec:followtourtask}
Et eksempel på en compoundtask er FollowTourTasken, hvis tasktræ kan ses
på figur~\ref{fig:followtour}. En FollowTourTask nedarver fra en
SeqCompoundTask og indeholder, udover takeoff og land-tasks, en 
parallel compoundtask der kan veksle mellem en MoveTask og en
HoverTrackTask (det vil sige, at der veksles mellem bevægelse og
fastholdelse af en erkendt position). Selve FollowTourTask 
enheden holder, ved hjælp af et
map-objekt (se sektion~\ref{sec:mapcreator}), styr på den tour der skal
følges og sørger for at opdatere HoverTrackTasken med en ny 
målposition, når det er nødvendigt. 
Hvis map-objekt anskues som en relationel graf, hvor alle knuder er
forbundne, minder den implementerede opførsel meget om Kuipers og Byuns metode der
beskrives i \cite{murphy2000introduction}. Specielt lægges mærke til, at der i
begge systemer udføres fejlkorrigering, hver gang en ny position
detekteres. I vores system sker dette ved, at HoverTrackTasken bringer
AR.Dronen indenfor en maksimum afstand af den præcise position, før
den sendes videre. Det ville desuden være let at tilføje metrisk data
fra DistanceTrackeren (se sektion~\ref{sec:sub:virtuel}), til
map-objektet, hver gang AR.Dronen har bevæget sig mellem to
positioner. 

\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/followtourhieraki.png}
  \caption{Opbygningen af en FollowTourTask.}
  \label{fig:followtour}
\end{framefig}

\subsubsection{HoverTrackTask}
\label{sssec:hovertracktask}
HoverTrackTasken, som er en simpel task, implementerer en
PID-controller,\cite{electronic:pid}, der kan holde AR.Dronen over en
erkendt position. 
Når en markør (se \ref{fig:advancedmarker}) er synlig for AR.Dronens
bundkamera, kan markørens billedkoordinat tilgåes via af
PositionSilhouetDetectoren (se sektion~\ref{sec:sub:virtuel}). Med
dette billedkoordinat udregnes en error-værdi i x og y-retningerne, i forhold til centrum af
billedet. For at kunne bruge errorværdierne, skal de 
konverteres fra pixels til millimeter og der skal tages højde for AR.Dronens
øjeblikkelige hældning. Denne konvertering og korrigering foregår,
efter den metode der beskrives og anvendes af Michaël Ludmann og
Guillaume Depoyant i deres speciale
LUDEP,~\cite{electronic:ludep},~\cite{electronic:ludepcon}. Efter
udregningen af errorværdierne, anvendes de som i en standard
PID-controller, til at bestemme de endelige værdier der sendes til
kontrolinterfacet, for at bevæge AR.Dronen. PID-funktionaliteten er
implementeret i HoverTrackTaskens 
update-metode, \cite{github:newest}.
  
I forbindelse med deltagelse i en FollowTourTask, kan HoverTrackTasken
tildeles information om den specifikke position der skal trackes. Den
kan således lade andre tasks bevæge AR.Dronen, hvis den rigtige position
ikke er fundet. HoverTrackTasken er, i forbindelse med en
FollowTourTask, også ansvarlig for at dreje AR.Dronen i retning af den
næste position, mens nuværende position fastholdes.

\subsubsection{AvoidTask}
\label{sssec:avoidtask} AvoidTasken er også et eksempel på en simpel
task. Den anvender PositionSilhouetDetectoren (se
sektion~\ref{sec:sub:virtuel}), til at detektere personer umiddelbart
foran AR.Dronen. Hvis en person detekteres, vil AvoidTasken bevæge
AR.Dronen op, indtil personen ikke længere er synlig for AR.Dronens
frontkamera. Den forholdsvis simple task udnytter AR.Dronens
muligheder for at bevæge sig i rummet. Grunden til at AR.Dronen bevæges op og
ikke til siden eller bagud er, at AR.Dronen ikke kan orientere sig i
disse retninger uden at rotere. Risikoen forbundet
med at bevæge sig op er lille (da loftshøjden er kendt), i forhold
til risikoen ved at bevæge sig sidelæns eller baglæns i ikke-erkendt
luftrum. Desuden er det let for de andre tasks, at fortsætte deres
udførelse efter AvoidTasken stopper. Idet AR.Dronen bibeholder samme
orientering mens AvoidTasken er aktiv. AvoidTasken kombineres let med
andre tasks, f.eks. en FollowTourTask eller en simpel MoveTask, ved
hjælp af taskcreator-værktøjet (sektion~\ref{sec:taskcreator}).

\subsection{Task hieraki} Ved hjælp af de ovenfor beskrevne tasktyper,
og specialiseringer af disse, kan vi danne en task-træstruktur med
parents og children. Strukturen minder om den beskrevet i
\cite{lenser:behaviorbasedarchitecture}. Det grundliggende formål med
denne træstruktur er at danne et task-hieraki. Taskmanageren skal ved
hjælp af task-hierakiet sørge for, at der aldrig er mere end én task
der bevæger AR.Dronen. Dette løses ved at give alle tasks en
prioritet(level). En given task kan så få tilladelse til at bevæge
AR.Dronen, på to betingelser: 1) Ingen task med højere level og samme
parent ønsker at bevæge AR.Dronen og 2) Taskens parent har tilladelse
til at flytte AR.Dronen.

På figur~\ref{fig:taskhieraky} ses et eksempel på et tasktræ
konstrueret med vores taskcreator (sektion ~\ref{sec:taskcreator}). Under antagelse af, at hver TestTask
ønsker at udføre et antal bevægelser med AR.Dronen og ønsker at udføre
disse bevægelser uden pause, vil TestTask med level 1 slutte først og
TestTask med level 2 slutte sidst.
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/taskhieraky.png}
  \caption{Eksempel på et taskhieraki. Hver task er annoteret med
    deres level. Dette tal beskriver taskens prioritet i forhold til
    andre tasks med samme parent.}
  \label{fig:taskhieraky}
\end{framefig}

\subsection{Task implementationer} 
Der er implementeret to forskellige udgaver
af vores task-system(taskmanageren og alle tasks), begge implementerer
den opførsel og struktur der er beskrevet ovenfor.

Den første, trådbaserede, implementation er inspireret af Lejos'
subsumption-pakke, ~\cite{lejossub}. Lejos er en firmwareerstatning
til LEGO Mindstorms RCX- og NXT-platform, der tillader afviklingen af
Java på disse platforme,~\cite{lejos}.  Den anden, ikke-trådbaserede,
implementation er primært inpireret af artiklen 'A Modular
Hierarchical Behavior-Based
Architecture',\cite{lenser:behaviorbasedarchitecture}.

\subsubsection{Trådbaseret} 
I den første trådbaserede løsning er hver task implementeret som en
individuel tråd. Alle tasks kører samtidig. Når en task ønsker at bevæge
AR.Dronen, får den tilladelse til dette ved at spørge sin parent (som
muligvis skal spørge videre op i træet).

Denne tilgang minder meget om arkitekturen der anvendes af Lejos'
subsumption pakke, \cite{lejossub}. Taskmanageren vil, som tasktræets
rod, have samme rolle som Arbitratoren i Lejos system. Denne
arkitektur har primært et 'bottom-up'-flow, da det er de individuelle
tasks der initierer bevægelsen af AR.Dronen. En ulempe ved designet er dog, at flowet ikke kun går
mod taskmanageren. En del kommunikation er nødvendig for sikre, at
lavere-level tasks bliver deaktiveret, når en højere-level task tager
kontrollen. Erfaringen med denne arkitektur er desuden, at den på
grund af overheadet ved at have en tråd per task, ikke skalerer
særligt godt. 

\subsubsection{Ikke-trådbaseret} Tilgangen i 'A Modular Hierarchical
Behavior-Based Architecture',\cite{lenser:behaviorbasedarchitecture},
er at en aktiveret behavior skal bestemme hvilke af sine
subbehaviors der skal aktiveres. I artiklen er dette praktisk, da
deres robot kan udføre forskellige bevægelser på samme tid. Tilgangen
sikrer, at det bedste sæt af parallelle behaviors kan bestemmes 
og herefter aktiveres.

I den ikke-trådbaserede tilgang forsimples ovennævnte princip. En
Compoundtask på opfordring skal aktivere \emph{den} bedst 
egnede subtask, da vi ikke opererer med flere aktive tasks
samtidig\footnote{De bevægelser vi har til rådighed kan ikke udføres
uden indflydelse på hinanden}. Den bedst egnede
subtask defineres altid, som den med det højeste level og et ønske om at bevæge
AR.Dronen.

Den eneste tråd i denne implementation er taskmanageren. Denne lader
kontinuerligt den bedst egnede task bevæge AR.Dronen. I Praksis 
sker dette ved at taskmanageren vedligeholder en sorteret liste over
de aktive tasks. Under hver iteration kalder taskmanageren
domove()-metoden på den første task i listen (tasken med det højeste
level). Kaldet til domove()-metoden propageres rekursivt ned gennem
tasktræet og returnerer True hvis AR.Dronen blev bevæget og False hvis
ingen task havde interesse i at bevæge den. Hvis metoden returner
False forsøges med den næste task i listen, hvis der returneres True
afbrydes iterationen og der startes forfra. Sammenlignet med den
trådede udgave, er flowet 'top-down', da bevægelsen altid initieres af
taskmanageren.

Denne ikke-trådbaserede tilgang virker mere velegnet til en træbaseret
taskstruktur som vores. Kommunikationen mellem tasks er simplere
og overheadet pga. de mange tråde undgåes.

\chapter{Klientværktøjer} % Morten, 
\label{chap:clienttools}
I dette kapitel gennemgåes tre værktøjer som er udviklet i forbindelse med specialearbejdet. Der er
tale om et program til at præsentere og visualisere datastrømmene fra
AR.Drone (Sensor display), et program til at bygge de map-objekter der
anvendes af FollowTourTasken(Mapcreator) og et program til at
instantiere, kombinere og afvikle Tasks (Taskcreator). Alle tre
værktøjer anvender et drone-objekt som vist på
figur~\ref{fig:initialiseringssekvens} og kan startes i henholdsvis
normal- og testtilstand (se
figur~\ref{fig:sensordisplayrun}, \ref{fig:mapcreatorrun} og
\ref{fig:taskcreatorrun}). Hvis brugeren vælger at starte et værktøj i
testtilstand, vil drone-objektet oprette forbindelse til et testdevice
(beskrevet i sektion~\ref{sec:sub:testdevice}) istedet for AR.Dronen.

\section{Sensor display}
\label{sec:sensordisplay}
\begin{framefig}
  \subfigure[Navdata display]{\includegraphics[width=0.49\textwidth]{grafik/guinav.png}
    \label{fig:navdatadisplay}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Video display]{\includegraphics[width=0.49\textwidth]{grafik/guivideo.png}
    \label{fig:videodisplay}}
  
  \subfigure[WIFI display]{\includegraphics[width=0.49\textwidth]{grafik/guiwifi.png}
    \label{fig:wifidisplay}}
 ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[WIFI samples display]{\includegraphics[width=0.49\textwidth]{grafik/guisamples.png}
    \label{fig:sampledisplay}}
 

  \caption{Sensordisplay værktøjet har fire præsentationstilstande. En
  til hver af de tre sensorstrømme(navdata, video og WIFI), samt en
  til at sammenligne WIFI-signalstyrker.}
  \label{fig:sensordisplay}
\end{framefig}

Sensordisplay-værktøjet er implementeret i
sensordisplay.py,~\cite{github:sensordisplay}. Programmet anvender et
drone-objekt til at få adgang til AR.Dronens
sensorstrømme. Brugeren vælger hvilken sensorstrøm der ønskes
præsenteret, ved at trykke på en af de fire radio buttons (Navdata,
Video, WIFI eller WIFI samples), som aktiverer hver deres
præsentationstilstand. Hvis brugeren ønsker at indsamle datapakker til
anvendelse i testtilstand, kan dette gøres ved at trykke på knappen
'Capture Sensor Data'. De rå datapakker vil så blive gemt ved
programmets afslutning og kan herefter bruges af det implementerede
testdevice, se sektion~\ref{sec:sub:testdevice}.

Når navdatastrømmen præsenteres, ser brugeren et skærmbillede som
det på figur~\ref{fig:navdatadisplay}. På dette billede ses værdier
såsom højde, hastighed, samt vinklerne $\phi$, $\theta$ og 
$\psi$. Desuden vises oplysninger om AR.Dronens umiddelbare tilstand,
såsom motortilstand, ultralydstilstand og om hvorvidt førnævnte Tait-Bryan-vinkler er
indenfor de tilladte grænser.

Når videostrømmen præsenteres, vises et billede som det på
figur~\ref{fig:videodisplay}. Afhængig af hvilken videostrøm der
modtages, eller om der løbende skiftes videostrøm, vil en af
billedrammerne muligvis være sort. Hvis der løbende skiftes videostrøm,
vises det nyeste billede fra hver videostrøm i de to billedrammer.

Hvis WIFI-sensorprogrammet er startet på AR.Dronen (beskrevet i
sektion~\ref{sec:sub:sub:wifisensor}) og WIFI-strømmen præsenteres, vil brugeren blive
præsenteret for et billede som det på figur~\ref{fig:wifidisplay}. Her
ses en søjle der repræsenterer den sidst kendte signalstyrke, for hver
erkendt WIFI-kilde. I denne præsentationstilstand er der desuden mulighed for
at indsamle signalstyrkesamples og sætte en mål-signalstyrkesample. 

Under udviklingen af WIFI-fingerprintingalgoritmen beskrevet i~\ref{sssec:wifialgo},
har det været nødvendigt, at kunne følge udviklingen i WIFI-signalstyrker
mellem forskellige lokationer. Dette kan gøres i den fjerde og sidste
præsentationstilstand. En signalstyrkesample registreres ved at trykke
på knappen 'Take Sample'. Disse indsamlede signalstyrkesamples kan
herefter sammenlignes. Skærmbilledet, figur~\ref{fig:sampledisplay},
viser en ramme for hver erkendt WIFI-kilde. I rammerne angives
WIFI-kildens signalstyrke for hver registreret
signalstyrkesample. Signalstyrkerne vises som søjler der således viser
kildens signalstyrkeudvikling.

Sensordisplay programmet startes fra en linuxkommandolinie, som vist
på figur~\ref{fig:sensordisplayrun}
\begin{framefig}
\begin{verbatim}
$ ./sensordisplay.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./sensordisplay.py -t
\end{verbatim}
\caption{Afvikling af sensordisplay-værktøjet fra en linuxkommandolinie.}
\label{fig:sensordisplayrun}
\end{framefig}

\section{Mapcreator}
\label{sec:mapcreator}
\begin{framefig}
  \subfigure[World editor]{\includegraphics[width=0.49\textwidth]{grafik/mapworld.png}
    \label{fig:mapworld}}
  ~ 
  \subfigure[Tour editor]{\includegraphics[width=0.49\textwidth]{grafik/maptour.png}
    \label{fig:maptour}}
    
  \caption{Mapcreator-værktøjet, som anvendes til at redigere 2D kort,
    har to redigeringstilstande, world(a) og tour(b).}
  \label{fig:mapcreator}
\end{framefig}
Mapcreator er et værktøj til at oprette og redigere map-objekter.
Map-objekter instantieres fra klassen PosMap(placeret i map.py,
\cite{github:map}) og bruges af FollowTourTasks til at navigere
efter. 
Mapcreator har to redigeringstilstande, world og tour, som kan 
vælges ved at trykke på det tilsvarende faneblad øverst i billedet. De
to tilstande ses på figur~\ref{fig:mapcreator}.

I world-redigeringstilstanden kan brugeren tilføje, flytte eller fjerne
punkter på et 2D kort. Meningen med punkterne er, at de skal matche en
visuel markør i den fysiske opstilling, som vist på
figur~\ref{fig:simplesetup}. Internt i map-objektet vedligeholdes en
liste med vinklerne mellem alle punkter. Når punkter tilføjes, flyttes
og slettes opdateres denne liste også automatisk. Udover punkternes
fysiske position i forhold til hinanden, kan man også tilføje
WIFI-signalstyrkemålinger (WIFI-fingerprint) til punkterne ved at trykke på
'Add/update WIFI sample'-knappen og herefter trykke med musen på det
punkt der ønskes markeret. Hvis WIFI-signalstyrkemålingen
ønskes slettet, trykkes først på 'clear WIFI sample' og derefter på
punktet. World-skærmbilledet kan ses på figur~\ref{fig:mapworld}.

I tour-redigeringstilstanden kan brugeren ikke længere redigere
punkterne, til gengæld kan disse forbindes til en tour. Dette gøres
ved at holde 'ctrl' nede og så trække en linie mellem to punkter med musen. Det
er kun muligt, at forbinde til det første eller sidste punkt i en
eksisterende tour. Det er muligt at fjerne liniesegmenter fra en tour
ved at markere segmentet med musen og herfter trykke på knappen
'Delete Segment'. Ved at markere et liniesegment og trykke enten
'Add/remove A' eller 'Add/remove B' kan man markere, at dette
liniesegment tilhører en specifik type. Denne annotering af
liniesegmenter anvendes endnu ikke til noget af FollowTourTasken. På
sigt er det dog meningen, at
FollowTourTasken skal kunne vælge en bevæge-algoritme på baggrund af
denne information. Tour-skærmbilledet kan ses på figur~\ref{fig:maptour}.

Når programmet sluttes, gemmes map-objektet i filen
./testdata/map.data ved hjælp af Pythons pickle-modul. I map klassens
constructor søges efter denne fil og hvis den findes, indlæses
map-objektets state fra denne.

Som sensordisplay-værktøjet, startes mapcreator-værktøjet fra en linuxkommandolinie, som vist
på figur~\ref{fig:mapcreatorrun}.
\begin{framefig}
\begin{verbatim}
$ ./mapcreator.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./mapcreator.py -t
\end{verbatim}
\caption{Afvikling af mapcreator-værktøjet fra en linuxkommandolinie.}
\label{fig:mapcreatorrun}
\end{framefig}

\section{Taskcreator}
\label{sec:taskcreator}

\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/taskcreator.png}
  \caption{Taskcreator-skærmbillede. Dropdown-listen med tilgængelige Tasks og  parameterinputfelterne
  til den valgte Task, er markeret med rødt.}
  \label{fig:taskcreator}
\end{framefig}

Taskcreator-værktøjet bruges til at instantiere, kombinere og afvikle
tasks (beskrevet i sektion~\ref{sec:tasks}). 
Brugeren kan vælge mellem de tilgængelige tasks ved at trykke
på dropdown-listen, som ses yderst til venstre i den røde firkant på
figur~\ref{fig:taskcreator}. Herefter kan standardparametrene for den valgte task
tilpasses i inputfelterne, bl.a. kan man her angive taskens level. En tasks level beskriver dens prioritet i forhold til
andre tasks med samme parent. Den valgte task
instantieres ved at trykke 'Add task' og kan fjernes igen, ved at markere den med musen
og trykke 'Delete task'. Tasks der nedarver fra klassen
CompoundTask (ParCompoundTask, SeqCompoundTask og FollowTourTask)
kan indeholde subtasks. Subtasks tilføjes ved at holde 'ctrl' nede og
trække en linie fra compoundtasken til den ønskede
subtask med musen. Hvis brugeren ønsker afkoble en subtask fra en compoundtask,
gøres det ved at gentage ovennævnte
procedure. Figur~\ref{fig:taskcreator} viser et opbygget tasktræ   
bestående af tre ParCompundTasks og fire TestTasks. 

Når brugeren har instantieret og kombineret de ønskede Tasks, kan
disse afvikles og dermed udføres af AR.Dronen. Dette gøres ved at markere den
ønskede Task og trykke på 'Execute task'. Afviklingen af en Task eller alle
Tasks kan stoppes ved at trykke på henholdsvis 'Stop task' eller 'Stop
all tasks'. 

Taskcreatoren startes som de andre værktøjer fra en
linuxkommandolinie, se figur~\ref{fig:taskcreatorrun}
\begin{framefig}
\begin{verbatim}
$ ./taskcreator.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./taskcreator.py -t
\end{verbatim}
\caption{Afvikling af taskcreator-værktøjet fra en linuxkommandolinie.}
\label{fig:taskcreatorrun}
\end{framefig}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Eksperimenter - section
\chapter{Eksperimenter} % Thomas & Morten
\label{chap:Eksperimenter}
En vigtig forudsætning for at en robot kan navigere i et miljø er, at
den er i stand til at lokalisere sig selv i dette miljø. Der er derfor
blevet ekperimenteret med forskellige former for lokalisering. Mere
specifikt er der arbejdet med lokalisering ved hjælp af
WIFI-signalstyrker og lokalisering ved hjælp af visuelle markører. I
eksperimenterne er der gjort brug af et map-objekt med beskrivelser af
lokationer(bestående af indsamlede WIFI-signalstyrker,
markørbeskrivelse og koordinater) og en rutebeskrivelse bestående af
en liste med lokationer. Map-objektet indeholder desuden en
fortegnelse over vinkelretninger mellem alle
lokationspar. Map-objektet oprettes let ved hjælp af
Mapcreator-værktøjet beskrevet i \ref{sec:mapcreator}. For at teste
forskellige de lokaliseringsmetoder anvendes HoverTrackTasks(se
section \ref{sssec:hovertracktask}) og FollowTourTasks(se section
\ref{sssec:followtourtask}).

Udover lokalisering, er mulighederne for at udlede andre
informationer fra AR.Dronens miljø også blevet
undersøgt. F.eks. hvor langt AR.Dronen har bevæget sig, hvorvidt den
befinder sig i en korridor og afstanden til andre objekter.

\section{Lokalisering via WIFI-fingerprinting} % Morten (1side)
\label{ssec:wifilokalisering}
Som nævnt i sektion~\ref{sec:udvidelse} er AR.Dronen blevet udvidet med
en WIFI-sensor. Denne sender løbende information om de omkringliggende
WIFI-kilders signalstyrker. I de følgende eksperimenter anvendes en
teknik kaldet WIFI-fingerprinting. WIFI-fingerprinting fungerer i
hovedtræk ved at der opbygges et WIFI-map
under en initiel offline-fase, hvorefter livesignalstyrkemålinger kan
sammenlignes med dette map. WIFI-mappet opbygges ved at foretage
signalstykemålinger på et antal faste punkter. Generelt kan det siges,
at jo mere offlinedata der indsamles, jo bedre fungerer 
lokaliseringen. WIFI-fingerprintingteknikken beskrives i
\cite{832252}. 

Under eksperimenterne anvendes kun den allerede tilstedeværende
WIFI-infrastruktur i testmiljøet(Zuse-bygningen, \cite{zuse}).

\subsection{WIFI-fingerprinting-algoritmen}
\label{sssec:wifialgo}
I de følgende eksperimenter anvendes en fingerprintingalgoritme, der
udfører en simpel nearest-neighbor søgning i et WIFI-map. Dette
WIFI-map opbygges under offline.proceduren umiddelbart før
eksperimentets udførelse og er en integreret del af det map-objekt der
anvendes af FollowTourTasken. Et overblik over offline-proceduren kan
ses på figur~\ref{fig:offline}. 
Nearest-neighbor søgningen udføres ved at sammenligne hvert
punkt i WIFI-mappet med de nuværende signalstyrker. To sæt af
signalstyrker sammenlignes ved først at finde en fællesmængde af
WIFI-kilder i de to sæt og herefter udregne en afstand mellem de to
sæt, baseret på denne fællesmængde. Lokaliseringsproceduren er
illustreret på figur~\ref{fig:online} Algoritmen er implementeret i
virtualsensors.py, \cite{github:virtuel} og WIFI-mappet er
implementeret som en del af map.py, \cite{github:map}. 

\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifioffline.png}
  \caption{Diagram der viser viser fingerprintingteknikkens
    offlinefase. WIFI-sensoren sender løbende signalstyrker til
    WIFI-receiveren, der stiller disse tilrådighed for det program der
    bruges til at opbygge WIFI-mappet(mapcreator.py). Figuren er
    tilpasset fra \cite{quan2010wi}.}
  \label{fig:offline}
\end{framefig}

\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifionline.png}
  \caption{Diagram der viser viser lokaliseringsfasen. Outputtet fra
    WIFI-receiveren sammenlignes med indholdet i WIFI-mappet og
    resultatet er den mest sandsynlige position. Figuren er
    tilpasset fra \cite{quan2010wi}.}
  \label{fig:online}
\end{framefig}

\subsection{Første Eksperiment}
Opstillingen til dette eksperiment består af 3 markører med 3 meters
mellemrum som vist på figur~\ref{fig:wifisetup}. Efter opstillingen
udførtes en indsamling af WIFI-signalstyrker som beskrevet i
sektion~\ref{ssec:wifilokalisering}. Til dette formål anvendtes   
mapcreator-værktøjet(sektion~\ref{sec:mapcreator}). Indsamlingen blev
udført i en højde af 1 meter, med AR.Dronen placeret på et fast
stativ.
\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifisetup.png}
  \caption{Setup til første WIFI-eksperiment, knappen "Add/update WIFI
    sample" anvendes til at associere en WIFI-signalstyrkemåling med
    en lokation.}
  \label{fig:wifisetup}
\end{framefig}

Efter den indledende fase, blev AR.Dronen skiftevis placeret over de
tre markører og WIFI-fingerprinting-algoritmen afviklet 200
gange. Dette blev gentaget 3 gange for hver position. På
tabel~\ref{fig:wifidetection1} ses detektionsraten for hver udførelse,
detektionsraten defineres som antal gentagelser(200) divideret med
antal korrekte lokaliseringer.
\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Position & 1. gentagelse & 2. gentagelse & 3. gentagelse \\
    \hline
    1        & 0.55          & 0.26          & 0.685         \\
    \hline
    2        & 0.67          & 0.81          & 0.735         \\
    \hline 
    3        & 0.925         & 0.915         & 0.93          \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for WIFI-fingerprinting, 3 positioner
    med 3.0 meters afstand}
  \label{fig:wifidetection1}
  \end{framed}
\end{table} 
Som det ses er resultaterne meget svingende. Med den implementerede fingerprinting
algoritme og afstande mellem lokationerne på 3 meter eller derunder,
kan en konsistent lokalisering ikke garanteres.

\subsection{Andet Eksperiment}
Det andet eksperiment med WIFI-lokalisering er direkte afledt af de
dårlige resultater i det første eksperiment. Det blev besluttet at øge
afstanden mellem punkterne til 6 meter, for herefter at gentage
eksperimentet som beskrevet ovenover.
Resultatet af dette eksperiment kan ses på
tabel~\ref{fig:wifidetection2}. De opnåede detektionsrater under denne
gennemførsel er markant bedre end under det første eksperiment 
\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Position & 1. gentagelse & 2. gentagelse & 3. gentagelse \\
    \hline
    1        & 1.0           & 1.0           & 0.995         \\
    \hline
    2        & 0.725         & 0.9           & 0.855         \\
    \hline 
    3        & 1.0           & 0.97          & 0.955         \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for WIFI-fingerprinting, 3 positioner
    med 6.0 meters afstand}
  \label{fig:wifidetection2}
  \end{framed}
\end{table} 

\subsection{Resultater}
Med denne opstilling, kan man kun forvente en konsistent lokalisering, hvis
lokationerne ligger med 6 meters afstand eller mere. Dette resultat
gør, at denne eksperimentrække ikke føres videre. WIFI-fingerprintingalgoritmen, eller en videreudvikling,
kunne muligvis fungere som støtte til en primær visuel
lokaliseringsmetode.
Bedre resultater kunne muligvis være opnået hvis der var opstillet en
WIFI-infrastruktur specifikt til eksperimentet. 

\section{Visuel lokalisering}
\label{ssec:visuellokalisering}
\begin{framefig}
  \centering
  \subfigure[Simpel markør]{
    \includegraphics[width=0.20\textwidth]{grafik/simplemarker.png}
    \label{fig:simplemarker}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[semiavanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/semiadvancedmarker.png}
    \label{fig:semiadvancedmarker}}
  ~
  \subfigure[avanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/advancedmarker.png}
    \label{fig:advancedmarker}}

  
  \caption{Markører vi har testet i forbindelse med visuel lokalisering af AR.Dronen}
  \label{fig:markers}
\end{framefig}

\subsection{Lokalisering via simple markører} % Thomas & Morten
\paragraph{Første eksperiment}
\label{p:simple1}
Det første forsøg med visuel lokalisering anvendte en simpel rød
markør i A5 størrelse, se figur~\ref{fig:simplemarker}. Til eksperimentet
var der implementeret en blobdetectionalgoritme i Python, \cite{github:blob}, som for
hver pixel vurderer om denne er rød nok til at være en del af markøren. Algoritmen antager at der i
billedet kun findes et rødt objekt og er på grund af Pythons
afviklingshastighed ikke videre hurtig. Til dette eksperiment hvor
hastighed ikke var afgørende, var den dog anvendelig.

Eksperimentet udførtes ved at markøren blev placeret på
gulvet og AR.Dronen placeret i position over markøren, hvorefter
blobdetectionalgoritmen blev startet. Detektionsraten defineressom
antal succesfulde detektioner delt med antal gentagelser af
algoritmen. Algoritmen kørtes 200 gange i henholdsvis 0.5, 1.0, 1.5
og 2.0 meters højde og resultaterne kan ses i tabel
~\ref{fig:detection1}, sammen med resultaterne for de andre markører.
 
Det første forsøg viste, at den simple markør, i højder fra 0.5 til
2.0 meter, altid kunne genkendes med AR.Dronen bundkamera.

\paragraph{Andet eksperiment} 
Næste skridt var at anvende ovennævnte lokaliseringsmetode i
forbindelse med afviklingen af en HoverTrackTask(sektion
~\ref{sssec:hovertracktask}). Dette blev gjort, for at få en idé om
metodens anvendelighed i en kontekst hvor effektivitet er afgørende.

Til dette forsøg anvendtes også kun en enkelt markør og forsøget
afvikledes ved at AR.Dronen blev fløjet i position over denne,
hvorefter HoverTrackTasken blev aktiveret. Det viste sig tydeligt, at
blobdetectoren ikke var effektiv nok. Der blev ikke opdateret hurtigt
nok og som resultat nåede AR.Dronen ofte at bevæge sig væk fra
markøren. Dette resultat fik os til at reimplementere HoverTrackTasken
således, at den anvendte den implementerede blobdetector til at opdage
markøren og herefter anvendte en af OpenCVs opticalflow-algoritmer
(cv2.calcOpticalFlowPyrLK, \cite{opencv:optflow}) til at tracke den i
videostrømmen. Afviklingen af opticalflow-algoritmen er omtrent 10
gange hurtigere end blobdetectoren. Ved gentagelse af forsøget med denne
tilføjelse, var AR.Dronen meget mere responsiv og efter at have
fintunet HoverTrackTaskens PID-parametre var AR.Dronen i stand til at
forblive over markøren. På fig~\ref{fig:simplehover} ses
HoverTrackTraskens PID error-værdier i forhold til markøren på gulvet,
over en periode på mere end 2 minutter. Errorværdierne er omregnet fra
pixels til millimeter ved hjælp af den højdeværdi vi løbende får fra
navdatastrømmen. Det bemærkes at der er et udsving omtrent midt i
forløbet. Dette er ikke usædvanligt, da AR.Dronen er meget påvirkelig
af turbulens og dette let forekommer, især i mindre lokaler.
\begin{framefig}
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/simpledronehover.png}
  \caption{HoverTrackTaskens errorværdier over en længere
    periode. Errorværdierne angiver hvor mange millimeter AR.Dronen
    befinder sig fra markørens centrum. Errorværdierne er indsamlet
    ved at gemme disse for hvert gennemløb af HoverTrackTaskens update-metode.}
  \label{fig:simplehover}
\end{framefig}

\paragraph{Tredje eksperiment}
\label{p:simple3}
Sidste eksperiment med simple markører anvendte en FollowTourTask
der fulgte en rute mellem fire markører på gulvet. Eksperimenterne startede
denne gang fra jorden, da en FollowTourTask selv letter og lander.
Eksperimentets setup, både det konstruerede map-objekt og den fysiske opstilling ses
på figur ~\ref{fig:simplesetup}.
\begin{framefig}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/simplesetup.png}
  \caption{Repræsentation af det map-objekt FollowTourTasken anvender til at
  Navigere og den fysiske opstilling map-objektet afspejler.}
  \label{fig:simplesetup}
\end{framefig}

Det var forventet, at udfordringen ville være at opdage markøren
tidsnok, til at stoppe AR.Dronen før den var fløjet forbi. Dette
problem kan dog løses ved at lade AR.Dronen flyve i en passende højde,
en højde på 1800 mm blev fundet passende til formålet. Til
gengæld afdækkede eksperimentet to andre problemer. Det første
problem var relaterede til, at alle vores markører var ens. Dette betød, at der var
tilfælde hvor AR.Dronen fandt samme lokation to gange i træk, men
antog at den fundne lokation var den næste i
rutebeskrivelsen. Problemet lagde grund til tanken om at anvende mere
avancerede lokationsmarkører for at opnå en mere bedre
lokalisering. Det andet problem var, at hvis AR.Dronen først mistede en markør
fra synsfeltet, så var det umuligt at fortsætte. Dette
problem løstes ved at tilføje en recovermode til HoverTrackTasken,
således at denne kunne bevæge AR.Dronen opad indtil enten markøren
igen kunne genkendes eller en højdegrænse blev nået, hvorefter
experimentet alligevel måtte afsluttes. Denne simple tilføjelse gjorde
vores FollowTourTask meget mere robust.

\paragraph{Resultater}
De overordnede resultater af eksperimenterne med simple
lokationsmarkører var, at AR.Dronens evne til at
genkende simple markører blev bekræftet. Desuden blev både HoverTrackTasken
og FollowTourTasken udviklet i løbet af eksperimentet. I løbet af
tredje eksperiment viste det sig, at markørkonceptet var
for simpelt til at være praktisk anvendeligt. Dette medførte
udviklingen af mere avancerede markører, for bedre understøttelse af FollowTourTasken.
\cite{electronic:youtube2}(youtube link) viser en video hvor AR.Dronen flyver frem
og tilbage mellem to simple markører og demonstrerer at lokaliseringen,
på trods af de to nævnte problemer, oftest fungerer. 

\subsection{Lokalisering via avancerede markører} % Thomas & Morten
\label{ssec:avancemarkoer}
Efter de indledende eksperimenter blevt det besluttet at designe og anvende en
mere avanceret markør. Det første redesign var markøren der
ses på figur~\ref{fig:semiadvancedmarker}. 

\paragraph{Detekteringsalgoritme}
Grundideen er at man først detekterer den røde firkant, hvorefter det
indkransede område afsøges for en farvet blob der relaterer markøren
til en specifik lokation. 

For at sikre en hurtig detektering(og dermed lokalisering) gør algoritmen omfattende brug af OpenCVs optimerede
billedbehandlingsalgoritmer,
\cite{opencv:main}. Detekteringsalgoritmen er implementeret i den
virtuelle sensor PositionSilhouetDetector og afvikles 
således konstant for at give et så nøjagtigt billede af det
øjeblikkelige miljø som muligt. Første skridt i algoritmen er at
udføre thresholding ved hjælp af OpensCV inRange algoritme. Dette
giver et billede, hvor alle de oprindelige røde pixels nu er hvide
og resten er sorte.  Andet skridt er at finde konturer i det binære
billede. Dette gøres med OpenCVs findContours algoritme. Tredje
skridt indbefatter at udvælge de konturer der har et stort nok areal
til at være interessante. Dette gøres med OpenCVs contourArea
algoritme.  Fjerde skridt er at undersøge områderne der dækkes af de
udvalgte konturer, for at finde en farvet blob. Denne undersøgelse
anvender igen OpenCVs inRange algoritme. Hvis en sådan farvet blob
findes, kan farven sammenlignes med beskrivelsen af de forskellige
lokationer i map-objektet og dermed lokalisere AR.Dronen
præcist.

\paragraph{Første eksperiment}
\label{p:advanced1}
Dette eksperiment udførtes på samme måde som
\ref{p:simple1}, blot var den simple markør skiftet 
ud med en semiavanceret markør, vist på figur
~\ref{fig:semiadvancedmarker}. Forventningen var fra start, at det
ville være svært at identificere markøren over en hvis højde, da den
røde markering ikke er lige så koncentreret som på den simple
markør. Efter at have tilpasset algoritmens thresholdværdier og udført
eksperimentet i højder fra 0.5 til 2.0 meter, blev forventningerne
bekræftet. Den semiavancerede markør kunne med en hvis sikkerhed
genkendes op til 1.5 meters højde, men herefter faldt detektionsraten
betydeligt, resultaterne kan ses i tabel \ref{fig:detection1}. 

En detektionshøjde på 1.5 meter ligger lidt under 1.8 meter, som tidligere var
bestemt som en passende flyvehøjde. En markør med større rød overflade
blev derfor udviklet, se
figur~\ref{fig:advancedmarker}, for at sikre bedre detektion. En
gentagelse af eksperimentet med den avancerede markør bekræftede, at
denne nu kunne detekteres i op til 2 meters højde. Den avancerede
markør er desuden designet sådan, at den på sigt kan bruges til at
udlede AR.Dronens præcise retning. Dette vil gøre FollowTourTasken mindre
afhængig af de upræcise $\psi$-målinger fra AR.Dronens piezo-sensor(
se sektion~\ref{ssec:dronerecap}). 

\paragraph{Andet eksperiment}
Dette eksperiment er næsten identisk, med det andet eksperiment der blev udført for
de simple markører. Det skal dog nævnes, at vores HoverTrackTask i
mellemtiden blev redigeret til at anvende den virtuelle sensors
detekteringsresultater, istedet for selv at udføre en
detekteringsalgoritme. Under dette eksperiment var den virtuelle sensor
indstillet til ikke at skifte mellem de to kamerastrømme. 
Som forventet var AR.Dronen også i stand til at holde positionen over
den avancerede markør.

\paragraph{Tredje eksperiment}
\label{p:advanced3}
Dette eksperiment varierede fra det ovenstående, ved at den virtuelle
sensor her løbende skiftede kamerastrøm. Det blev undersøgt om
HoverTrackTasken stadig var i stand til at stabilisere AR.Dronen over
en erkendt lokation, når bundkameraets framerate blev begrænset. Forsøget viste
AR.Dronen stadig forblev stabil over lokationen. Til gengæld stoppede
den pludseligt da AR.Dronens originale styringssoftware lukkede ned efter omtrent ti
minutter. Nærmere undersøgelser viser at AR.Dronens software muligvis
har en memoryleak. Dette viser sig ved at softwarens hukommelsesforbrug
stiger kraftigt, så snart de hurtige kameraskift iværksættes ved brug
af AR-kommandoen set\_config. Denne opførsel beskrives også i sektion~\ref{sec:sub:virtuel}. 

\paragraph{Fjerde eksperiment}
Ligesom \ref{p:simple3}, anvendtes til dette eksperiment en opsætning med
fire markører. Formålet var at bekræfte, at der stadig med de nye
markører og den ændrede HoverTrackTask, kunne navigeres mellem
de fire punkter med vores FollowTourTask. Opstillingen kan ses på
figur ~\ref{fig:advancedsetup}.

Eksperimentet bekræftede, at FollowTourTasken konsistent kunne
lokalisere AR.Dronon og følge den fastlagte tour. 
\begin{framefig}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/advancedsetup.png}
  \caption{Billede af opstilling med avancerede markører til brug for
    det fjerde eksperiment med disse markører}
  \label{fig:advancedsetup}
\end{framefig}

\subsection{Detektering af markører på små billeder}
\label{ssec:smallmarks}
I de ovenstående eksperimenter anvendes AR.Dronens bundkamera.
Dette har en opløsning på 176x144. Der er dog andre tasks der skal
bruge data fra frontkameraet. Af den grund kan det være nødvendigt kontinuerligt at zappe mellem
de to kameraer og dette fungererer som nævnt i sektion~\ref{p:advanced3} ikke særligt
godt. Derfor undersøges mulighederne for at anvende den
videostrøm hvor bundkameraets billeder er lagt oven på 
frontkameraets(se \ref{fig:2frames}). Når bundkameraets billeder udtrækkes fra
denne strøm, er disse billeder i en opløsning på 88x72.
Eksperimenterne er fuldstændig magen til dem der udførtes i sektion~\ref{p:simple1} og
 sektion~\ref{p:advanced1}. De er ligeledes udført i højderne 0.5, 1.0, 1.5 og 2.0 og er
udført både med de simple, semiavancerede og avancerede
markører. Resultaterne ses i tabel~\ref{fig:detection2}, de viser tydeligt at
det ikke er muligt at anvende de små billeder, da de selv for de
simple markører kun giver et pålideligt resultat op til 1.5 meter,
hvorefter detektionsraten falder betydeligt. 

\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Højde & Simpel & Semiavanceret & avanceret \\
    \hline
    0.5   &  1.0   & 1.0           & 1.0       \\
    \hline
    1.0   & 1.0    & 1.0           & 1.0       \\
    \hline 
    1.5   & 1.0    & 0.805         & 1.0       \\
    \hline
    2.0   & 1.0    & 0.045         & 1.0       \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for de forskellige markører i højder op til
    2.0 meter}
  \label{fig:detection1}
  \end{framed}
\end{table} 

\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{4,0cm}| p{3,0cm} }
    Højde & Simpel(88x72) & Semiavanceret(88x72)  & Avanceret(88x72)\\
    \hline
    0.5   & 1.0           & 0.615                 & 1.0 \\
    \hline
    1.0   & 1.0           & 0.065                 & 0.935 \\
    \hline 
    1.5   & 1.0           & 0.0                   & 0.015 \\
    \hline
    2.0   & 0.17          & 0.0                   & 0.015 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for de forskellige markører i højder op til
    2.0 meter når der anvendes 88x72 billeder}
  \label{fig:detection2}
  \end{framed}
\end{table} 


\subsection{Lokalisering via QR-koder} % Morten
\label{sec:QRdecoding}
I det følgende afsnit undersøges muligheden for at anvende QR-koder til
lokalisering. Man er begyndt at se QR-koder mange steder i det
offentlige rum, hvor de ofte bruges i forbindelse med
reklamekampagner. QR-koder bruges ofte til at indkode en webadresse,
men kan i princippet anvendes til at indkode en arbitrær
streng. QR-koder er blevet mere populære i takt med at opløsningen på
mobiltelefonkameraer er steget. De fleste moderne mobiltelefoner har
således et kamera med en opløsning flere gange højere end AR.Dronens 
kamera. 

Til dette eksperiment anvendtes bibliotekerne pyqrcode, 
\cite{electronic:pyqrcode}, og Zbar, 
\cite{electronic:zbar}, til henholdsvis
indkodning og afkodning af QR-koder. Zbar har pythonbindinger og anvendes også på
iPhone-platformen. Dette betød at vi kunne teste den genererede
QR-kode ved at scanne den med en iPhone.

\begin{framefig}
  \centering
  \includegraphics[width=0.35\textwidth]{grafik/woah.png}
  \caption{Teksten ``woah'' som QR-kode}
  \label{fig:qr_woah}
\end{framefig}

Formålet med eksperimentet var primært, at undersøge om AR.Dronens
kameras opløsning er høj nok til at afkode QR-koder. Til
formålet fremstilledes QR-koden der ses på figur
\ref{fig:qr_woah}. Den blev printet på et A4 ark og det blev
kontrolleret at koden kunne afkodes ved hjælp af en iPhone. Herefter
konstrueredes et lille testprogram der henter billeder fra AR.Dronens
frontkamera (320x240 pixels) og scanner disse for QR-koder ved hjælp af Zbar.

Afviklingen af eksperimentet foregik ved at QR-koden blev placeret på
en væg og AR.Dronen placeret i skiftende afstande til væggen. QR-koden
blev forsøgt afkodet på afstande op til en meter, men der var på intet
tidpunkt en succesfuld afkodning. IPhonen kunne samtidig 
afkode QR-koden på alle afstande op til en meter. Resultatet viser med
al tydelighed, at QR-koder ikke kan anvendes med AR.Dronens kamera og
at andre markører derfor må anvendes til lokalisering.


\section{Afstandsbedømmelse vha. triangulering}
\label{sec:afstand}
For at lette formidlingen af ideen bag dette eksperiment er det her
forsøgt at introducere problemstillingen via en lignende, men noget
simplere testopstilling, helt uden quadrokoptere.

Figur~\ref{fig:stranden} viser hvordan to mennesker på to forskellige
positioner langs en kyst kan udregne afstanden ud til et skib på
havet ved hjælp af triangulering.
\begin{framefig}
  \centering
  \includegraphics[width=0.85\textwidth]{grafik/strandloeve.png}
  \caption{To mennesker på en kyst kan i fælleskab bedømme afstanden
    ud til en båd på vandet, ved at kende afstanden imellem sig ($b$) og
    vinklerne imellem hinanden og båden (hhv. $a1$ og $a2$).}
  \label{fig:stranden}
\end{framefig}
Hvis $b$, $a1$ og $a2$ er kendt, så kan de to på stranden i fællesskab
udregne afstanden ud til båden. Afstanden imellem personen
bagerst og båden i figuren vil eksempelvis kunne udregnes ved 
hjælp af,~\cite{wiki:sinus}: $A1 = \frac{\sin(a2) * b}{
  \sin(180-(a2+a1)) }$.

Ideen med eksperimentet er så at få AR.Dronen til alene at agere de to
mennesker i eksemplet herover og derigennem finde afstande
til objekter foran frontkameraet. Hvis AR.Dronen sættes til at flyve
\emph{sidelæns} i et lige stykke, svarende til strandkanten herover, vil den 
til to tider $t1$ og $t2$ være ved to positioner $p1$ og $p2$. Hvis
tiden imellem de to tider er tilpas lille, eller hvis AR.Dronen flyver
tilpas langsomt, vil der være et overlap i billederne taget med
frontkameraet ud for de to positioner (figur~\ref{fig:optflowpyrlk}).

\begin{framefig}
  \centering
  \includegraphics[width=0.85\textwidth]{grafik/strand2.png}
  \caption{AR.Drone flyver sidelæns mens frontkameraet filmer
    væggen. Den grønne cirkel ($f2t$) er en genkendelig feature (uddybes i
    sektion~\ref{sec:sub:feature-tracking}) som optræder i begge
    billeder. Billederne er taget ud for position $p1$ og $p2$. Det er
    afstanden mellem 
    $f2t$ og $p2$, i planet, der ønskes fundet. Den grå pil i figuren viser
    AR.Dronens sidelæns flyveretning. De orange pile viser
    hvilken retning AR.Dronens frontkamera filmer.}
  \label{fig:stranden2}
\end{framefig}
I figur~\ref{fig:stranden2} får den genkendelige feature (i den grønne cirkel) den samme
rolle som båden i figur~\ref{fig:stranden}, mens AR.Drones to
positioner har menneskenes rolle. Kodeimplementationen er ikke
begrænset til kun at holde styr på afstanden til et enkelt 
punkt, men tager vare på en hel liste af features i billederne fra videosekvensen fra
AR.Dronens frontkamera. Fra hver feature er det muligt at udregne
afstande til AR.Dronens position. I sektionen herunder (sektion~\ref{sec:sub:feature-tracking}) beskrives den valgte
metode til at finde, genfinde og genkende features i en
videosekvens. I de næste to sektioner derefter (\ref{pixel2vinkel} og
\ref{yaw}) forsøges forklaret hvorledes vinklerne $a1$ og $a2$ i
figur~\ref{fig:stranden} udledes af pixel-positioner i billedet fra
frontkameraet, til brug i beregningen.
%
I sektion~\ref{liniestykkeb} forklares udregningen af liniestykket $b$
imellem de to positioner $p1$ og $p2$, for til sidst at kunne udregne
afstanden $|ft2\ p2|$ i sektion~\ref{afstandenf2tp2}.
Sektion~\ref{afstandresult} fremlægger resultatet af det eksperiment
der blev udført.


\subsection{Optisk feature-tracking}
\label{sec:sub:feature-tracking}
I det følgende bruges OpenCV-funktionerne \emph{goodFeaturesToTrack} og
\emph{calcOpticalFlowPyrLK}.
Programmet sættes til først at finde en liste af egnede
features\footnote{En egnet feature er et karakteristisk 
  punkt i billedet som kan genkendes selvom billedet flytter
  fokuspunkt. En feature kan være flere ting, bl.a. 2. ordens
  gradient i billedintensiteten over en hvis magnitude, hjørner på objekter,
  karakteristisk teksture, osv.} $(ft2_i)$ een gang i billedramme $0$ med 
\emph{goodFeaturesToTrack}. Hvorefter hver enkelt $f2t$-position
følges fra billedramme til billedramme med opticalflow-funktionen
\emph{calcOpticalFlowPyrLK}. 
%Når frontkameraet har skiftet så meget
%fokus at der ikke længere er overlap mellem det nuværende billede og
%det oprindelige billederamme0, foretages en ny indhenting af egnede
%features-to-track. Således at der altid er mindst $K$ antal f2t aktive ad gangen.
%
\begin{framefig}
  \centering
  \includegraphics[width=0.85\textwidth]{grafik/optflowpyrlk.png}
  \caption{Et typisk egnet good-feature-to-track er som den grønne
    ring på de to billedrammer: Et hjørne på en billedramme af et
    billede i billedet. Punktet bag den grønne cirkel i billedet til venstre er
    genfundet i billedet til højre vha. funktionen
    \emph{calcOpticalFlowPyrLK}. Funktionen returnere en liste med nye
    positioner for genkendte features (grønne cirkler).}
  \label{fig:optflowpyrlk}
\end{framefig}
%
%$K$ kan have forskellige værdier, men 15 har virket fornuftigt. Hvis
%AR.Dronen flyver hurtigt fremad falder features-to-track drastisk ud
%af billedrammen, og listen med features skal opdateres ofte. Jo højere $K$
%er, des oftere vil listen med features blive opdateret. Når AR.Dronen
%flyver langsomt sidelæns går der længere tid imellem hver tab af
%feature og $K$ kan sættes til en mindre værdi, for at opdatere sjælnere.

Konkret så følges en feature ved at den har en fast indgang i en liste der
returnes af \emph{calcOpticalFlowPyrLK}. Imellem hvert nyt billede
laves en ny beregning af optical flow, og feature-indgangene i listen opdateres med nye 
%$(x_i,y_i)$-
 pixelpositioner.


\subsection{Fra pixel-position til virkelig retning}
\label{pixel2vinkel}
Fordi AR.Dronen ikke altid peger præcis i retningen af det der
skal afstandbedømmes (oftest ligger den fulgte feature enten til venstre eller til højre for midten af
billedet, se de to billeder i figur~\ref{fig:optflowpyrlk}), er det
nødvendigt at udregne featurens retning i forhold AR.Dronens front.

Af en features horisontale pixel-position i billedet kan man udregne vinklen i forhold til retningen på AR.Dronens frontkamera, se
vinklen $a_{\Delta}$ i figur~\ref{fig:optflowpyrlk2}.
\begin{framefig}
  \centering
  \includegraphics[width=0.85\textwidth]{grafik/optflowpyrlk2.png}
  \caption{
    Bredden på billederne i videostrømmen fra AR.Dronens frontkamera
    er kendt til at være 320 px og kameraoptikken på AR.Dronens
    frontkamera er kendt til at have et horizontal udsyn på 92 $^{\circ}$. Hvis
    den horisontale afstand ($\Delta px$) i pixels mellem 2 punkter
    (midten og featuren) i et billede er kendt, kan denne
    viden tilsammen give vinklen $a_{\Delta}$, som $a_{\Delta} =
    \Delta px * \frac{92^{\circ}}{320 px}$.} 
  \label{fig:optflowpyrlk2}
\end{framefig}
%\begin{framefig}
%  \centering
%  \includegraphics[width=0.35\textwidth]{grafik/f2t.png}
%  \caption{Udregning af en features ($f2t_i$) vinkel i forhold til AR.Dronens
%    ligeud-retning. Vinklen beregnes udfra $x$-værdien $(x_{f2t i})$ af $f2t_i$' pixelposition.}
%  \label{fig:f2kvinkel}
%\end{framefig}
Vinklen $a_{\Delta}$ er dog kun orienteret i forhold til AR.Dronens
front, altså AR.Dronens lokale, interne, koordinatsystem. For at få en
retning ud som kan bruges til triangulering, er det nødvendigt at
flytte den over i et globalt koordinatsystem, så retninger og
positioner har samme referencepunkt og kan bruges i afstandsberegningen.

\subsection{Yaw som fælles reference}
\label{yaw}
Der er imidlertid et problem i at AR.Dronens front nærmest aldrig peger i den
samme retning (i figur~\ref{fig:absoluteevinkler} ses at AR.Dronens front
peger i forskellige retninger ved de to positioner $p1$ og $p2$). For
at triangulere skal der bruges to pejleretninger med fælles 
reference. Den piezo-elektriske yaw-sensor er ikke præcis nok til at
kunne udgøre det for kompas via dead-reckoning gennem en hel
flyvning. Tanken er istedet at yaw-værdien fra AR.Dronens navdata, over korte
tidsperioder, alligevel er stabil nok som fælles referencevinkel,
f.eks. mellem $p1$ og $p2$.
\begin{framefig}
  \centering
  \includegraphics[width=0.65\textwidth]{grafik/absolutevinkler.png}
  \caption{Opstillingen er set oppefra. AR.Dronen har flyttet sig fra
    position $p1$ til $p2$ i planet. Den grønne cirkel (f2t) er et punkt på
    væggen (feature-to-track) som følges gennem
    frontkameraet. Vinklerne $a1$ og $a2$ er orienteret i forhold til
    samme referencepunkt, ved at bruge AR.Dronens yaw-sensor.}
  \label{fig:absoluteevinkler}
\end{framefig}

\subsection{Sidestykket b, 3. parameter i trekanten} %Triangulering i 2
                                %dimensioner}
\label{liniestykkeb}
Der mangler den sidste parameter ud af 3 for at kunne bestemme
alle de andre elementer i trekanten $|p1\ f2t\ p2|$, heriblandt
afstanden $|f2t\ p2|$. Den sidste ukendte parameter længden $|p1\ p2|$
(se figur~\ref{fig:linieb})
er lidt besværlig at fremskaffe idet der ikke findes andre afstande at
måle udfra. Til gengæld kendes hastigheden som AR.Dronen har fløjet
med imellem $p1$ til $p2$. Den sideværts hastighed fåes fra AR.Dronens navdata 
som $v_y$, og er hastigheden i y-aksens bevægelsesretning
($v_{\leftrightarrow}$ i figur~\ref{fig:linieb}).
%( hhv. $v_{frem-tilbage}$ og $v_{venstre-højre}$ i figur~\ref{fig:linieb}).

Testopstillingen er forsøgt simplificeret til kun at foregå i den
sideværts bevægelsesretning. % planet, så det kun er frem-, tilbage- og sideværts- bevægelser der
Frem- og tilbage bevægelser, rotationer og ændringer i højden forsøges minimeret under
flyvningen imellem $p1$ og $p2$. Det antages herved at de parametre er så små
at de kan ignoreres i udledningen af afstanden imellem $p1$ og $p2$.
\begin{framefig}
  \centering
  \includegraphics[width=0.65\textwidth]{grafik/linieb.png}
  \caption{Opstillingen er set oppefra. AR.Dronen har flyttet sig fra
    position $p1$ til $p2$ i planet på tiden $\Delta t$. Her ønskes
    sidelængden $b = |p1\ p2|$ fundet. Afstanden $b$ findes ved:
    $b = \Delta t * v_{\leftrightarrow}$.}
  \label{fig:linieb}
\end{framefig}

\subsection{Afstanden ud til objektet findes}
\label{afstandenf2tp2}
Det er sidestykket $|f2t\ p2|$ i trekanten på
figur~\ref{fig:opticalafstand} der er forsøgt udregnet. Eftersom der
nu er fremskaffet tre elementer af trekanten $|p1\ f2t\ p2|$'s vinkler og
sider, er det nu muligt at beregne afstanden $|p2\ f2t|$.
\begin{framefig}
  \centering
  \includegraphics[width=0.65\textwidth]{grafik/optical_afstand.png}
  \caption{Opstillingen er set oppefra. AR.Dronen har flyttet sig fra
    position $p1$ til $p2$ i planet på tiden
    $\Delta t$. Den grønne cirkel (f2t) er et punkt på
    væggen (feature-to-track) som følges gennem frontkameraet. I
    opstillingen ønskes liniestykket $|f2t\ p2|$ fundet. Ved at kende
    vinklerne $a1$ og $a2$, samt længden $b = |p1\ p2|$, er det nemt at
    finde den sidste afstand som: $|f2t\ p2| = \frac{\sin(a2) * b}{
    \sin(180-(a2+a1)) }$}
  \label{fig:opticalafstand}
\end{framefig}

\subsection{Resultater}
\label{afstandresult}
\begin{framefig}
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/afstande_avg11_gauss11_thumb.png}
  \caption{Den beregnede afstand ud til et fast punkt på væggen i en filmsekvens over
    290 billedrammer, hvor AR.dronen flyver langsomt sidelæns ca. 5
    meter fra punktet. Den gule og blå streg er optegnet i hånden for
    tydelighed.
    Den gule og blå streg er hhv. gennemsnittet og
    gaussfiltreret over et vindue med 11 målinger. Enheden for
    $y$-aksen er millimeter, mens $x$-aksen er billedrammenummeret.}
  \label{fig:afstandavg11}
\end{framefig}
Resultatet af eksperimentet kan ses i figur~\ref{fig:afstandavg11}. De
16 vildeste outsidere i målingerne er klippet af ved 27 meter. Den
sande værdi (afstanden mellem AR.Dronen og væggen) ligger imellem $4,0$ og $6,5$ meter.

Dette er et godt eksempel på et af de tidspunkter hvor man bør træde et skridt
tilbage og se situationen fra et andet perspektiv. Grunden til at det
ikke virker som opsætningen er nu, er fordi vinklen der skal
trianguleres udfra bliver for spids. Liniestykket imellem $p1$ og $p2$ er for kort. Det er meget
begrænset hvor langt AR.Dronen kan nå at flyve mellem 2
billedopdateringer. Selv hvis der kun er 10 billedopdateringer per sekund, så vil
$\Delta t$ være lig $0,1$ sekunder. Hvis AR.Dronen bevæger sig med en
hastigheden af eksempelvis $100$ millimeter per sekund, så er
afstanden mellem de to punkter $100\ mm/s * 0,1\ s = 10\ mm$. Hvis de 2
andre sidelængder så er $5\ m$, altså en faktor 500 gange større, er det nemt at se
hvad lidt varians vil have af indflydelse på kvaliteten af
afstandsbedømmelsen.

En udbygning af afstandsbedømmelsen vil være at lade $\Delta t$ vokse
lidt, således at afstanden udregnes hen over flere billeder ad gangen.
Et andet forsøg kunne være at udbygge implementationen til at
projektere featurens horisontale afstand i billedrammen ned på
virkelighedens vandret ved at bruge $\phi$-vinklen fra AR.Dronens navdata.


\section{Korridor-detektering} % Thomas
\label{sec:korridor}
%intro
Denne sektion omhandler en metode til at detektere gangarealer og
korridorer gennem frontkameraet på AR.Dronen. Målet for dette
eksperiment er at implementere og teste to 
ting: 1) At finde ud af om AR.Dronen er i en korridor, og
2) At finde ud af hvor i et billede enden af korridoren (forsvindingspunktet) er.

%mere intr, !det kan bruges til!
For en autonom AR.Drone er det hensigtsmæssigt at have kontekstuel viden for at kunne
træffe funderede beslutninger om navigationen i specifikke
omgivelser. Derfor vil det være interessant at se om det er muligt at
implementere en korridor-detektions funktionalitet som senere kan
blive en del af det virtuelle sensor hieraki.
Robotten Minerva, \cite{Thrun99minerva:a}, fik lov at lave guidede
tours på the Smithsonian i to 
uger. I eksperimentet var robotten blevet givet forskellige
opførsler at udfører under forkellige forhold. Eksempelvis skulle den
så ofte som muligt bruge analogien Coastal-planner, dvs. at følge
væggene rundt så den havde mulighed for at genkende allerede gemte
kendemærker. Når Coastal-planner ikke var muligt skulle den gå over i
Open-sea mode, hvor den så havde et andet bevæggrundlag.
Et andet eksempel på forskellige operationstilstande er i,
\cite{single_img_pespective_cues}, hvor en quadrokopter, med
frontkamera, med godt resultat kan kategorisere dens omgivelser til at
være i en trappeopgang, i et gangareal eller i et lille kontor. I
modsætning til Minerva bruger denne tilgang ikke 2D kort eller
omgivelser til pathplanning. Viden om omgivelserne bruge til at
indsnævre de optiske strukture (perspective cues) der skal søges og
orienteres efter.

\begin{framefig}
  \centering
  \includegraphics[width=0.59\textwidth]{grafik/korridoreksempel2.png}
  \caption{Kendetegnende for et billede der er taget ned ad et
    gangareal er forsvindingspunktet, som består af en del diagonale
    linier som forsvinder ind i billedet og skære cirka i det samme punkt i billedet.}
  \label{fig:korridoreksempel}
\end{framefig}

Eksistensen af et forsvindingspunkt i billedet fra AR.Dronens
frontkamera kan bruges som indikator for om AR.Dronen befinder sig i
en korridor.
%
Hvis et billede viser en korridor vil man i strukturen i billedet
kunne se mange linier gå vandret og lodret, mens en del linier vil gå
diagonalt i billedet, altså ind i korridoren. Gennem dette
eksperiment har det vist sig, at der hvor de diagonale linier skærer
hinanden er der ofte et sammenfald med enden af korridoren i billedet. 

\subsection{Korridordetektering som procesdiagram}
Processen med at finde et forsvindingspunkt handler om at finde
skæringspunktet af de diagonale linier som går ind i billedet.
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/korridorvandfald.png}
  \caption{Detekteringsprocessen tager et billede fra
    AR.Dronens frontkamera som input. Billedet filtreres for at finde
    Houghtransformerede linier. De fundne linier sorteres så kun de
    diagonale linier er tilbage. Fordelingen af linieskæringspunkter i
  billedet beregnes. Den celle eller det område af billedet med størst
  koncentration af linieskæringer udpeges som forsvindingspunkt.}
  \label{fig:korridorvandfald}
\end{framefig}
Forsvindingspunktets placering i et billede findes gennem en række
deltrin (se figur~\ref{fig:korridorvandfald}). Inputbilledet roteres
først ift. AR.Dronens roll-vinkel, $\phi$ (se
figur~\ref{fig:korridorphi}), herefter beregnes linierne i billedet
ved hjælp af et Canny-filter,~\cite{Canny1986}, og
Houghtransformation,~\cite{duda1972houghline} (gennemgåes i
sektion~\ref{sec:introCannyHough}).
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/korridorphi.png}
  \caption{For senere at lette frasorteringen af de vandrette og
    lodrette linier fra i den reele korridor, bruges AR.Dronens roll-vinkel $\phi$. Herefter kan linier filtrerers
    via deres hældning alene.}
  \label{fig:korridorphi}
\end{framefig}
De ikke-diagonale linier frasorteres, hvorefter skæringspunker mellem
hvert par af de diagonale linier beregnes. Antallet af linieskæringer
tælles op for hver celle i billedet (der er $11 \times 11$ celler per
billede, se figur~\ref{fig:phiaugmentet}).
Afhængig af om der er en korridor eller ikke, er det som sagt oftest
ved cellen med størst antal linieskæringer at forsvindingspunktet kan
findes. Hvorvidt der er tale om et billede af en korridor kan ofte
afgøres ved at kigge på om antallet af linieskæringer i cellen med
flest skæringer er over en vis størrelse (en grænse på 140 har vist
godt resultat).
%
Herunder gennemgåes metoder til at finde kanter (edges) via
Canny-filter, samt hvorledes disse kanter går fra at være
repræsenteret som klumper af billedpunkter til at blive til
Houghtransformerede linier (der er en uddybende forklaring af
teknikken i appendiks~\ref{app:linier}). Herefter forklares hvordan
linierne filtreres for til sidst at sige noget om et tilstedeværende
eller ikke tilstedeværende forsvindingspunkt.

\subsection{At finde linier i billedet}
\label{sec:introCannyHough}
Det at finde linier foregår i to skridt via to forskellige
algoritmer. I dette eksperiment er open source computer vision
bibliotekets, OpenCV,~\cite{opencv:main}, implementation af et
Canny-filter,~\cite{Canny1986}, brugt til at finde kanter med. Udfra
kanterne er der brugt en OpenCV implementation af
Hough-transformation,~\cite{duda1972houghline}, til at finde linier
med. Der er lavet en mere detaljeret beskrivelse af metoderne til at
detektere kanter og linier i appendiks~\ref{app:linier}.

Figuren~\ref{fig:houghprocess} illustrerer forskellige trin i
filtreringsprocessen fra råt billede til en samling af diagonale
linier på polær form.

\begin{framefig}
  \subfigure[Et billede af en gang påført gauss-sløring]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_a.png}
    \label{fig:vanishopa}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Billede a efter Canny kantdetektering]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_b.png}
    \label{fig:vanishopb}}

  \subfigure[Houghtransformerede linier, udregnes udfra kantpunkterne i billede b]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_c.png}
    \label{fig:vanishopc}}
  ~ 
  \subfigure[Vandrette og lodrette hough-linier sorteres fra billede c]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_d.png}
    \label{fig:vanishopd}}

  \caption{Detektering af Hough-linier.}
  \label{fig:houghprocess}
\end{framefig}

% \subsection{Augmentering med $\phi$}
% Hver af de fundne linier har en hældning som herefter sammenlignes med
% grænseværdier opsat omkring variabelværdien $\phi$. Frasorteringen
% gennem grænseværdierne gør at problemet der søges løst kan forenkles
% til kun at omhandle det at finde området med flest
% linie-skæringspunkter blandt de diagonale linier i billedet. På
% figur~\ref{fig:phiaugmentet} kan man se $\phi$-vinklen augmenteret på
% billedet. Den tykke blå-farvede linie indikere det vandrette plan i
% billedet. Linien er en funktione af $\phi$.

% %% \subsection{Augmentering med $\phi$}
% %% Hver af de fundne linier har en hældning som herefter sammenlignes med
% %% grænseværdier opsat omkring variabelværdien $\phi$. Frasorteringen
% %% gennem grænseværdierne gør at problemet der søges løst kan forenkles
% %% til kun at omhandle det at finde området med flest
% %% linie-skæringspunkter blandt de diagonale linier i
% %% billedet. På figur~\ref{fig:phiaugmentet} kan man se $\phi$-vinklen
% %% augmenteret på billedet. Den tykke blå-farvede linie indikere det
% %% vandrette plan i billedet. Linien er en funktione af $\phi$.

\begin{framefig}
  \centering
  \subfigure[Gangareal Ada-bygningen, Katrinebjerg]{
    \includegraphics[width=0.32\textwidth]{grafik/out2.png}
    \label{fig:out2}}
  %~
  \subfigure[Billedet er opdelt i $11 \times 11$ celler. I hver celle
    gemmes antallet af linier som skærer hinanden. I dette tilfælde er
    cellen med flest skæringer godt overensstemmende med korridorens
    forsvindingspunkt i billedet.]{
    \includegraphics[width=0.6\textwidth]{grafik/out_put_2jpg.png}
    \label{fig:phiaugmentet}}

  \caption{Visualisering af vanishinpoint detektor.}
  \label{fig:linieceller}
\end{framefig}

\subsection{Celle med flest linie-skæringspunkter}
Ved at finde det område af billedet hvor flest linier skærer
hinanden, kan man under de rigtige forhold få et godt gæt på hvor
billedets forsvindingspunkt kan findes. I figur~\ref{fig:phiaugmentet}
ses ved området for celleindgang (5,5) 1307 linie-skæringer: En god
indikator på et forsvindingspunkt. 
%
Observationer under eksperimenterne har vist, at cellen med flest
skæringspunkter skal indeholde et hvis antal for at algoritmen med
rimelig sikkerhed kan sige at der faktisk er en korridor med
tilhørende forsvindingspunkt. 


\subsection{Resultater}
Billedet a i figur~\ref{fig:linieceller} er et eksempel på et billede med optimale
forhold til at finde et forsvindingspunkt. 

Ud af de 111 detekteringstest der er lavet, hvor korridorenden ses i midten af et billede, hvor billedet
er taget fra midten og ned ad Ada 100-gangen, finder detektoren
området for forsvindingspunktet 86 gange. Udover de 86 er der 11
tilfælde hvor detekteringen ikke virker. Et eksempel er hvis billedet
er taget i den ene side af gangen, hvor Canny-filteret har detekteret mange næsten lodrette linier, men ikke lodret nok til at blive frasorteret
via $\phi$-grænsen. Der vil der blive et stort antal linieskæringer i
siden som vil føre til fejl-placering af forsvindingspunktet (se figuren~\ref{fig:vanishfeler}).

\begin{framefig}
  \centering
  \includegraphics[width=0.96\textwidth]{grafik/vanishresultgraf.png}
  \caption{Testen har ialt gennemløbet 111 billeder. 11 af de 111 er
    med det blotte øje vurderet til at være uden korridor (f.eks
    billeder af dørkarm, eller et vindue i et kontor). Ud af de
    resterende 100 billeder indeholdende korridor-ender, har detektoren
    været i stand til at finde forsvindingspunkter i de 89 af
    billederne. De resterende 11 billeder kategoriserede detektoren
    som værende uden forsvindingspunkt, selvom der med det blotte øje
    kunne ses en korridor ende omkring midten af billedet.}
  \label{fig:vanishresult}
\end{framefig}

\begin{framefig}
  \centering
  \includegraphics[width=0.8\textwidth]{grafik/out_put_6jpg_error.png}
  \caption{Eksempel på synsvinkel hvor detektoren ikke fungerer
    optimalt. Canny-filteret har fundet mange kantpunkter i venstre
    side af billedet hvor vægoverfladen er meget uensartet. Hvilket
    det er umuligt for Houghtranformeringen at danne linier udfra som
    der ikke er fejl på.}
  \label{fig:vanishfeler}
\end{framefig}
%
Oprindeligt blev der opsat et mål om at
detektoren skulle kunne give en sandsynlighed for hvorvidt AR.Dronen befandt
sig i en korridor. Det mål er ikke opfyldt.
%
Med de opstillinger der blev lavet test med, er der til gengæld ikke observeret
nogle false positives, dvs. detektoren har ikke fejlagtigt 
vist at der var en korridor i de test hvor der ikke var en korridor.
I den opstilling der er lavet test med har detektoren konkret gættet
rigtigt 100 gange ud af 111 (se figuren~\ref{fig:vanishresult}).

Observationer har indikeret at detektoren kan gøres mere robust ved
yderligere at analysere fordelingen af linieskæringspunkterne i
billedet. Skærringspunkterne skal gerne være jævnt fordelt over det
meste af billedet, gerne i et diagonalt kryds over
forsvindingspunkt som i figur~\ref{fig:phiaugmentet}. Eksempelvis bør
fordelingen af skæringspunkter ikke kun være koncentreret omkring en
enkelt linie som ifigur~\ref{fig:korridorwrong} eller kun omkring et
enkelt område. 
\begin{framefig}
  \centering
  \includegraphics[width=0.8\textwidth]{grafik/korridorwrong.png}
  \caption{Et sted hvor detektoren ikke skal finde en korridor.}
  \label{fig:korridorwrong}
\end{framefig}

En videreudvikling kunne være at udbygge detektoren til at huske mere
end fra bare en billedramme til den næste, evt. ved at parre
placeringen af korridorens endepunkt med en $\psi$-værdi fra AR.Dronens yawvinkel, som hele tiden
opdateres også når AR.Dronens front drejer rundt og står på tværs af
korridoren, således at endepunktet senere kan genfindes.
 
Implementationen er en simplificeret løsning der virker i de lette
tilfælde og er knap så resoucekrævende en tilgang end den
af, \cite{single_img_pespective_cues}, hvor der blandt andet bruges en
Hidden markov model og Viterby-algoritme til at huske hvor korridorenden sidst blev set og om
det så er rimeligt at tro at den er der hvor detektoren siger den er
nu.

% Implementationen er en simplificeret løsning der virker i de lette
% tilfælde og er knap så resoucekrævende, som den tilgang der beskrives
% af Bills,  Chen og Saxena, \cite{single_img_pespective_cues}. Her
% anvendes der blandt andet en Hidden markov model og Viterby-algoritme
% til at huske hvor korridorenden sidst blev set. Så kan det vurderes om
% det er rimeligt at tro, at forsvindingspuktet er der hvor detektoren
% siger det er nu.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % % %

\section{Odometrisk afstandsmåling} % Thomas
\label{sec:distance-mesu} 
%Hvad vil vi vise, afvise teste?  
%Hvadkan/skal resultatet bruges til?  

Dette eksperiment undersøger virkningen af en odometrisk
afstandsmåler. Den odometriske afstandsmåler er implementeret i den
virtuelle sensor DistanceTracker,~\cite{github:virtuel}, og holder
styr på den afstand AR.Dronen har bevæget sig i løbet af en
flyvning. Konceptet er kendt som en triptæller i biler og som en
tacho-counter i motoren på kørende robotter.  

Indenfor navigation anvendes en teknik kaldet dead
reckoning,~\cite{wiki:deadreck}. Dead reckoning-teknikken går ud på at den
nuværende position udregnes på baggrund af en tidligere
positionsudregning og viden om retning og hastighed. En mulig
anvendelse af odemetri-afstandsmåleren kan således være, som en del af en
sådan dead reckoning algoritme. En forudsætning for en robust og brugbar dead
reckoning er dog, at den udledte afstand er troværdig. Præcisionen er
noget af det der testet her.
%Præcisionen undersøges herunder.

DistanceTrackeren fungerer ved at integrere over de
hastighedsestimater ($v_x$ og $v_y$), der modtages i AR.Dronens navdatastrøm. Disse
hastighedsestimater er baseret på en kombination af accelerometerdata
og en avanceret hastighedsudledning fra
bundkameraet,~\cite{velocity_estimation_inside_the_drone}.   

\subsection{Eksperimentet}
Eksperimentet blev udført ved at placere to markører med 3 meters
afstand på gulvet. Herefter blev der gennemført 3 forskellige typer
flyvninger mellem de to markører: Manuel flyvning med ensartet
underlag (uden gode features), automatisk flyvning
med ensartet underlag og automatisk flyvning med uensartet underlag (med gode
features at tracke). Hver type flyvning blev gentaget 10
gange. Den manuelle styring forgik ved brug af joystick, mens de
automatiserede flyvninger blev udført ved brug af en
FollowTourTask
(sektion~\ref{sssec:followtourtask}).~\cite{electronic:youtube3}
(youtube video) viser en flyvning med en FollowTourTask, hvor den
tilbagelagte afstand måles.   

\subsection{Resultater}
Generelt blev de
manuelt styrede flyvninger gennemført med højere 
hastighed end de automatiserede, mens de automatiserede flyvninger i
reglen tog længere tid at færdiggøre. Den længere flyvetid kommer
af at odometri-afstandsmåleren afvikles sammen med markør-detektoren (gennemgået i sektion~\ref{ssec:avancemarkoer})
som skal bruge ekstra tid på at udføre markør-detekterering i FollowTourTasken som
brugt i eksperimentet.

Resultaterne fra flyvningerne kan ses på figur~\ref{fig:xdistr} og
figur~\ref{fig:ydistr}. Figur~\ref{fig:xdistr} viser en fordeling over
de målte afstanden i fremadrettet flyveretning, mens figur~\ref{fig:ydistr} viser
fordelingen af de målte afstande i y-aksens (sideværts) retning.

\begin{framefig}
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/3mxdistribution.png}
  \caption{Kumulativ fordeling af målte afstande. Målingerne blev
    foretaget under fremadrettet flyvning over 3000 millimeter. Der
    blev udført manuelle og automatiserede flyvninger over ensartet og 
    uensartet overflade. Grøn er manuel flyvning, mens blå og rød er autonom flyvning.}
  \label{fig:xdistr}
\end{framefig}
Af figur~\ref{fig:xdistr} (og figur~\ref{fig:ydistr}) observeres det, at medianen af de målte afstande under
de manuelt styrede flyvninger (grøn graf) ligger tydeligt lavere end
medianen af afstande målt under de autonome flyvninger.

\begin{framefig}
  \centering
  \includegraphics[width=0.91\textwidth]{grafik/3mydistribution.png}
  \caption{Kumulativ distribution af målte afstande. Målingerne
    repræsenterer AR.Dronens tilbagelagte afstand på tværs af
    flyveretningen og burde altid være lav, da AR.Dronen kun blev
    fløjet fremad og ikke til siderne. Grøn er manuel flyvning, mens
    blå og rød er autonom flyvning.}
  \label{fig:ydistr}
\end{framefig}
En anden observation af eksperimentet og graferne herover er at
fordelingen af de målte afstande ikke er distribueret omkring den sande
værdi på 3000 mm, men generelt er fordelt omkring lavere værdier. Der
er to mulige forklaringer på observationen: Enten er tidsfaktoren som
der integreres over for lille, eller også er hastighedsudledningen fra
AR.Dronens navdata generelt for lav. Hvis AR.Dronen sættes til altid
kun at flyve med en og samme hastighed, kan problemet løses ved at bruge en
større tidsfaktor i integrationen af hastigheden. En mere
hensigtsmæssig og mere interessant løsning vil være at undersøge om
hastigheden fra AR.Dronen skal kompenseres for med både en offsetfaktor og en
hældningsfaktor i forhold til den reele værdi.

%perspektiver, hvad skal / kan vi bruge den nye viden til?
%Indledende øvelse til at kunne måle afstande frontalt via triangulering <- optisk flow


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Konklusion - section

\chapter{Konklusion}
%Rodney Brooks at udnytte det at det er en mobil robot, bekræft eller afkræft
%antagelser om model af miljø ved at flyt rundt og observer fra forskellige vinkler/synspunkter,
%elephant?
I det følgende konkluderes på specialet tilstand målt i forhold til
tese- og mål-formuleringen. Herefter evalueres på valget af specialets
metode. Til sidst præsenteres løsningsforslag i forhold
til det arbejde der ikke blev nået at færdiggøre samt forslag til
alternative fremtidige eksperimenter og metoder.
%I sektionen~\ref{sec:som} nævnes

\section{Specialets tilstand}
\subsection*{1. Konstruktion af klientplatform} 
%Klientplatformen har været det primære arbejdsredskab under
%ekperimenterne med AR.Dronen. 
% på trods af kontinuerlig / samtidig med
Undervejs i hele specialeforløbet har den konstruerede platform
understøttet arbejdet med AR.Dronen og har fungeret som en let
anvendelig grænseflade til AR.Dronen.

\subsubsection*{1.(a) Facilitere eksperimenter}
På trods af kontinuerlig udvikling under hele projektet,
har klientplatformen understøttet eksperimenterne ved at give let
adgang til AR.Dronens sensorstrømme og ved at tilbyde
task-abstraktionen ovenpå kontrolinterfacet. Det udviklede tasksystem
har været så fleksibelt, at det både har kunnet anvendes til simple,
tidsbegrænsede flyvninger i en specifik retning og til sammensatte
navigationsmønstre med genkendelse af gulvmarkører.

Tasksystemets træstruktur gør det muligt at kombinere tasks i
henholdsvis sekventielle og parallele grupper. Strukturen giver
anledning til et forholdsvis kompliceret tasksystem, som ofte kunne
være implementeret i et fladt hieraki med samme resultat. Fordelen ved
strukturen i tasksystemet er at en forholdsvis kompleks task som
FollowTourTask kan sammensættes af flere mindre og ikke så komplekse
subtasks. Dermed åbnes samtidig op for at udnytte muligheden for
genbrug af disse subtasks. En vigtig pointe er at træstrukturen også
giver mulighed for grafiske repræsentationer af komplicerede tasks,
der giver et bedre overblik over den implenterede funktionalitet.

\subsubsection*{1.(b) Tilgængeliggøre et interface til AR.Dronen}
Eksemplerne med let anvendelse af pythoninterfacet i
sektion~\ref{sec:platformsadgang} viser, at
klientplatformen, og delelementer af denne, er nem at anvende for
udviklere. Det implementerede testdevice giver desuden mulighed for
udvikling gennem simulering uden en AR.Drone tilstede.

\subsection*{2. Semi-autonom navigation}
Klientplatformen har under eksperimenterne med visuel lokalisering
vist (sektion~\ref{ssec:visuellokalisering}), at den muliggør
semiautonom navigation i et miljø med visuelle markører. Semi-autonom
navigation defineres her som, at AR.Dronen er i stand til at følge en
på forhånd planlagt rute. Denne funktionalitet implementeres af
FollowTourTasken,~\ref{sssec:followtourtask}, som kan læse og følge en
rute. En mere avanceret opførsel der også lader AR.Dronen undgå
mennesker i synsfeltet, kan opnåes ved at afvikle en
AvoidTask,~\ref{sssec:avoidtask}, sideløbende med FollowTourTasken. En
sikker navigation kan opnåes når markørerne ligger forholdsvis tæt på
hinanden (afstand < 3 meter) og positionsdetektoren er kalibreret til
lysforholdene. Under disse forhold, er FollowTourTasken blevet
afprøvet med gode resultater på en række forskellige ruter(i
forbindelse med eksperimenterne).

\subsection*{3. Udledning af yderligere navigationsdata}
Udledning af yderligere højere-ordens navigationsdata fra
AR.Dronenssensorstrømme har været et hovedtema i de udførte
eksperimenter. Herunder konkluderes specialets  opsatte delmål:

\subsection*{3.(a) Lokalisering via WIFI}
Eksperimenter der undersøgte anvendeligheden af WIFI-fingerprinting
til lokalisering i sektion~\ref{ssec:wifilokalisering}, viste at det
var muligt at lokalisere AR.Dronen til at være i f.eks den ene eller
den anden ende af en bygning (lokationer med 6 m afstand). 
%
Der er teknikker til at forbedre resultaterne, men teknikken er meget
følsom og kræver et større eksperiment-setup end vi har haft mulighed
for. En forbedring på dette område vil samtidig knytte navigationen
tættere op på at skulle gemme og opdatere en repræsentation af
omverdenen, hvilket generelt er søgt undgået i dette speciale.

Det forudgående arbejde med at få tilkoblet en USB-WIFI-adapter
til AR.Dronen, har dog vist, at det generelt er muligt at udvide
AR.Dronen med USB-enheder.

\subsection*{3.(b) Lokalisering via visuelle markører}
En stor del af de udførte eksperimenter har omhandlet visuel
lokalisering på forskellige måder. Under denne process er der udviklet
en visuel markør, figur~\ref{fig:markers}, som konsistent kan
genkendes med AR.Dronens bundkamera. Markøren og FollowTourTasken, som
er udviklet sideløbende, muliggør AR.Dronens navigation efter en
planlagt rute.

\subsection*{3.(c) Detektering af navigationsmiljøtype}
Eksperimenterne med korridordetektion, sektion~\ref{sec:korridor}, har
vist meget lovende muligheder, både for at kunne detektere at
AR.Dronen faktisk befinder sig i en korridor og for at kunne navigere via
en sådan korridor.
% Korridordetektoren er oftest i stand til at
%detektere en tilstedeværende korridor i billederne fra AR.Dronens
%frontkamera og i de tilfælde detekteringen ikke sker, er det på grund
%af faktorer der kan tages højde for. \todo{omformuler kan tag højde for...}

\subsection*{3.(d) Odometrisk afstandsmåling}
Eksperimenterne med odometrisk afstandsmåling (sektion~\ref{sec:distance-mesu})
har vist, at denne teknik kan anvendes til grove estimater for
AR.Dronens tilbagelagte afstand. Resultaterne viser at den målte
afstand er meget følsom overfor forskellige miljøfaktorer som
belysning, overflade aftegninger i gulvet der flyves over og den 
hastighed der flyves med. En dead-reckoningalgoritme, kun baseret på
denne afstand og en retning, vil ikke være pålidelig, men afstanden
kunne f.eks. anvendes til at visualisere AR.Dronens omtrentlige
position på et digitalt oversigtskort evt. kombineret med lokation via
WIFI.

\subsection*{3.(e) Optisk triangulering af afstande}
På trods af at resultaterne fra eksperimentopsætningen med
afstandstriangulering (sektion~\ref{sec:afstand}) ikke var overbevisende, så ser det alligevel ud som
om at det er muligt, at forfine teknikken og producere bedre
resultater. Teoretisk er der adgang til alle de nødvendige parametre i
trianguleringsberegningen og mulige forbedringer til teknikken
beskrives sidst i sektionen. 

\section{Metodens anvendelighed}
Metodevalget har taget udgangspunkt i AR.Dronen og er samtidig
begrænset af AR.Dronen. De eksperimenter der er udført i specialet er
udvalgt udfra hvad der umiddelbart synes at ligge ligefor at teste,
ved at kigge på AR.Dronens specifikationer og egenskaber. 
%
Eksperimenterne er lavet for at teste ideer til udbygning af AR.Dronen
som robot udfra AR.Dronens umiddelbare formåen som fjernstyret
quadrokopter.% godt ?
%Eksperimenterne er valgt, %specifik til at teste AR.Dronens formåen, oplagte muligheder 

\subsection*{Vurdering af AR.Dronen som robotplatform}
I sektion~\ref{ssec:dronerecap} er vurderingen af AR.Dronen
beskrevet. Generelt vurderes AR.Dronen som et godt alternativ til at
bygge en quadrokopter-platform fra bunden. 
%
Selvom AR.Dronen var valgt på forhånd i specialet, og eksperimenterne
er opstået ud af den. Så har AR.Dronen også været en begrænsende
faktor i specialet, på godt og ondt. AR.Dronen har været god hjælp til at
afgrænse domænet for specialet, men de indbyggede sensorer har begrænset udviklingsmulighederne: %hvor?
%både for semi-autonom navigation og udledning af features. 
Eksempelvis udelukker den begrænsede videoopløsning afkodning af
QR-koder til brug i lokaliseringseksperimentet
(sektion~\ref{sec:QRdecoding}), og bestemmelse af AR.Dronens
orientering (kompas) besværliggøres på grund af sensordrift. En mulig
løsning kunne være at gøre brug af de ny AR.Drone version
2.0, hvorpå der er udskiftet en del sensore og installeret nye
(frontkamera med større opløsning og magnetometer-sensor).
%
En anden mulighed var måske selv at udvide AR.Dronen med andre sensorer
som beskrevet i sektion~\ref{sec:udvidelse}. I starten af specialet
blev der nævnt ideer til at montere gennem serialforbindelsen,
f.eks. montering af IR-lys eller laser til afstandsmåling. Argumentet
for ikke at gøre det var, at nu var der lavet et proof-of-concept med
installationen af WIFI-USB-adapteren (sektion~\ref{sec:wifisensorudvidelse}).


\subsection*{Klientplatform}
Valget af Python som implementationssprog til klientplatformen har
haft både positive og negative konsekvenser. Fra et Performance-synspunkt har
Python ikke været et godt valg. Enkelte gange med den trådbaserede
taskimplementation, har systemet været presset til det punkt, hvor en
flydende afvikling ikke længere var mulig. En sidegevinst har
imidlertid været at det har fordret et fokus på at skrive mere
effektiv kode, især i forbindelse med udviklingen af
taskssystemet. For at kombinere hastigheden fra 
sprog som C og C++ med udviklingshastigheden og letlæseligheden fra
Python, vil en mulighed være at implementere et eller
flere af klientplatformens moduler i C/C++ og så have bygget
python-wrappere til disse moduler.

Såfremt det ikke havde været et mål i sig selv at implementere en
platform, var det en mulighed at anvende ROS (Robot Operating
System),~\cite{wiki:ros}. ROS er et mere generelt framework til
robotudvikling og har i forvejen en driver udviklet til
AR.Dronen,~\cite{brown:ros}. 

\subsection*{OpenCV}
Anvendelsen af OpenCVs billedbehandlingsalgoritmer har været en
nødvendighed. Det vil ikke have været muligt at lave beregningerne
på det samme antal billedrammer, hvis algoritmerne var forsøgt
implementeret i Python. Anvendelsen af OpenCVs Python interface har
været relativt nemt at tilgå, på trods af at det få steder ikke er
lige klart hvilken version af en implementeret algoritme bruger
hvilken parametertyper (billede-argumenterne repræsenteres som
IPlImage-type i en version og som et array i en anden
version. Samtidig er ikke alle dele af biblioteket lige opdateret).

%\section{Hvad ville vi gerne have nået}
\section{Fremtidigt arbejde}
Det er oplagt at undersøge alternativer til anvendelsen af Psyco i
forbindelse med billeddekodning. En mulighed kunne være at
implementere hele dekodningsalgoritmen i C og så lave python wrappere
hertil, en anden mulighed kunne være at anvende eller tilpasse en
allerede fungerende JPEG afkoder.

Det har været planen at udvide sensorhierakiet med flere virtuelle
sensorer. Efter de lovende resultater med korridordetektoren, kunne denne
algoritme med fordel implementeres i klientplatformen som en virtuel
sensor.
Udledning af AR.Dronens retning ved hjælp af den udviklede markør er
en forbedring, som kan opnåes med forholdsvis simple midler. 

Som vist er der muligt at udvide AR.Dronen med USB-moduler, det vil
være nærliggende at undersøge denne tilgang yderligere, ved at montere
andre sensorer til at supplere med udledningen af mere højere-ordens
navigationsdata. Specielt hvis AR.Dronen skal flyve indendørs blandt
mennesker vil det øge sikkerheden at montere afstandssensor rundt på
AR.Dronen. 
%udvide med flere/andre USB-moduler, IR-afstand...

%optisk triangulering teste om det
%faktisk kan lade sig føre indikationer
%på at sensor nøjagtighed, at billedet wobler...
At afstansmålingen gennem triangulering fra AR.Dronen ikke lykkedes er
ærgeligt, og er en oplagt mulighed at opsøge. I eksperimentet viste
det sig at beregningen var forkert, men det svarer ikke på om
trianguleringen kan eller ikke kan lade sig gøre. Der er flere
faktorer der skal være opfyldt før man kan sige det kan lade sig
gøre. Både den udledte odometriske afstand, vinklen udledt fra
frontkameraets optik og orienteringen fra piezo-sensoren skal opfylde
nogle tolerancekrav for at afstandsmålingen er brugbar. Det vil være
et meget interesant eksperiment at færdiggøre.

%% Skal det være her? 
%\section{Perspektivering}
AR.Dronens store udbredelse og det aktive udviklermiljø vil fungere
som både hjælp og inspiration under udviklingen af nye projekter med
AR.Dronen.

Selv om andre kritisere AR.Dronen som legetøj og ikke anvendelig som
videnskabelig platform,~\cite{quteprints33767} og andre allerede er begyndt at
eksperimentere med mindre quadrokoptere med mindre inerti-kapacitet,~\cite{turpin:swarm},
så virker AR.Dronen til prisen som værende god-nok. Vores oplevelse af
AR.Dronen gennem projektet er måske at den ikke virker helt så godt
til indendørs brug som man skulle tro (når der nu følger et indendørs
skjold med). I små rum har AR.Dronen vanskeligheder ved stabilt at stå
stille i luften, pga. vind turbulens, ekko fra de omgivende overflader
mv.
%
I stedet for at AR.Dronen bruges på et museum indendørs, er et andet
og bedre brugscenarie måske at den skal bruges udendørs. AR.Dronen kan flyve
steder hvor der ikke er umiddelbar adgang for mennesker (se eksemplet
med inspektioner af broer og højspændingsledninger). Denne egenskab
kan udnyttes så AR.Dronen kan bruges som trækplaster til events eller
på turist-atraktioner.
\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/stevns.png}
  \caption{AR.Drone flyvning ved Stævns klint. Der filmes udenfor klinten og ind på kirken.}
  \label{fig:stevns}
\end{framefig}
Højerup gamle kirken ved stævnsklint vil være et oplagt sted. For
hundrede kroner kan gæster styre AR.Dronen ud for at se Kirken fra
klinte-siden. Gæster kan se og dele andres flyvninger 
på optagelser. For et lille ekstra beløb kan gæsten købe
filmoptagelsen på en CD eller SD-kort.
%



\pagebreak

\appendix

\bibliographystyle{abbrv}
\bibliography{referencer}

\chapter{AT kommandoer}
\label{sec:at}
Dette afsnit beskriver de enkelte AT-kommandoer, en mere detaljeret
beskrivelse findes i~\cite{techreport:ardroneDevGuide}, side 29.
En AT-kommando må ikke deles over flere UDP-pakker, men en UDP-pakke kan
indeholde flere AT-kommandoer, kommandoerne skal blot være adskilt med
en "linefeed" karakter og den samlede længde må ikke overskride 1024
karakterer. Såfremt en kommando overskrider længdebegrænsningen eller
på anden måde ikke overholder syntaksen, vil AR.Dronen ignorere
kommandoen.   

Tabel \ref{tab:kommandoer} viser hvilke kommandoer der
jf., \cite{techreport:ardroneDevGuide} kan bruges til at
kontrollere AR.Dronen.

\begin{table}
  \begin{center}
  \begin{tabular}{ l | p{3,5cm} | p{5,5cm} }
    Navn & Argumenter & beskrivelse\\
    \hline
    REF & input & Kommando til takeoff, land og nødstop\\
    \hline
    PCMD & flag, roll, pitch, gas, yaw & Flytter AR.Dronen\\
    \hline 
    FTRIM &  & Sætter den horisontale referenceværdi \\
    \hline
    CONFIG & key, value & Ændrer en konfigurationsparameter\\
    \hline
    LED & animation, frequency, duration & Viser en LED animation\\
    \hline
    ANIM & animation, duration & Afspiller en flyvesekvens\\
    \hline
    COMWDG & & Resetter komunikationstimeren
  \end{tabular}
  \end{center}
  \caption{Gyldige AT-kommandoer} 
  \label{tab:kommandoer}
\end{table} 


\chapter{Canny kanter og Houghtransformerede linier}
\label{app:linier}
\todo{Tilføj Oles rettelser}
%plus hvad er kanter, og hvad er linier...
\section{At finde kanter i et billede}
Herunder gives en definition på hvad kanter (edges) er, samt deres egenskaber.
Forklaring er simplificeret og tager udgangspunkt i et eksempel med
kantdetektering i kun 1 dimension, hvor den rigtige teknik bruger
gradienten i 2 dimensioner. 
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/kant_detektering1a.png}
  \caption{Graf over rød-, grøn-, blå-farveværdier for en linie i
    billedet med pixel indeks $j=67,\ i=\{0,1,\ldots,320-1 \}$. Samt
    pixel intensitet, her givet ved summen af de tre farver for $i$ og
    $j$. Der hvor grafen, eg. for intensiteten, har stejlest hældning
    er der stor sandsynlighed for en kant i billedet.}
  \label{fig:kanter1}
\end{framefig}

Kanter i et billede defineres som abrupt skift i farveværdi eller
lysintensitet lokalt\footnote{Man kan også lede efter kanter globalt,
  se f.eks., \cite{electronic:wiki_stepdetectglobal}, men det er en
  anden tilgang. I dette afsnit beskrives en metode der søger 
lokalt.} mellem naboer af pixels. Kanter er ofte en indikator for
eksistensen af bl.a. ændringer i dybden af et billede og for
overfladeændringer m.fl., se figur~\ref{fig:kanter1}. 

For at forbedre resultatet af kant-filteret, kan indput-billedet
forinden flyttes ned i dimension ved at bruge et
båndpasfilter. Båndpasfilteret er en udglatning over
billedet, som udføres med en \todo{convolve?} gaussisk
klokke-operation, \cite{electronic:wiki_gaussklokke}, (fra
billedbehandlingsprogrammer, eg. GIMP, \cite{gimp}, er funktionen kendt
som Gaussian-blur).

For herefter at finde kanter bruges cannyfilteret, \cite{Canny1986}.
Canny blev valgt over Sobel, fordi Canny blev kendt først i specialet, men også
fordi det ser ud til at kantmarkeringerne af et Sobelfilter er bredere end
med Canny. Hvilket senere vil have indvirkning på Houghtransformens
udførsel. \todo{er det rigtigt? hjemmel?}

Cannyfilteret tager et farvebillede i 3 kanaler og returnere et binært billede
(sort/hvid) som det i figur~\ref{fig:vanishopb}. Hvide billedpunkter
\todo{hedder det noget andet end billedpunketer}
indikere at der er en kant i det oprindellige billede, mens sort
betyder at der ikke er ændringer. 

Tager man alle pixelpositioner hvor den første afledte er over
threshold \todo{problemet med Sobel?} kan man ende op med mange tykke
kanter, for at præcisere placeringen kan man udtynde dem, ved at søge
i retningen af den lokale gradient og istedet finde
optimum. \todo{skal dette med?: Videreudbygninger af Cannyfilter-implementationen efter 1986
bruger bl.a. endnu en retningsbestemt afledt af gradienten og finder
maximum ved den anden afledtes skæring med 0. - Lindenberg88 den
2. og 3. afledte...}

\section{Liniedetektering via Houghtransformation}
Generaliseret houghtransformation, \cite{duda1972houghline}, er en
metode til at finde perfekte linier hvor der ikke andet end
billedpunkter. Dvs. metoden finder steder i et billeder hvor der ligger mange
billedpunkter på en lige række, og kalder rækken for en linie.

For hvert billedpunkt $(x_0, y_0)$ i billedet kan man skrive de linier
der går gennem punktet som
\begin{equation}
  \label{linigennempunkteq}
  \rho_{\theta} = x_0 * \cos{ \theta } + y_0 * \sin{ \theta }
\end{equation}
Hvor $0 \le \theta < 2 \pi$ og $0 \le \rho$, og hvor hvert par $(\theta, \rho_{\theta})$ vil repræsentere en linie igennem
det punkt (se figur~\ref{fig:houghting}).

\begin{framefig}
  %\centering
  \subfigure[En linie beskrevet ved det velkendte almindelige
    cartesisk koordinatsystem som: $y=ax+b$, samme linie beskrevet i
    polar koordinatsystem ved en vinkel $\theta$ og en afstand
    $\rho$. Forholdet imellem de 2 repræsentationer er $y = 
\left( 
  - \frac{ \cos \theta }{ \sin \theta} 
\right)x 
+
 \left(
   \frac{ \rho }{ \sin \theta } 
\right) $]{
    \includegraphics[width=0.32\textwidth]{grafik/polar2cartesian.png}
    \label{fig:polar2cartesian}
}
  %~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Houghtransform, linie med flest stemmer, de 3 blå kryds
    er billedpunkter]{
    \includegraphics[width=0.66\textwidth]{grafik/houghting.png}
    \label{fig:houghting}}
  \label{fig:houghprocess2}
  %\caption{Markører vi har testet i forbindelse med visuel lokalisering af AR.Dronen}
\end{framefig}
Man kan opsætte et kriterie for hvad der gør en
god linie, ie. at linien skal gennemløbe mindst $X$ billedpunkter, for
komme i betragtning til at være detekteret som en linie (minimumfor $X$
er 3 punkter). De linie-par som er løsning til flest konkrete
\ref{linigennempunkteq}-ligninger vil rangere højest i resultatlisten.
Resultatet af at køre det cannyfiltrerede billed igennem en
Houghtransform er en liste med linier, repræsenteret ved ($\theta_i$,
$\rho_i$) par, og sorteret efter godhed.

OpenCV har to implementationer af Houghtransforms: en standard som
ovenfor beskrevet, og en probabilistisk Hough line transform. Den
probalististiske returnere en liste af linier, repræsenteret ved
liniernes endepunkter $(x_{i1}, y_{i1}), (x_{i2}, x_{i2})$ istedet for
et $(\theta_i$,$\rho_i)$ par som ovenfor.

\end{document}

%\begin{center}
%  \includegraphics[scale=0.35]{pic/selfserv.jpg}
%\newline
%\textbf{Figur 1}
%\end{center}
%\lstinputlisting[language=Python]{../drone.py}





% \bm{J} = \bigtriangledown I (\bm{x} ) = \left(\frac{ \partial I }{ \partial x }, \frac{\partial I }{ \partial y }\right)(\bm{x})
%  G &= \sqrt{ G_x^2 + G_y^2 }\\ \theta &= \arctan \left( \frac{G_y}{G_x} \right)\\ \text{ hvor } G_x & = \frac{ \partial I }{ \partial x } \text{ og } G_y \frac{\partial I }{ \partial y }

% \bm{J}_{\sigma} = \bigtriangledown [ G_{\sigma} (\bm{x} ) \times I (\bm{x} ) ] = [ \bigtriangledown G_{\sigma} ] (\bm{x} ) \times I (\bm{x} )

% \bm{S}_{\sigma}(\bm{x} )=\bigtriangledown \dot{} \bm{J}_{\sigma}(\bm{x} )=[ \bigtriangledown ^2 G_{\sigma} (\bm{x} ) \times * \bm{I} (\bm{x} ) ]
% \bigtriangledown G_{\sigma}(\bm{x} )=(\frac{\partial G_{\sigma}}{\partial x},\frac{\partial G_{\sigma}}{\partial y})(\bm{x})=[-x -y]\frac{1}{\sigma^3} \exp \left( - \frac{x^2 + y^2}{2 \sigma ^2} \right)

%#################3

%% Hvis man observere på ibrugtagning af robotter gennem tiden, ses robotteknologien anvendt
%% mange steder i industrien ofte med hensigten om 
%% at aflaste mennesker for monotone og farlige arbejdsprocesser (Unimate1950\cite{electronic:wiki_unimate}), eller med
%% hensigten om at effektivisere en produktionsvirksomhed ved indførelsen
%% af faste maskiner til at afvikle forprogrammerede
%% rutiner. Introduktionen af ny teknologi har altid en konsekvens;
%% ændringer betyder, at noget udskiftes med noget andet.
%% Den store Erhvervsredegørelse fra 1997 konkluderede også at
%% \begin{quotation}
%%   ``globalisering og ny teknologi overflødiggør den kortuddannede arbejdskraft.''
%% \end{quotation}%http://ibog.danmarkshistorien.systime.dk/index.php?id=224
%% %google søg: ``industrialisering efterspørgsel på arbejdskraft''
%% Hvis ny teknologi skal indføres bør det ikke kun afvejes iforhold til
%% økonomiske nytteværdi og omkostninger, men også med omtanke for det
%% enkelte menneske og under hensyntagen til langtsigtende konsekvenser for samfundet.

%% Fra de faste stationere robotter i produktionen kan man gå til de
%% mobile, hvor Elmer og Elsie er eksempler. Allerede fra 1948 lavede
%% Dr. W. Grey Walter 2 selvkørende
%% robotter\cite{Walter:1950:elmer:elsie} med analog styring ombord.
%% Senere ifølge Wikipedia\cite{electronic:wikirobot:comparison} er
%% der i

%1997 lavet en rengøringsrobot Robosanitan\cite{electronic:robosanitan1997}.


%%%%%%
%% \section{Udvidelse af AR.Dronen}
%% En robotplatform der kan udvides med nye sensorer efter behov, er
%% klart mere anvendelig end en helt lukket platform. Vi har derfor
%% undersøgt mulighederne for at anvende AR.Dronens OTG-USB* port til
%% andet end at udføre softwareopdateringer. Dette har blandt indbefattet at
%% kompilere kernemoduler(*) til AR.Dronens Linuxkerne(*ref til blog*),%som beskrevet i indledende sektion~\ref{sec:blog}
%% samt at omgå AR.Dronens software for ikke at miste strømmen til
%% porten. Vores proof-of-concept var at mounte en almindelig USB memorystick
%% og at skrive og læse fra denne(*ref til blog*), dette var forholdsvis
%% simpelt, da Linuxdriverne til dette formål er meget generiske.

%% \begin{figure*}[htp]
%%   \centering
%%   \subfigure[Hjemmelavet USB-kabel, en USB memorystick og en WIFI USB-dongle til tilslutning på AR.Dronen.]{
%%     \includegraphics[width=0.47\textwidth]{grafik/usbdevices.png}
%%     \label{fig:cableandusbdevices}}
%%   ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
%%   \subfigure[AR.Dronen med USB-memorystick tilsluttet.]{\includegraphics[width=0.47\textwidth]{grafik/usbattached.png}
%%     \label{fig:usbattached}}
%%   \label{fig:usbdevices}
%% \end{figure*}

%% \subsection{Wifisensor}
%% Efter vores indledende erfaringer med AR.Dronens USB-port, valgte vi at
%% implementere en wifi-sensor for på sigt at kunne anvende denne som
%% lokaliseringsredskab. Vores wifisensor består af en USB-wifi-adapter
%% og et snifferprogram til at aflæse og videresende signalstyrker fra de
%% omkringliggende accesspoints og andre wifi-kilder. Snifferprogrammet
%% antager at det trådløse interface der anvendes er sat op i monitor og
%% promiscous mode så det modtager alle pakker der sendes fra de
%% omkringliggende kilder, dette opnåes ved at kalde vores eget
%% startup-script(*ref til annoteret startup*) til sidst i dronens
%% oprindelige bootup-sekvens, i dette script loades også alle de
%% nødvendige kernemoduler.

%% \subsubsection{WIFI-adapter}  
%% Vi har valgt at udstyre AR.Dronen med en Dlink DWL-G122~\cite{electronic:dwlg122}
%% wifi-adapter. Denne specifikke adapter bygger på et Ralink~\cite{electronic:wikiralink} chipset og
%% understøtter monitor mode. Ralink udgiver desuden linuxdrivere til
%% deres chipsets. Efter længere tids søgen og eksperimenteren, fandt vi
%% den korrekte driver og fik denne kompileret til ARM-arkitekturen og
%% kompileret til at være kompatibel med linuxkernen anvendt på
%% AR.Dronen. Efter at have loadet driveren, genkendes vores adapter og
%% interfacet ra0 oprettes i \code{/sys/class/net/}. 

%% \subsubsection{Sniffer}
%% Vores første eksperimenter med hensyn til at kompilere og afvikle kode
%% på AR.Dronen kan ses på *link til blog*, dette var simple 'hello
%% world' eksempler, men gav os indsigt i proceduren omkring
%% crosscompiling til AR.Drone arkitekturen og den anvendte toolchain. 

%% For at skrive et program der kan modtage pakker fra et
%% netværksinterface anvender man ofte linux' pcap bibliotek, dette er
%% dog ikke er installeret på AR.Dronen som standard og skulle først
%% kompileres til ARM-arkitekturen før vi kunne linke til det og senere
%% anvende programmet på AR.Dronen, se *blog ref*.

%% Sniffer programmet har mulighed for at tage et interfacenavn som input
%% når det startes, men som standard anvendes 'ra0' interfacet. Man kan desuden
%% vælge at angive en kanal, at angive om man ønsker at anvende
%% channelhopping til at modtage pakker fra forskellige kanaler og angive
%% om man ønsker at få output til terminalen. Når programmet startes vil
%% det vente på en UDP initieringspakke fra en klient før det begynder at
%% modtage og behandle pakker. Sniffer programmet sender signalstyrker og
%% mac-adresser videre til klienten ved hjælp af UDP kommunikation på
%% port 5551, hvilket gør at det er transparent for klienten om hvorvidt
%% den modtager data fra den originale AR.Drone software eller fra vores
%% udvidelse. 
