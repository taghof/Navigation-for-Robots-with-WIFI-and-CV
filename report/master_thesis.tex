%\documentclass[12pt,a4paper,twoside,openright,fleqn]{memoir}
%\documentclass[10pt]{article}
\documentclass[12pt]{report}
%\documentclass[12pt,twoside,openright]{report}
%\documentclass[10pt,twoside]{report}
 
\usepackage[danish]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{float}
% \floatstyle{boxed} 
% \restylefloat{figure}
\usepackage{listings}

\usepackage{natbib}
\usepackage{titleref}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tocbibind}

%\usepackage{caption}
%\usepackage{subcaption}
\usepackage[hyphens]{url}

%Define a new 'leo' style for the package that will use a smaller font.
\makeatletter
\def\url@leostyle{
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}

\usepackage{hyperref}

\newenvironment{framefig}
   {\begin{figure}[H]\begin{framed}}
   {\end{framed}\end{figure}}

\newcommand{\uri}[1]{ #1 }
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textbf{todo:}(#1)}
\newcommand{\dronen}[1]{#1}
\newcommand{\drone}[0]{AR.Drone} 

\begin{document}
\title{Master Thesis}
\author{Thomas Thyregod 20051688\\ Morten Daugaard 20051715\\ \texttt{tysse, taghof}@cs.au.dk\\\\ Department of Computer Science, University of Aarhus\\ Aabogade 34, 8200 Aarhus N, Denmark\\\\
\hline\\
\vspace{4cm}\\
\makeatletter
\texttt{Semi Autonomous Indoor}\\ %Indoor? - nogle udendørs robotter heller ikke bruge gps, for dyrt i præcision...
\texttt{Navigation for Arial Robots}\\
\vspace{4cm}\\
\hline\\
}
\date{\today}
\maketitle
%\newpage

\pagestyle{headings}

\section*{Abstract}
In english...

\section*{Resume}
%Indeholder og gennemgår i det følgende (på dansk).
%-Kort intro hvad vil vi gøre...
Dette speciale handler om at klarlægge muligheder for at gøre en
quadrotor %\cite{electronic:wiki_quadrotor}
 delvis selvflyvende i et
indendørsmiljø. Mere specifikt handler det i første omgang om at
kontruere en base i software som der kan bygges videre på senere ved at
udnytte en AR.Drones %\cite{electronic:ardrone} fra Parrot
 indbyggede sensorapparat
og kontrolinterface. I anden omgang handler det om at undersøge
hvorledes sensorapparatet ombord på AR.Dronen kan udnyttes til at
udlede mere abstrakt information ud af som igen kan anvendes til at
abtrahere en indforstået model som AR.Dronen kan navigere i til støtte
for en menneskelig bruger.
\todo{Kort uddybning af, hvad har vi gjort, hvad var resultatet.}

\pagebreak

\tableofcontents

\pagebreak

\chapter{Introduktion}  %Thomas
% på videnskabeligt grundlag identificere, afgrænse og formulere en faglig problemstilling, 
% definere og opstille testbare hypoteser inden for fagets emneområde, 
% selvstændigt planlægge og under anvendelse af fagets videnskabelige metode gennemføre et større fagligt projekt, 
% analysere, kritisk diskutere og perspektivere en faglig problemstilling, 
% vurdere, kritisk analysere og sammenfatte den videnskabelige litteratur inden for et afgrænset emneområde, 
% formidle videnskabelige resultater objektivt og koncist til et videnskabeligt forum
\label{sec:introduktion}
%\subsection{området, teknologi}
%emne generelt, related work, hvad har andre lavet?
%problem specifik
%definition af nøglebegreber
Med 1970'ernes øgede tilgængelighed af halvlederteknologi og de faldende
priser på mikroprocessore %http://en.wikipedia.org/wiki/Intel_4004
er der de senere år (gen)opstået en
interesse blandt innovatøre og forbrugere for mobile robotter, specielt
indenfor ``gadget''-genren. Kendte eksempler herpå er robotstøvsugeren
Roompa, \cite{electronic:irobotroomba} af 2002 fra iRobot og den selvkørende
plæneklipper; Husqvarna's Solar Mower, \cite{techreport:solarmower} af 1995.

\paragraph{Fremmende teknologier}
Sammenfaldet ligger som sagt i at teknologien nu er moden
til, med billige komponenter, at konstruere maskiner der selvstændigt
kan indsamle elektronisk sensordata og træffe simple beslutninger
baseret på den indsamlede digitale data. Computeren har således været
en nøgleteknologi for udviklingen af det nogle kalder kunstig
intelligens (AI) eller det at en maskine er \emph{autonom}.

\paragraph{Andre anvendelser}
Man ser en stor del af robotudviklingen idag bliver båret af løftet om at
støtte menneskers arbejde og dagligdag gennem
robotteknologi. Sammenlignet med tidligere (og til stadighed) hvor
mekaniseringen ofte har haft konsekvenser for samfundet i form af øget
arbejdsløshed gennem fokus på reduktion af omkostninger i produktionen,
ser man nu også robotter blive taget i brug i
støttefunktioner, altså som medhjælp istedet for afløsning. Eksempler
herpå er udviklingen af en redningsdrone til at afhjælpe
kystvagter. I artiklen, \cite{electronic:redningsdronedk} refereres til
en næste %kinect 
version hvor redningsdronen selvstændigt flyver ud til den
nødstedte og aflevere en redningsvest. Et andet anvendelsesscenarie er
brugen af de flyvende maskiner (på engelsk; UAV (Unmanned Arial
Vehicle)) i inspektionen af blandt andet højspændingsledninger og
broer, \cite{uavbridgeinspection2007}. Robotter bruges ikke for at
overtage menneskernes job, men som støtte. Menneske og maskine
samarbejder, fordi mennesker er bedre til at træffe beslutninger i
uforudsete situationer og dermed guide robotten, mens robotten er en
perfekt stedfortræder i farlige, kedelige, eller belastende
situationer. 

\paragraph{Omkring AR.Dronen} %problem domæne,
\todo{hvis læserens første møde med ar.dronen, så introduceres bedre!
  + Parrot er producent}
Grundmotivationen bag specialet har langt af vejen været at ``der skal være et underholdende
indhold''. Kigger man på hjemmesiden til promovering af Parrots
AR.Drone, \cite{electronic:ardrone} skinner underholdningfaktoren også
tydeligt igennem (se figur~\ref{fig:devisen}).
\begin{framefig}
  \centering
  \includegraphics[width=0.50\textwidth]{grafik/flyingvideogame.png}
  \caption{AR.Dronens logo med slagordene \emph{the Flying Video Game} april 2011 http://web.archive.org/web/20110715044741/http://ardrone.parrot.com/parrot-ar-drone/en/.}
  \label{fig:devisen}
\end{framefig}
AR.Drone blev introduceret i 2010 som ``the~Flying~Video~Game'' og
promoveres stadig af producenten som et stykke legetøj, der fjernstyres fra
brugerens iPhone eller iPad. Siden har AR.Dronen dog fundet stor
anvendelse, ikke blot som et avanceret stykke legetøj; forskere,
studerende og hobbyister har taget den fjernstyrede quadrokopter til
sig som en færdig platform og selv udviklet videre ovenpå den.%cite
%\ ... {Related works}          %hobby selvbyg (ombygge fysisk
                                %LED/GPS/WIFI) / flashe ny
                                %styreprogram
                                %ombord \paragraph{Lignende arbejde -
                                %forskning} %\cite evt. flere som har
                                %brugt AR.Dronen i deres forsøg
                                %%\cite{electronic:ludep}\cite{single_img_pespective_cues}
%%%forskning, bl.a. LUDEP. --- UD_DYB hvad det er referencerne har
%%%lavet 
%
%Sousveillance vs. surveillance, noget multimedieæstetik, occupy wallstreet movement thing
%http://www.alternet.org/occupywallst/153542/ows\_fights\_back\_against\_police\_surveillance\_by\_launching\_\%22occucopter\%22\_citizen\_drone/ 
% også cyborgen Steve Mann

\paragraph{At quadrokoptere er mulige} % enabling technologies
At ``billige'' quadrokopter UAV idag flyver er kun gjort muligt
gennem forbrugselektronikkens udvikling og brug af Micro
Electro-Mechanical Systems (MEMS) accelerometer og gyroskober, samt
evigt faldende priser på mikroprocessorer. AR.Dronen bruger, udover
hovedprocessoren, hurtige accelerometre og dedikeret elektronik til
motorstyring for at stabilisere flyvningen. Uden dem ville det være
umuligt for en utrænet pilot ikke at styrte konstant. 

%flyvende energieffektivt... overblik ift. kørende
%specialet/flyvende udsprunget fra den verden vi befinder os i
%flyvende robotter naturligt valg, flow, 
%bruge i område/case som før kørende robot entity, hvad giver forskellen - hvad
%skal man være bedre til når man flyver, hvad kan man ikke, hvad opnår
%man ved flyv.
Selvom elektronikken ombord på quadrokopteren gør mange ting
lettere, er der stadig mange udfordringer i at opfatte en 3
dimensionel verden og i at styre en robot i 3 dimensioner, samtidig
med at robotten udføre andre opgaver selvstændigt.
En stor del af forskningen i quadrokoptere har hidtil gået på at
optimere mekanikken, \cite{quteprints33767}, stabiliseringen og
motorstyringen. I mange eksperimenter har quadrokopteren været
fjernstyret gennem ekstern observation. Eksemplevis i forskningen af
små agile Mikro Arial Vehicle (MAV) i flok, \cite{turpin:swarm} er der
fastmonteret kameraer i lokalet omkring MAV til udregning og feedback
af absolut position.

\paragraph{Quadrokopter styring gennem Kinect} %nyeste forskning
Idag er der, igen gennem ny nøgleteknologi, lavet
forsøg med ``Simultaneous localization and mapping''
(SLAM), \cite{shen2011kinect}, hvor quadrokopteren er påmonteret en
kinect, \cite{electronic:kinect} til selv at opfatte omgivelserne i 3
dimensioner og derigennem selv orientere sig.\newline

\paragraph{Specialets tilgang} % <- Tese herop?
Dette speciale anvender den fjernstyrede AR.Dronen til at opbygge en
robotplatform omkring for at undersøge hvilke krav der nødvendigvis
skal opfyldes for at konstruere en modulopbygget kontrolplatform til
styring af en quadrokopter.

\paragraph{Domæne afgrænsning}
AR.Dronen siges at kunne flyve både indendørs og udendørs. Dette
specialet er afgrænset til at behandle navigation indendørs. For at
skabe en god basis at arbejde videre udfra, defineres afgrænsningen
yderligere. Der er fundet inspiration i indendørsområder som de
beskrevet i arbejdet med den kørende museumsrobotguide Minerva fra
1999, \cite{Thrun99minerva:a}. Af hensyn til at lave praktiske forsøg,
bliver de konkrete navigationsløsninger designet til at virke i
IT-byens kontorer og gangarealer samt Zuse-bygningen på Katrinebjerg i
Århus.

%%
%% Hvorfor vil vi ikke opbygge en ``war-driving'' database først, 
%% offline database med 
%% - wifi-signalstyrke samples
%% - reference billeder til opslag af lokation
%%
%% Burde i alle fald nævne at vi ved det eksisterer...
%%

%-introduktion til andres arbejde / resultater, 
%-hvad bygger vi videre på (hvis arbejde), 
%-hvad er vores antagelser til at starte med



\section{Motivation} % Thomas & Morten, Hvorfor
\label{sec:motivation}
%Anvendelser}%anvendelser fra dette speciales emner

%\paragraph{Hvem vil vores resultater komme til gode}
forløberen har været ``fremkommelsen'' af nøgleteknologier
``enabling-teknologies'' - har gjort det muligt at udbrede teknologien
kommercielt i legetøjs-helikoptere, hvilket igen har skaffet kapital
og resourcer til udvikling af mere teknologi, - vi ønsker mere
teknologi - derfor arbejde hen imod at konstruere en base som
tilgængeliggør automatisering af flyvende robotter i højere grad for
en bredere gruppe af hobbyister og dermed udvide viden og udvikling på
området. 

\paragraph{Hvorfor er det relevant}
+Optimering -> resource besparelser...?
+Grundforskning...?
+vigtigt at klarlægge - ting, så man forstår konsekvenser hvis senere
tages ibrug...?

\paragraph{Anvendelses områder}
+For hurtigere at kunne gennemløbe cyklusen mellem ide til
implementation til aftestning til analyse til ide.
+ fordi modulopbygget - Lave gode robuste/specialdesignede
hjælpemidler til dedikeret støtte i forskellige arbejdssituationer.

\paragraph{Hvorfor er det bedre/billigere med vores løsninger}
+modulopbygget, hurtigt at udskifte genbruge komponenter
+Nemmere / hurtigere kunne implementere nye produkter.
+en robot skal ikke kunne det hele, så behøver ikke være en stor
computer monolith, eller mekanisk klumpet platform, men lille hvor
tilføje / fjerne moduler afhængig af produkt der designes til.

\paragraph{referencer til lignende arbejde}, vores forskellig fordi..
Minerva museums tourguide robot\cite{Thrun99minerva:a}?
er lavet til at vise gæster rundt på et museum *billede af Minerva*
har kamera og afstandsmåler til at se publikum og navigere ift. pupl,
*billede af ``photoshop'' ar.drone flyvende i l'Louvre*
hvad ville der ske hvis vi tog samme scenarie og brugte en flyvende
helikopter istedet, uddyybning at den skulle kunne fjernstyres, optage
film af flyvning, fjernstyres hjemmefra via et webinterface (nævne
problematisk hvis andre gæster-måske i konklusion).


\section{Teseformulering/mål} % Thomas & Morten, Hvad
\label{sec:tese}
HVAD er det præcis vi vil vise eller afvise?! Lave et

framework/platform til at afhjælpe tiden mellem kodeimplementation til
aftestning 

nærmest punktform, - punkter kan konfirmeres eller afvises i konklusionen.
er det en platform som andre kan arbejde videre på, undersøge hvad der evt. skal til for at lave en sådan
Det er noget med en [semi autonom] [navigation] [indendørs] [luftbåren]

Vi har identifiseret nogle punkter/elementer som vi mener skal med /tilstede
indeholde til modulopbygget kontrolplatform, teste om det kan
implementeres, ---
Hvor kommer de identificerede elementer fra, vores tidligere robotleg
med Lego? - andres artikler?, minerva / ROS men i luften istedet for
på jorden?

fra semiautonom og navigation er der noget, 1) bevægelse i forskellige miljøer, og 2) avoid opførsel, møde med silhuet eller lavt batteri.

at det er indendørs giver nogle begrænsninger, kan bestemme et bestemt miljø ala Hopper-gangene / Ada-gangene eller Zuse.

Luftbåren at vi ligepludselig har en 3. dimension at forholde os til

Er det en del af opgaven at se om AR.Dronen er den rigtige platform at lave dette på? - er en konklusion at den er adequate, men at det ville være bedre havde man haft tiden til at lave sin egen, finde en anden???

\section{Metode} % Thomas, Hvordan, skal den være en SUBsection eller section?
\label{sec:metode}
%% Antagelser, problemdomæne formuleret i introduktionen sektion \ref{sec:introduktion}.
%% (Lavpraktisk:)
%% - hvilke værktøjer vil vi tage i brug AR.Dronen, PC, joystick/controller, Python, => hvorfor, argumentation.
%%    tage AR.Dronen gå fra at bruge den som en fjernstyrbar enhed til at gøre den til en næsten selvstændig semi autonom enhed.

%% -Hvordan vil vi faktisk teste vores tese? - arbejde går imod at lave en testopstilling, hvad skal der til for at opstillingen virker? framework, processor-kraft båndbredde
%% Højniveau?:
%% - er der noget højt,... abstraktion?
hvordan teori



%%%% Introduction - Metode
\subsection{AR.Drone} % Thomas,
Selvstændig selvstående paragraf omkring, valg af AR.Dronen. Hvorfor har vi valgt den, hvad tilfredstiller den, hvad tilfredstiller den ikke. Hvad skal den bruges til.

generelle sensore på dronen; 2 kameraer, acceleration, rotation, hastighed, 
wifi-kommunikation

Discussion af AR.Dronen som tilfredstillende basis... ``Modelling and
control of a quad-rotor robot'', Pounds et al, \cite{quteprints33767}
bygge en ny mekanisk forsimplet udgave af en quadrocopter, mere robust
=> færre reperationer... --- formentlig ofte et spørgsmål om
tilgængelighed, ``it's good enough'' ej genopfinde dybtalerken....

Definition på en robot; kan udføre en opgave - en sekvens af operationer/handlinger -- eksemplet en ``dum'' industrirobot,
anden karakteriserende pind/punkt/egenskab; at den på baggrund af sensorinput (feedback kontrol) kan træffe beslutninger / beslutningstræ...
Disse to egenskaber findes allerede i AR.Dronen, udfra det er den tilpas til at starte ud med når vi skal lege med robotter

\paragraph{2 AR.Droner} % Thomas, en med ekstern lab-forsyning, en flyvende

\paragraph{Nu med USB-driver}% Thomas & Morten
- udvide features / muligheder / operationer / operationsområder, 
- samt undersøgt muligheden for at udvide AR.Dronens Hardwareplatform med yderligere sensore eg. USB / mere flashdrive

%%%% Introduction - Metode
\subsection{Domæne/navigationsmiljø} % Thomas,
Nogle bygninger på Katrinebjerg, indendørs...
hvorfor / uddybende beskrivelse af kontor, gang, openspace
%% - hvad er miljøet, Vi begrænser anvendelses-/test-området, hvorfor argumentation er simplificering for os selv, fokusere på det relevante, hvad er det (relevante)?! - pege tilbage på tesenformuleringen sektion \ref{sec:tese}.

\subsection{Python}
platform skrevet i python med bibliotek / wrapper til opencv (ffmpeg), pygame, numpy...
hvorfor valgte vi python, - fordi de er præcist lavet til
prototype-programmering, evt. nævne at vi er kommet frem til at det i
fremtiden vil være bedre at skrive en del i C og så lave python
wrapper ovenpå - cite kunne være til en lærebog kendt af Ole, til at
lære programmering gennem robotter på nettet...

\subsection{OpenCV}

%%%% Introduction - Metode
\subsection{Platform samt hjælpeværktøjer} % Thomas & Morten
%arkitektur perspektivering analyse?
forskellige frameworks til sensordata opsamling og
udledning/abstraktioner Dey's toolkit / JCAF, Lenser m.fl.
- vi forestiller os en platform med understøttelse af virtuelle
sensore 

Som hjælp til andre og til dette projekt er der konstrueret en række værktøjer. For f.eks. at minimere den tid der går fra at en ide implementeres i kode til aftestning på quadrokopteren, er det muligt at lave scripts (Task) bestående af kun få liniers kode der nemt afvikles på systemet.
%
\paragraph{drone.py} -modulet / -klassen er konstrueret for at tage sig af al kommunikationen mellem PC og AR.Dronen. Kommunikationen består i at opretholde forbindelsen, samt modtage datapakker fra quadrokopterens sensor system og at afsende styrekommandoer fra brugeren til AR.Dronen.
%
\paragraph{Tasks} er måden at afvikle og teste selvstyrende/kørende opførsel. For at muliggøre manuel støttes, styring og afhjælpning under aftestning kan brugeren anvende PC tastatur og XBox joystick.
%
\paragraph{Joystick- og Keyboard-Controller} er den del af platformen som lytter efter bruger input events til manuel styring af AR.Dronen og som input til den grafiske brugergrænseflade (GUI).
%
\paragraph{GUI til datarepræsentation}


%%%% Introduction - Metode
\paragraph{Mapdrawer / turplanlægger}


%%%% Introduction - Metode
\paragraph{Task manager plugin og tasks}


\subsection{Eksperimenter} % Thomas,

\paragraph{Lokalisering via Wifi-fingerprinting}
%\paragraph{Simpel-blob}
\paragraph{Blob-detection og PID styring} simpel blob, og udbygget nested blob
\paragraph{Korridor detektering}
\paragraph{distance-measurererere}

\paragraph{avoid silhouet}
\paragraph{low battery}
\paragraph{Erkendelse af miljø...}
 Korridor (Hopper) vs. Åbent område (Zuse)

%%%% Introduction
\section{Raport struktur} % Thomas, %/læsevejledning
Introduktion til rapportens arbejde, hvilke afsnit der gennemgåes og hvad afsnit indeholder.
intro til medfølgende CD og blogpost, med kode på Github

\section{Medfølgende CD}
Dude its a CD

\section{Projekt Blog på Github}
\label{sec:blog}
Beskrivelse af indholdet på bloggen, versionshåndtering, tilgængelighed for comunity
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Quadrokopter AR.Drone}
\label{sec:AR.Drone}
AR.Drone er en quadrokopter fremstillet og udviklet af det franske
firma Parrot, \cite{electronic:parrot},
\cite{electronic:wiki_parrot}. AR står Augmented Reality,
\cite{electronic:nbc}. En quadrokopter benytter fire faste rotorer til
at generere opdrift, samt til at styre sin bevægelse i rummet.
Parrots quadrokopter kommer med et sæt sensorer bestående af: To
kameraer, et vertikalt nedadrettet og et horisontalt fremadrettet, en
nedadrettet afstandssensor, et gyroskop, samt et accelerometer. I 2012
ventes en opdateret udgave af AR.Dronen. Den nye version 
tilføjer yderligere et kamera i høj opløsning (HD), en trykmåler
(digitalt barometer) og et kompas til platformen, 
\cite{electronic:ar2}.AR.Drone er opbygget som en letvægtskonstruktion
af kulfiber, plast og skummateriale og kommer med to forskellige
skumskjold til henholdsvis indendørs
(figur~\ref{fig:indendoersskjold}) og udendørs flyvning
(figur~\ref{fig:udendoersskjold}).

I det følgende gennemgåes i afsnit \emph{\nameref{ssec:hardware}} de
fysiske enheder som quadrokopteren består af, i afsnittet
\emph{\nameref{ssec:fysik}} gives en introduktion til hvorledes
quadrokopteren bruger rotorerne til at skabe opdrift og bevægelse i
luften, i afsnittet \nameref{ssec:software} og afsnittet
\emph{\nameref{ssec:kommunikation}} beskrives kommunikationen mellem
AR.Dronens styreprogram og forskeliige typer klienter, mens afsnittet
\emph{\nameref{ssec:dronerecap}} opregner de observationer der er gjort
under specialearbejdet og som kan hjælpe andre med at træffe en
beslutning om hvorvidt AR.Dronen er den rigtige platform at bygge
videre på.

\begin{framefig}
\centering
  \subfigure[Indendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone.jpg}
    \label{fig:indendoersskjold}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Udendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone-outdoor.jpg}
    \label{fig:udendoersskjold}}
  \caption{AR.Drone med indendørs- og udendørsskjold. Figurene er fra
    AR.Drone Developer Guide, \cite{techreport:ardroneDevGuide}}
  \label{fig:ardrone}
\end{framefig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Hardware - subsection
 
\section{AR.Dronens Hardware}
\label{ssec:hardware}
Det er nærmest umuligt for et menneske at styre en helikopter ved
direkte kontrol med mindre man har trænet i mange år (hvilket er det
profesionelle piloter gør, \cite{electronic:heli}). Brugerens styring af AR.Dronen er således
ikke direkte men støttet af en feedback kontrol hvortil der
er sensorinput fra omverdenen. Sensorerne, aktuatorerne der gør det
muligt at kontrollere rotorerne, strømforsyningen og computeren
ombord beskrives herunder.

%og må siges at være en af de grundlæggende egenskaber for en robot. At der så er mulighed for at udbygge en stillingstagen iforhold til 
%
%og en processorenhed. Herunder beskrives sensorene og aktuatore samt den ombordværende computer og trømforsyningen.

\subsection{Sensorer}
\label{sssec:sensorer}
\begin{framefig}
  \subfigure[AR.Dronens frontkamera]{\includegraphics[width=0.45\textwidth]{grafik/frontcamera.jpg}
    \label{fig:frontkamera}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[AR.Dronens bundkamera]{\includegraphics[width=0.45\textwidth]{grafik/bundkamera2.png}
    \label{fig:bundkamera}}

  \label{fig:kameraer}
  \caption{AR.Dronens to kameraer}
\end{framefig}

\paragraph{Kameraer}
AR.Drone er som sagt udstyret med to kameraer: Et
bundkamera(nedadrettet) og et frontkamera(fremadrettet). Frontkameraet
har en synsvinkel på $93 ^{\circ}$ og består af en
CMOS\footnote{Complimentary-Symmetry Metal Oxide Semiconductor, på
dansk: en sensor af komplimentær metaloxid halvleder
teknologi.}-billedsensor som kan levere $640 \times 480$ pixel billeder.
AR.Dronen transmitterer dog kun med en opløsning på $320 \times 240$, pixels Quarter Video
Graphics Array (QVGA)  med en framerate på 15 FPS.
Bundkameraet er en CMOS sensor med $64 ^{\circ}$ synsvinkel,
der tager billeder i $176 \times 144$ pixels opløsning, Quarter Common
Intermediate Format (QCIF), med en framerate på 60 FPS. 
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/navboard.png}
  \caption{Over- og undersidebilleder af AR.Dronens
    navboard. Navboardet indeholder bl.a. den nedadrettede
    afstandssensor. Figurerne er fra to internetbutikker, 
    \cite{electronic:arct} og, \cite{electronic:dparts},  hvorfra der kan bestilles AR.Drone reservedele.}
  \label{fig:navboard}
\end{framefig}

\subsubsection{Afstandssensor} Til at bestemme AR.Dronens afstand til gulvet
anvendes en ultrasonisk afstandssensor. Denne opererer ved at udsende
en ultralydspuls og herefter måle tidsintervallet der går før ekkoet
måles. Ultralydsmåleren er angivet til at virke op til 6 meter og er
rent fysisk placeret på navboardet (se figur \ref{fig:navboard}) på
AR.Dronens underside.

\subsubsection{Acceleration og inerti} AR.Drone er udstyret med to
gyroskoper, et 2-akses gyroskop til at måle rotationsvinklerne
$\theta$ og $\phi$ (pitch og roll) samt et 1-akses piezoelektrisk gyroskop til at
måle rotationsvinklen $\psi$ (yaw). Gyroskoperne kombineres med et
3-akses accelerometer i en samlet enhed til at måle inerti
(IMU)\footnote{Inertial Measurement Unit}. For en grafisk
repræsentation se figur \ref{fig:drone_akser}.
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/drone_akser.png}
  \caption{AR.Dronens lokale koordinatsystem.}
  \label{fig:drone_akser}
\end{framefig}

\subsection{Aktuatorer}
\label{sssec:aktuatorer}
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/motor_gear.png}
  \caption{a) Motorreservedel, b) samlet motor og gear på kulfiberkryds og c) gear-reservedel.}
  \label{fig:motorgear}
\end{framefig}
AR.Dronens aktuatorer er de fire børsteløse 15 Watt
motorer, \cite{electronic:engines}. Motorerne
arbejder i intervallet 10350 til 41400 omdrejninger per minut(RPM) og når AR.Dronen står stille i
luften arbejder motorerne med 28000 RPM. Dette svarer, grundet gearingen,
til 3300 RPM for propellerne, \cite{electronic:engines}. Motorerne har uden sammenligning det
største strømforbrug af alle AR.Dronens enheder.

\subsubsection{Den indlejrede computer}
\label{sssec:processor}
Den centrale computer på AR.Dronen er en ARM9 468 MHz
processor (Parrot 6 ARM926EJ) med 128 MB DDR RAM til 
arbejdshukommelse og 128 MB NAND flash til persistent (vedvarende) hukommelse.

Tilkoblet den indlejrede computer er der et integreret trådløst netværkskort (Atheros AR6102G-BM2D) til WIFI. 

På undersiden af quadrokopteren er der et 7 benet molex-stik til
ekstern serialkommunikation, \cite{electronic:molex}. Porten virker
bl.a. som en On-The-Go USB-port, \cite{electronic:otg}, der primært
anvendes ved softwareopdateringer.%otg beskrives nærmere i \ref{sec:udvidelse}

\subsection{Strømforsyning}
\label{sssec:forsyning}
\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/batteries.jpg}
  \caption{Batterier til AR.Dronen (Figuren er fra \cite{electronic:techpower}.}
  \label{fig:batterier}
\end{framefig}
Batteriet der driver AR.Dronens enheder er et 3 celle lithium-polymer
batteri med en open-circuit\footnote{Open-circuit spændingen er den
spænding, der er over batteriet, når det er uden belastning og ikke
oplades.} spænding på 11,1 Volt, med ladning på 1000
milli-ampere-timer(mAh) og en afladnings hastighed på 10 Coulomb(C),
\cite{electronic:ar.drone_parrot_technologies}. Opladning af batteriet
kan foregå i løbet af 90 minutter. Efter sigende skulle batteriet give
omking 12 minutters flyvetid, \cite{electronic:wiki_ar.drone}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%      - Hardware -
%%%%      + Fysik - subsection
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{AR.Dronens fysik}
\label{ssec:fysik}
AR.Dronen fåes som nævnt med 2 skrog eller skjold
(figur~\ref{fig:indendoersskjold}), og vejer 420 gram med det

indendørs skrog monteret. 420 gram er en relativ lille vægt i forhold til andre
quadrokoptere, \cite{quteprints33767}, som har 4-5 kg de skal
løfte. 

På en AR.Drone sidder de 4 propeller i hver sit hjørne af en
kulfiberkryds. Når en propel roterer om sin akse flytter den 
luften i et areal rundt om propellen. Lufttrykket på undersiden af
propellen bliver herved større end lufttrykket på oversiden af
propellen. Hvis AR.Dronen ligger vandret vil forskellen i lufttryk
giver en Bernoullieffekt,\cite{nla.cat-vn856146}, som påvirker
AR.Dronen med en kraft modsat tyngdekraften(thrust), 
\cite{electronic:wiki_quadrotor}. Udover thrust påvirker den 
roterende bevægelse også propellens base med et moment(torque)
modsat propellens retning. Eksempelvis vil en helikopter uden haleror
dreje ukontrollerbart rundt om sig selv, men fordi AR.Dronen har et par
rotorer som drejer med uret og et andet par som drejer modsat mod
uret, \cite{techreport:ardroneDevGuide}, bliver summen af
momentpåvirkningen nul, såfremt de 4 rotorer drejer med samme hastighed. 
Denne egenskab bruges også til at styre AR.Dronens flyvning og bevægelser i
rummet, som kan beskrives med 3 bevægelsesakser: x, y og z, hvor
henholdsvis $\theta$, $\phi$ og $\psi$ er de tilhørende
rotationsvinkler. På figur~\ref{fig:drone_movements} forklares hvordan
en quadrokopter i det generelle tilfælde kan bevæges omkring de tre akser, det bemærkes at x og y-akserne på
denne figur er forskudt 45$^{\circ}$ så de flugter med motorerne(for at
lette forklaringen), hvorimod AR.Dronens akser er forskudt så y-aksens positive retning peger i
samme retning som dennes frontkamera, se figur~\ref{fig:drone_akser}.   

AR.Dronens bevægelser styres ved at kontrollere dens vinkel i forhold
til de 3 akser, ligesom på figur~\ref{fig:drone_movements}. Skal
AR.Dronen f.eks. flyve fremad, ændres $\theta$ ved at sænke
omdrejningshastigheden på de to forreste propeller, samtidig med at
omdrejningshastigheden på de to bagerste propeller øges. 
%spørgsmålet er, hvad er moment påvirkningen af $(2*omega_H + delta_A - delta_B) - (2*omega_H)$ 
Dette resulterer i at AR.Dronen hælder lidt fremad. Samme princip gør sig
gældende når AR.Dronen skal flytte sig sideværts. Når AR.Dronen skal
bevæges om z-aksen foregår det som på figur~\ref{fig:drone_movements}d,
det vil sige, at man enten øger eller sænker omdrejningshastigheden på
de propelpar som drejer samme vej, således at det ene pars
momentpåvirkning overstiger det andets og dermed får AR.Dronen til at
rotere. 

\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/drone_movements.png}

  \caption{(a) vinkelhastigheden $\Omega_H$ øges med $\Delta_A$ på
    alle 4 propeller, quadrokopteren øger dermed løftekraften
    (acceleration i Z-aksen), (b) Vinkelhastigheden øges på venstre og
    mindskes på højre propel, quadrokopteren laver et ``roll'' til højre
    (acceleration i Euler-vinklen theta), (c) samme som b, men i pitch
    (acceleration i euler-vinkel phi) (d) En større vinkelhastighed på de
    med-uret-drejende propeller gør at torque bliver ulige og at
    quadrokopteren drejer mod venstre (acceleration i
    yaw/psi-vinklen). Ved bevægelser frem og tilbage udledes en positiv
    eller negativ hastighed ($V_x$), ligesom ved bevægelser sideværts
    ($V_y$). figuren er fra ~\cite{techreport:ardroneDevGuide}.}

  \label{fig:drone_movements}
\end{framefig}

Når AR.Dronen går fra at ligge vandret til istedet at hælde, så
flyttes noget thrust fra at virke lodret imod tyngdekraften til
også at virke sideværts. Det betyder at propellernes samlede
omdrejningshastighed skal øges for at opretholde den samme afstand til
gulvet. Ifølge SDK DevGuide\cite{techreport:ardroneDevGuide} bør $\theta$ og $\phi$ ikke overstige 0.52 rad(30$^{\circ}$)
ellers kan flyvehøjden ikke opretholdes. 

AR.Dronens styreprogram har en parameter der beskriver den øvre grænse for hvor
meget quadrokopteren maximalt må krænge. Denne parameter,
\emph{Euler\_angle\_max }, virker som en cut-offvinkel, dvs. at
AR.Dronen under flyvning løbende tester at hverken pitch ($\phi$)- eller roll
($\theta$)-vinklen overskrider grænsen. Sker dette lukkes systemet ned,
kraften til rotorene stoppes og AR.Dronen falder til jorden. 

Den maximale hældning kan sættes til f.eks. 0.25 rad via AT-kommandoen i figur

\ref{fig:ateulerangle}.

\begin{framefig}
\begin{verbatim}
 AT*CONFIG=605,"control:euler_angle_max","0.25"
\end{verbatim}

  \caption{Eksempel på indholdet af en AT-kommandobesked. Beskeden
    overføres fra klient-enheden til AR.Dronen på UDP-port 5556. Lige
    netop denne besked sætter en parameter i AR.Dronens
    styreprogram, der bestemmer den maksimalt tilladte hældning på $\phi$
    og $\theta$.}   
  \label{fig:ateulerangle}
\end{framefig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Software - subsection

\section{AR.Dronens Software}
\label{ssec:software}
AR.Dronens computer kommer med en Linux 2.6.27 kerne
installeret. Det bemærkes at 2.6.27 er den første kerne med indbygget
support for UBIFS, \cite{electronic:wiki_ubifs}, som er det filsystem der
anvendes på AR.Dronens NAND flash-hukommelse.


\subsection{Busybox Linux platformen på AR.Dronen}
\label{sssec:busybox}
Udover Linux kernen leveres AR.Drone også med
Busybox installeret, \cite{electronic:busybox}.  Busybox implementerer
en række Unix værktøjer (\code{cd}, \code{cp}, \code{ls}, \code{mv}, osv.) 
optimeret til brug i indlejrede systemer. Busybox konfigureres så kun
de ønskede værktøjer medtages og disse pakkes til en enkelt
eksekverbar fil.  Dette betyder, at størrelsen holdes på et
minimum. Busybox på AR.Drone fylder 483 kbyte og udstyrer AR.Dronen
med de brugerkommandoer man forventer på et standard Linux
system. AR.Dronens Busybox leverer desuden en Dynamic Host Configuration
Protocol(DHCP), \cite{rfc2131}, service, som er den DHCP-server AR.Dronen
anvender til at oprette %DHCP cite?  IP\cite{rfc791}-adresser og lave
et netværk mellem en ekstern klient og AR.Dronen selv, samt en
telnet-server, \cite{rfc854}, der giver adgang til AR.Dronens terminal
og en texteditor(Vi) der gør det muligt at redigere filer direkte på
AR.Dronen. 


\subsection{AR.Dronens styreprogram fra Parrot}
\label{sssec:program.elf}
Det styreprogram der står for selve motorkontrollen, transmitteringen af data og
bearbejdning af sensordata i forbindelse med flyvning er implementeret
i \uri{/bin/program.elf}. Da der er tale om proprietært software
betragtes programmet som en lukket kasse uden hensyntagen til
implementeringen. Vi kan dog udlede forskellige elementer efter at
have observeret AR.Dronen under flyvning. Program.elf indeholder blandt andet:
\begin{itemize}
\item en PID controller, \cite{electronic:pid}, \cite{electronic:pid2}, til at stabilisere AR.Dronen når der svæves.
\item en algoritme til at udlede AR.Dronens hastighed ved hjælp af
  bundkameraet, \cite{velocity_estimation_inside_the_drone}.
\item og en algoritme til at detektere tags i billeder i forbindelse
  med augmented reality spil, se figur~\ref{fig:tags}.
\end{itemize}
AR.Dronens styreprogram modtager kommandoer fra brugeren gennem en User
Datagram Protocol(UDP) port, \cite{rfc768}. Kommunikationsinterfacet
mellem bruger og quadro\-kopteren beskrives i afsnit~\ref{ssec:kommunikation}.

\begin{framefig}
  \subfigure[Roundel]{\includegraphics[width=0.30\textwidth]{grafik/tag1.png}
    \label{fig:roundel}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Hull]{\includegraphics[width=0.30\textwidth]{grafik/tag2.png}
    \label{fig:hull}}
 ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[stripes]{\includegraphics[width=0.30\textwidth]{grafik/tag3.png}
    \label{fig:stripes}}

  \label{fig:tags}
  \caption{Visuelle tags der kan genkendes af AR.Dronen, billeder fra
    \cite{electronic:roundel} og \cite{techreport:ardroneDevGuide}}
\end{framefig}

\begin{framefig}
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/telnetscreenshot.png}
  \caption{Forbindelse til AR.Dronens Telnetserver på AR.Dronens
    standard IP addresse $192.186.1.1$.}
  \label{fig:telnetterminal}
\end{framefig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Klient/server kommunikation - subsection


\section{Kommunikation mellem AR.Dronen og klienter} %mellem AR.Dronen og en klient-enhed
\label{ssec:kommunikation}
Interaktionen med AR.Dronens Linux-platform og AR.Dronens styreprogram
kan kategoriseres i 3 grupper:
\begin{itemize}
  \item Gennem en ekstern FTP, \cite{rfc959}, eller Telnet-klient over
    WIFI fra en PC, smartphone eller lignende. 
  \item Gennem en ekstern klient der sender AT-kommandoer\footnote{AT(tention)-kommandoer over
      trådløst netværk (gennemgåes nærmere i sektion
      \ref{sssec:atinterface}}) via UDP protokollen til AR.Dronens
    styreprogram og modtager sensordata fra dette, ligeledes over WIFI fra en PC, smartphone eller lignende.
  \item Gennem en intern klient, som er et program der eksekveres og kommunikerer internt
    på AR.Dronens hardware. En intern klient kommunikerer med
    AR.Dronens styreprogram  gennem de UDP-porte der også anvendes af
    eksterne klienter.
\end{itemize}

Det bemærkes, at så længe der ikke anvendes en intern klient, vil der
altid være tale om en form for fjernstyring af AR.Dronen.

\begin{framefig}
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/ar_drone_nokia900.png}
  \caption{Fjernstyring af AR.Dronen med en Smartphone, her en
    Nokia-telefon. Figuren er oprindeligt fra \cite{electronic:nokia}.}
  \label{fig:nokiastyring}
\end{framefig}

\subsection{FTP og Telnet baseret kommunikation}
\label{sssec:tcp}
Fra en PC eller smartphone er det muligt at få adgang til AR.Dronens 
Linuxinstallation ved at oprette en Telnetforbindelse til AR.Dronens 
faste IP-adresse: $192.168.1.1$. Efter at forbindelsen er etableret 
får brugeren her adgang til en root-terminal (se figur~\ref{fig:telnetterminal}).

Ved at oprette en FTP-forbindelse AR.Dronens FTP-server, ved hjælp af
en FTP-klient, kan man overføre filer mellem AR.Dronen og en PC med
FTP-protokollens PUT og GET kommandoer. Hvis der ikke angives en sti lægges 
overførte filer som standard i \uri{~/data/video} mappen på AR.Dronen.

\subsection{UDP baseret kommunikation}
\label{sssec:udp}
I modsætning til FTP- og Telnet-klienterne, som er TCP baserede, anvendes
UDP-protokollen til den kommunikation der sker mellem AR.Dronens styreprogram og
in- og eksterne klienter.

AR.Dronens styreprogram sender og modtager pakker på tre UDP-porte:
\begin{itemize}
  \item På port 5556 (command port) lytter styreprogrammet efter
    AT-kommandoer afsendt fra klienten, AT-kommandoer gennemgåes i \ref{sssec:atinterface}.
  \item på port 5555 (video port) sender AR.Dronen en videostrøm,
    indeholdende billeder fra èn af fire videokanaler til klienten.
  \item på port 5554 (navdata port) sender AR.Dronen navdatastrøm
    indeholdende statusinformation om f.eks. hastighed, eulervinkler,
    afstand til gulvet og generel information om navigationstilstanden.
\end{itemize}


\subsection{AT kommando interface}
\label{sssec:atinterface}
En klient som skal kommunikere med AR.Dronens styreprogram, gør dette ved hjælp af
såkaldte attention-kommandoer(AT-kommando). En AT-kommando er basalt set blot en
tekststreng repræsenteret som 8 bits karakterer. AT-kommandoer sendes til AR.Dronens
kommandoport(5556) som UDP-pakker. Formatet for AT-kommandoer ses på
figur \ref{fig:atsyntaks}.

\begin{framefig}
\begin{verbatim}
 AT*[kommandonavn]=[sekvensnummer],[arg1, arg2 ... argN]<LF>
\end{verbatim}
\caption{Syntaksen for en AT-kommando: [kommandonavn] kan f.eks. være
  \code{PCMD}, se \ref{tab:kommandoer}. [sekvensnummer]-heltallet skal være stigende for hver
  ny afsendt AT-besked. Listen med argumenter: [arg1, arg2 ... argN],
  varierer i længde afhængig af konteksten. Ved afsending af en
  \code{PCMD}-kommando skal der medsendes 5 talværdier for hhv. flag,
  roll, pitch, gas og yaw.} 
\label{fig:atsyntaks}
\end{framefig}

Ved at afsende en kommando som i figur~\ref{fig:ateksempel} bestemmer
man hvorledes AR.Dronen skal bevæge sig ved translation og rotation,
som beskrevet i \nameref{ssec:fysik}, sektion \ref{ssec:fysik}. 

\begin{framefig}
\begin{verbatim}
 AT*PCMD=21625,1,0,0,0,0<LF>
\end{verbatim}
\caption{Eksempel på en \code{PCMD} AT-kommando. Denne specifikke
  kommando bringer AR.Dronen i hovertilstand. [sekvensnummer]'et er 21625. Listen med argumenter er hhv. flag=1, roll=0, pitch=0, gas=0 og yaw=0.}
\label{fig:ateksempel}   
\end{framefig}
%    [kommandonavn] er navnet på den ønskede kommando
%    [sekvensnummer] er et globalt, altid stigende sekvensnummer
%    [arg1, ... argN] er et variende antal parametre

Brugen, syntaksen og betydningen af 7 forskellige AT-kommando navne
beskrives i detaljer i afsnit 6 af AR.Drone Development
Guide, \cite{techreport:ardroneDevGuide}. Det skal dog nævnes, at
man i reglen skal sikre sig, at man vedbliver at sende beskeder
periodisk for at AR.Dronen ikke skal tolke forbindelsen til
klientenheden som tabt. En tabel med AT-kommandoerne kan ses i tabel
\ref{tab:kommandoer} i appendiks \ref{sec:at}. 

\section{Udvidelse af AR.Dronen med USB-moduler}
\label{sec:udvidelse}
En robotplatform der kan udvides med nye sensorer efter behov, er
klart mere anvendelig end en helt lukket platform. Derfor blev det
undersøgt om det er muligt at anvende AR.Dronens OTG-USB, 
\cite{electronic:otg} port til andet end at lave softwareopdateringer.
Dette har blandt andet indbefattet at kompilere kernemoduler til AR.Dronens linuxkerne,
samt at omgå AR.Dronens software for ikke at miste strømmen til
porten, \cite{blog:usb}. Den indledende proof-of-concept test var at få mounted en almindelig USB-memorystick
og skrive til og læse fra den. Indstallationsprocesuren er forholdsvis
simpel, idet Linuxdriverne til dette formål er meget generiske. Proceduren
til at aktivere AR.Dronens USB-port og læse fra en USB-memorystick kan læses
i en af specialets tilhørende blogpost: "Enabling the Drone USB Port",
\cite{blog:usb}. 
\begin{framefig}
  \centering
  \subfigure[Hjemmelavet USB-kabel, en USB memorystick og en WIFI USB-dongle til tilslutning på AR.Dronen.]{
    \includegraphics[width=0.45\textwidth]{grafik/usbdevices.png}
    \label{fig:cableandusbdevices}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[AR.Dronen med USB memorystick tilsluttet.]{\includegraphics[width=0.45\textwidth]{grafik/usbattached.png}
    \label{fig:usbattached}}
  \label{fig:usbdevices}
\end{framefig}

\subsection{WIFI-sensor-program på AR.Dronen}
Efter de indledende erfaringer med AR.Dronens USB-port, blev der
implementeret en WIFI-sensor for på sigt at kunne anvende denne som 
lokaliseringsenhed. WIFI-sensoren består af en USB-WIFI-adapter
og et snifferprogram til at aflæse og videresende signalstyrkerne fra AR.Dronens
omkringliggende accesspoints og andre WIFI-kilder. 

Signalstyrken af et accesspoints radiosignalet siger noget om hvor
langt modtageren er fra accesspointet. Hvis man ved på hvilken etage
og i hvilket lokale et accesspunkt er placeret, og man modtager et
signal med svag signalstyrke, så ved man at man er langt væk fra den
placering. Hvis man istedet modtager et kraftigt signal fra samme
accespoint, så ved man at man er tæt på placeringen. Lokalisering via WIFI
bruger den egenskab at radiosignalstyrker aftager med sendeafstanden
til at give specifikke lokationer unikke karakteristika, ie. et
fingeraftryk. Et WIFI-fingerprint kommer i form af en 
mængde af accesspoint-identifikationer med tilhørende målte
signalstyrkeværdier på en given lokation. Man kan på forhånd indsamlet
WIFI-fingeraftryk på reference positioner omkring i en
bygning. Reference punkternes tilhørende signalstyrker kan så
senere sammenlignes med et nyeligt indsamlet WIFI-fingerprint
signalstyrker til at finde en tætteste match og dermed ens nuværende
lokation.

For at teknikken skal fungere og for at få fingeraftryk med flest markøre
antager snifferprogrammet at det 
anvendte trådløse interface er sat op i monitor og 
promiscous mode\footnote{Normalt vil et netværksinterface frasortere og
  ikke videresende datapakker der ikke er adresseret til processoren,
  men i promiscous mode sendes al trafik videre.} så det videresender alle pakker der sendes fra de
omkringliggende WIFI-kilder. Netværksinterfacet indstilles gennem et
script (\uri{load.sh}, \cite{github:loadsh}) som kaldes i forlængelse af AR.Dronens 
oprindelige bootup-sekvens. Scriptet sørger også for at indlæse alle de
nødvendige kernemoduler.

\subsubsection{WIFI-sensor hardware}
AR.Dronen er blevet udstyret med en Dlink DWL-G122 WIFI-adapter,
\cite{electronic:dwlg122}. Lige netop denne adapter blev valgt fordi
den bygger på et Ralink chipset, \cite{electronic:wikiralink} og
understøtter monitor mode. Desuden udgiver Ralink linuxdrivere til
deres chipsets. Efter længere tids søgen og eksperimenteren, blev den
korrekte driver fundet og kompileret til ARM-arkitekturen så den var
kompatibel med linuxkernen der anvendes på AR.Dronen. Når driveren er
indlæst, genkendes adapteren og interfacet \emph{ra0} oprettes i
\code{/sys/class/net/}. 

\subsubsection{WIFI-sensor software}
\label{sec:sub:sub:wifisensor}
De første eksperimenter med hensyn til at kompilere og afvikle kode
på AR.Dronen kan ses i specialets blogpost "Compiling code for the
AR.Drone", \cite{blog:compiling}. Det var simple 'hello 
world' eksempler, men det gav indsigt i proceduren omkring
crosscompiling til AR.Drone-arkitekturen og den anvendte toolchain. 

For at skrive et program der kan modtage pakker fra et
netværksinterface anvender man ofte linux' pcap (packet capture)
bibliotek\cite{electronic:pcap}. Dette er dog ikke installeret på
AR.Dronen som standard og skulle først kompileres til ARM-arkitekturen
for så at linke til det og senere anvende programmet på AR.Dronen,
se blogposten \cite{blog:compiling}. 

Arbejdet med at installere et yderligere netværk på AR.Dronen
resulterede i et \emph{WIFI-sensor}-program der køre på AR.Dronen. Det
ny program henter WIFI-pakker som det indstallerede
netværksinterface henter fra luften. De opsnappede WIFI-pakker skilles
ad og MAC\footnote{Media Access Control}-adressen på pakkens afsender samt signalstyrken puttes i en ny
besked, som afsendes fra AR.Dronen. De afsendte pakker er del af en sensorstrøm der
ligner det der ellers kommer fra AR.Dronen, ie. navdata og video.

Det konstruerede WIFI-sensor-program kan opstartes med et interfacenavn som
inputargument, men som standard anvendes \emph{ra0}-interfacet. Man
kan desuden vælge at angive en fast kanal, eller man kan vælge at anvende
channelhopping til at modtage pakker fra forskellige
kanaler. Derudover er det muligt at få tekstuel output til terminalen. 
WIFI-sensor-programmet er lavet så det kan startes automatisk når AR.Dronen booter op.
For ikke unødigt at spilde processorkrafter med at modtage og behandle
pakker, hvis de alligevel ikke skal bruges, så venter WIFI-sensor-programmet efter opstart på et start-signal fra
klienten via en UDP-initialiseringspakke. Modtages startsignalet
begynder WIFI-sensor-programmet at sender signalstyrker og
MAC-adresser videre til klienten. UDP strømmen fra WIFI-sensoren går
over UDP-port 5551. Kommunikationen er valgt til at gå over UDP-port nummer 5551 for at klientens
interface til AR.Dronen forbliver nogenlunde konsistent.

\section{Vurdering af AR.Dronen som robotplatform} % Thomas,
\label{ssec:dronerecap}
Under den indledende informationssøgning til dette speciale og gennem
diverse forsøg og eksperimenter undervejs, er der indsamlet en del erfaringer med
AR.Dronen. Nogle af de mest markante observationer er opregnet her.

\subsection{Hastighed og inerti}
Det giver mening at beskrive AR.Dronens bevægelser i 3
dimensioner for derved nemmere direkte at kunne bruge accelerometer- og gyro-målinger
fra sensorene om bord. Under arbejdet har vi kunnet konstatere, at $\theta$
og $\phi$-værdierne ikke afviger fra det forventede, hvorimod
$\psi$-værdien har en tendens til at blive mere og mere upræcis under
flyvning, hvilket også bekræftes i \cite{electronic:parrotforum3}.

En observeret egenskab er at man ikke kan sige noget udfra
accelerometer og gyroer (de tilstedeværende sensore) om præcis hvor
langt AR.Dronen har fløjet eller med hvilken hastighed AR.Dronen
bevæger sig. Dog kan der udledes estimater for hastigheder ved at integrere over
accelerationen, men hastigheden i det horisontale plan er i praksis
afhængig af miljøet og kan ikke umiddelbart bestemmes. Roll og pitch
værdierne kan være aflæst til 0 og AR.Dronen vil ligge horisontal, men
kan stadig godt bevæge sig sideværts i luften\footnote{AR.Dronen siges
  at være ``trimmet'' (med en konstant hastighed, evt. $0m/S$, men også forskellig fra $0m/S$) hvis den
  ikke påvirkes af en resulterende kraft og acceleration dermed er 0}
på grund af indebåren inerti eller f.eks. en stille konstant
vind. Bevæger AR.Dronen sig med en konstant hastighed kan det ikke
aflæses af accelerometeret. Afstanden AR.Dronen har flyttet sig kan
således heller ikke udledes af accelerometer og gyro alene. Parrot har
derfor underbygget hastighedsestimatet med en optical-flow algoritme i
styreprogrammet, der bearbejder data fra bundkameraets billedstrøm,
således at hastighedsestimatet bliver mere troværdigt, \cite{velocity_estimation_inside_the_drone}. 

Dette speciales egne eksperimenter med hastighedsudledning er beskrevet i
afsnit~\ref{sec:distance-mesu}. Det viser sig at under de rette 
forhold er hastighedsudledningen udmærket brugbar. Udledningen af
hastighed besværliggøres ved forhold med dårlig belysning og ved
jordoverflader uden forskelligartet tekstur, hvor det ikke er muligt for
bundkameraet at detektere gode features at følge.

\subsection{Afstandsmåleren}
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/sonarzone2.png}
  \caption{a) Udbredelsesområde for ultralydssensorsignal, b) man ser
    at afstanden ikke nødvendigvis er vinkelret på sensoren idet der kan
    forekomme ekko fra et større område under sensoren (figuren er fra
    \cite{electronic:brown})}
  \label{fig:sonicbeam2}
\end{framefig}
Den nedadrettede afstandsmåling varetages som nævnt af en ultralyds-sender og
-modtager. Teknologien gør at afstanden man får ud ikke nødvendigvis er
fra det vinkelrette punkt udfor sensoren, men derimod returnerer den
mindste afstand indenfor et område som det på figur
\ref{fig:sonicbeam2}. Det vil sige højden er udledt af den tid der går
fra en lydpuls afsendes, til der modtages et ekko. Ekkoet behøver ikke
komme fra en stor overflade som en væg eller et gulv, men kan være en
kasse inde i sensorområdet som figur~\ref{fig:sonicbeam2}a. Det kan
specielt være et problem hvis der flyves i et trangt lokale med
kasser, stole og borde på gulvet, hvor de mange legemer vil give en
falsk opfattelse af højden til gulvet.  Fordi teknologien netop er lyd
vil der også være problemer med ekko på skrånende og bløde
overflader.

De ovennævnte problematikker kan have indflydelse ved udvikling med
AR.Dronen. Specielt i de tilfælde quadrokopteren hælder meget, f.eks. ved
flyvning i høj hastighed kan vinklen til gulvet indføre fejl i
højdemålingen.

\subsection{Realtidssystem} 
På forummet AR Drone Flyers, \cite{electronic:ardroneflyers80}, har en bruger skrevet,
at Linuxinstallationen, styreprogrammet og de eksterne processer der kører på
AR.Dronen tilsammen bruger op mod $80\%$ af CPU-kraften.  Om det lige
akkurat er $80\%$ kan formentlig diskuteres. Under alle omstændigheder
skal man tage højde for den begrænsede CPU-ressource, hvis man vælger at afvikle
yderligere processer på AR.Dronen, som f.eks. USB-understøttelse og en
WIFI-sensor, som beskrevet i afsnit~\ref{ssec:wifilokalisering}. Pointen er at man ikke kan bruge
mere end de resterende f.eks. $20\%$ uden at det vil gå ud over
kontrollen med flyvningen.

\subsection{Trådløs båndbredde} 
På grund af den manglende transmissionskontrol operer UDP-protokollen
med noget et forholdsvist lille overhead. Det gør, at der i
princippet kan sendes flere f.eks. videopakker end med en
synkroniseret kommunikationsprotokol som TCP. Oveni at der
kontinuerligt sendes friske datapakker, så giver det mest mening at
anvende en asynkron protokol i realtidskommunikationen med
AR.Dronen, idet det ikke kan betale sig at bruge resourcer på at
generhverve det fåtal af billedframes der går tabt pga. fejl i
transmissionen. Det er bedre at indlæse den næste datapakke
istedet. Information i den gamle datapakke vil alligevel være forældet
% Kan den måles?, hvad er den teoretisk effektive overførselshastighed?
%
og irrelevant. Vores erfaring har også været, at vi har modtaget både
video og navdatastrømmen med en passende framerate til vores formål.
\todo{kontrol af manual merge morten?}

\subsection{Billedkvalitet} AR.Drone er et produkt der er designet til
at skulle være billig i produktion for dermed let at kunne erhverves af menigmand. I
dette valg af design har Parrot afvejet et tradeoff mellem
videokvalitet of framerate. Det
betyder, at opdateringshastigheden af billeder og data fra AR.Dronen er
tilstrækkelig i de fleste tilfælde, men, som det fremgår afafsnit~\ref{sec:QRdecoding}, er billedkvaliteten ikke tilstrækkelig
til f.eks. afkodning af QR-koder, \cite{electronic:wikiqr}.

\begin{framefig}
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/2frames.png}
  \caption{Et billede fra AR.Dronens bundkamera (med opløsning på
    $88\times72$ pixels) i øverste venstre hjørne af et billede fra
    frontkameraet ($320 \times 240$ pixels).}
  \label{fig:2frames}
\end{framefig}

%\subsection{Zap imellem 2 kameraer} I forbindelsen mellem klient og
%AR.Dronen kan man kun modtage en billedramme ad gangen. Dvs. at enten
%skal man få billeder fra frontkameraet eller fra bundkameraet. -- Skal
%man bruge billeder fra begge kameraer, må man skifte imellem dem
%(zappe) og skiftevis få et billede fra frontkamera hver anden gang og
%bundkameraet de andre gange.  Dette er ikke praktisk idet det vil
%nedsætte opdateringshastigheden yderligere i eksempelvis en
%feedback-kontrol løkke. Ydermere er der et lille delay forbundet med
%at skifte feed fra fra det ene til det andet kamera.  Insisterer man
%alligevel på at få billeder fra begge kameraer samtidig, er det dog
%muligt. Billeder fra de to kameraer lægges ovenpå hinanden til eet
%billede (se figur~\ref{fig:2frames}), som så kan modtages af klienten.
%Det billede der er indeholdt i det andet er af noget mindre opløsning
%($88 \times 72$) end hvis det havde været sendt alene ($320 \times
%240$). Denne forskel i kvalitet har tydeligt en indflydelse på
%resultatet af diverse billedanalysealgoritmer.

\subsection{Zap imellem 2 kameraer} 
I videostrømmen mellem en klient og AR.Dronen sendes kun et billede ad
gangen. Man kan modtage fire forskellige slags billeder: fra
frontkameraet, fra bundkameraet, fra frontkameraet med bundkameraets
billede overlagt i øverste venstre hjørne og fra bundkameraet med
frontkameraets billede overlagt i øverste ventre hjørne. Skal man bruge billeder i fuld størrelse fra
begge kameraer, må man skifte imellem dem (zappe) og skiftevis få et
billede fra front- og bundkamera. Dette er ikke praktisk, idet det vil nedsætte
opdateringshastigheden i eksempelvis en feedback-kontrol
løkke. Ydermere er der et lille delay forbundet med at skifte feed
fra det ene til det andet kamera. Anvender man de kombinerede
billeder, se figur~\ref{fig:2frames}, skal man være opmærksom på
billede der er indeholdt i det andet, er af noget mindre opløsning end det originale($88
\times 72$ pixels for bundkameraets billede når det er lagt ovenpå
frontkameraets, mod $176 \times 144$ pixels originalt). Denne forskel
i kvalitet har tydeligt en indflydelse på resultatet af diverse
billedanalysealgoritmer, se sektion~\ref{ssec:smallmarks}.
%
%% sjovt at vi i begyndelsen af arbejdet hele tiden har sagt at nu
%% skal vi også huske at teste præcist hvor meget den drifter i yaw
%% piezo-måleren, men enten aldrig rigtig er kommet til det punkt
%% eller også kommet forbi det - ved at acceptere det og så styre udenom
%
%Derefter finde ud af at jo mindre des mindre inerti kan være indeholdt, mere agile agil/adræt
%gå fra centraliseret styring til fosøg med decentraliseret, swarm anonymitet, største proble er sanse apparatet, førhen flere ekserne kameraer til observation og feedback af position til flokken, nu forsøg med lokal dataindsamling fra bl.a. Kinect {http://www.ros.org/wiki/kinect}, {pelican quadrotor}

%strømforsyning, det er godt med 9 batterier, og at have dem opladt inden dagens arbejde, der er lige knap 12 minutters flyvetid - ``så erder i alle fald heller ikke mere''...

\todo{afsluttende bemærkning... evt. reference yderligere
  behandling/opsummering i Resultater/Konklusion...}
 
\chapter{Klient platform} % Morten,
Den grundlæggende platform er udviklet med det formål, at
understøtte det videre arbejde med AR.Dronen. Platformen skal sikre
let og effektiv adgang til AR.Dronens sensor output, samt give
mulighed for at styre AR.Dronen, både manuelt med joystick og tastatur
gennem en PC (altså ikke iPhone, iPad el. lign.) og ved hjælp af
forprogrammerde sekvenser af bevægelsaer %prædefinerede opgaver
 som beskrives nærmere i sektion~\ref{sec:tasks}.

\section{Platformsadgang via Python}
Python sproget blev i første omgang valgt for at lette tilgængeligheden for
brugeren. Et af specialet mål er at stille en let anvendelig og
let udbyggelig platform til rådighed for udviklermiljøet omkring
AR.Dronen. 

\subsection{Nem fjenstyring af AR.Dronen i Python}
\label{sec:sub:fjernstyrpy}
Et simpelt eksempel på anvendelse af vores kontrolinterface gives ved at
starte pythons fortolker, importere examples-modulet og herefter
afvikle en metode indeholdende kommandoer til
AR.Dronen. Fremgangsmåden ses på figur~\ref{fig:pythoninterpex1}.

\begin{framefig}
    \begin{verbatim}
>>> import examples as e
>>> e.square()
    \end{verbatim}
\caption{Import og afvikling af simpelt kontrolinterface-eksempel i pythonfortolkeren}
\label{fig:pythoninterpex1}
\end{framefig}

Metoden \code{square()} der kaldes i eksemplet i figur~\ref{fig:pythoninterpex1},
lader AR.Dronen lette, flyve rundt i en firkant og for herefter at
lande igen(under antagelse af at klientcomputeren er forbundet med AR.Dronens
trådløse netværk). Bevægelsesmønstret opnåes ved at der kaldes en
metode(\code{take\_off}, \code{move} eller \code{land}) på kontrolinterfacet, hvorefter der er en pause
før den næste metode kaldes. Metodekaldende oversættes af interfacet til
AT-kommandoer og sendes så som UDP-pakker til AR.Dronens
kommandoport. Den konkrete implementation af \code{square()} ses i
figur~\ref{fig:pythonimpl1}. Det bemærkes at kontrolinterfacetråden
skal startes (kald af \code{c.start()}) før kommandoerne har nogen effekt.

\begin{framefig}
  \lstset{language=Python, frame=none}
  \begin{lstlisting}
def square():
    import controllers
    import time

    c = controllers.ControllerInterface()
    c.start()

    print 'taking off'
    c.take_off()
    time.sleep(5.0)
    print 'moving Forward'
    c.move(0.0, 0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving right'
    c.move(0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving back'
    c.move(0.0, -0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving left'
    c.move(-0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'landing'
    c.land()

    c.stop()
\end{lstlisting}
\caption{Python implementation der importerer og anvender vores kontrolinterface til en
  simpel flyvning}
\label{fig:pythonimpl1}
\end{framefig}

\subsection{Modtagelse af sensordata fra AR.Dronen i Python} % Introduktion til modtagelse af video, navigationsog wifidata fra AR. Dronen
I det følgende gennemgåes mulighederne for at modtage datapakker med video, navigation- og wifi-data fra AR.Dronen.
For at kunne anvende AR.Dronen som en robot er det nødvendigt at kunne
indsamle data fra de ombordværende sensorer, samt at kunne stille
disse data til rådighed for andre dele af systemet. Til dette formål
har vi designet og implementeret en receiver-struktur bestående af tre
receiver-moduler, disse moduler eksekveres i hver deres process og
sørger selv for at initiere forbindelsen med dronen. Efter
initialiseringen lytter hver receiver efter enten video-, navigations-
eller wifi-pakker og når en sådan modtages udføres en passende
afkodning af pakken og den afkodede data deles med processen der
startede receiveren. De tre receivere er ikke
afhængige af hinanden og kan afvikles hver for sig, ligeledes kan vi
vælge at afvikle vores samlede system med en eller flere receivere
tilkoblet. Sektion~\ref{subsection:receiver} giver et eksempel
på hvordan man lettest anvender videoreceiveren, til at modtage og vise
et enkelt billede fra AR.Dronens videostrøm. Anvendelsen af de to
andre receivere foregår på fuldstændig samme måde som videoreceiveren,
den eneste forskel er typen af data der returneres. 

\subsection{Eksempel på modtagelse af videostream}
\label{subsection:receiver}
Eftersom vores videoreceiver kan anvendes for sig selv, vises her et
eksempel på hvor få linier kode man behøver for at modtage og vise et
billede fra AR.Dronens kamera. Eksemplet antager at der
allerede er oprettet en virkende forbindelse med AR.Dronens trådløse
netværk. Eksemplet startes fra Pythons kommandopromt.  
\begin{framefig}
\begin{verbatim}
>>> import examples as e
>>> e.receive_and_show_picture()
\end{verbatim}
\caption{Import og afvikling af simpelt videoreceiver-eksempel i pythonfortolkeren}
\label{fig:pythoninterpex2}
\end{framefig}

Som det ses på figur~\ref{fig:pythonimpl2} kræver det tre liniers reel
kode for at modtage et billede(og endnu fire for at vise det), vi
mener at dette fint illustrerer, hvordan vi med dette abstraktionslag kan
tilbyde et let anvendeligt interface til AR.Dronens forskellige
sensoroutput.   
\begin{framefig}
  \lstset{language=Python, frame=none}
  \begin{lstlisting}
def receive_and_show_picture():
    import receivers, settings, time
    import cv2.cv as cv
   
    video_sensor = receivers.VideoReceiver(VIDEO_PORT)
    video_sensor.start()
    time.sleep(1)
    pic = video_sensor.get_data()
      
    cv.StartWindowThread()
    win = cv.NamedWindow('win')
    cv.ShowImage('win', cv.fromarray(pic))
    cv.WaitKey()
    cv.DestroyWindow('win')

    video_sensor.stop()

  \end{lstlisting}
  \caption{Python implementation der importerer og anvender
    videoreceiver til at modtage og vise et billede}
\label{fig:pythonimpl2}
\end{framefig}


\pagebreak

\section{Platformens opbygning}
Platformen består af et antal klasser placeret i et antal
Python-moduler som beskrives nærmere i det følgende. Her er
indledningsvis en kort introduktione for overblikket skyld. Der er
udviklet tre forskellige receiverklasser til at modtage henholdvis video, 
navdata og WIFI-målinger disse beskrives under
\nameref{subsec:receivers}, sektion \ref{subsec:receivers}. Der er udviklet to forskellige
controllerklasser til joystick og keyboard som beskrives i sektion
\nameref{subsec:controller} \ref{subsec:controller}). Der er
kontrueret en taskmanager (sektion~\ref{subsec:taskmanager}) med ansvar for at opstarte autonome
delopgaver heri kaldet task(tasks beskrives i
sektion~\ref{sec:tasks}). For at styre AR.Dronen skal
kontrolinterface-klassen bruges. Et simpelt eksempel herpå er allerede
vist i figur~\ref{fig:pythonimpl1}. Controllerinterface-klassen
gennemgåes i sektion~\ref{sec:sub:controlinterface}).
 Med inspiration fra~\cite{lenser:behaviorbasedarchitecture} og
 \cite{Dey01:ctoolkit} er der udover de allerede nævnte grundreceivere
 implementeret en virtuel sensor (sektion~\ref{sec:sub:virtuel}) som
 bruges til at fortolke datastrømmene fra de almindelige receivere.

Platformens klasser instantieres, startes og tilgåes som udgangspunkt
gennem droneklassen (dronemodulet). På denne måde kan udviklere der ønsker at
anvende platformen blot importere drone modulet og instantiere
droneklassen, hvorefter der er let adgang til diverse sensordata, se
sekvensdiagrammet i figur~\ref{fig:initialiseringssekvens}.
\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/initialisering.png}
  \caption{Klientapplikationens entrypoint til platformen, er gennem
    instantiering af drone-klassen, som sørger for at instantiere
    resten af objekterne og processerne i platformen.}
  \label{fig:initialiseringssekvens}
\end{framefig}
Udover ovenstående er der lavet et testdevice modul, som anvendes i forbindelse
med udvikling, når det ikke er muligt at have en AR.Drone fysisk
tilstede. Sektion~\ref{sec:sub:testdevice} beskriver muligheden for at
simulere output fra AR.Dronen gennem testmodulet.
% De enkelte klasser kan også anvendes hver for sig, se*.

%klasse diagram eller lignende


\subsection{Datastrømme-modtagere på klienten}
\label{subsec:receivers}
Subklasser af \emph{Receiver} er abstraktioner over de fysiske hardwaresensorer på
AR.Dronen. Receivernes opgaver er at initialisere UDP-kommunikationen
med AR.Dronen ved at sende en startbesked og herefter modtage de
datapakker AR.Dronen sender. Desuden skal datastrømmen holdes i live
ved at sende startbeskeden \todo{andet ord end startbesked: watchdog-signal?} i faste intervaller. 
\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/receiveruml.png}
  \caption{Klassehierakiet for receiverklasser. Nedarvende klasser kan
    specialisere sig ved at override de to metoder
    \code{on\_receive\_data} og \code{on\_request\_data}.}
  \label{fig:receiveruml}
\end{framefig}
Receiverklasser nedarver fra en baseklasse: Receiver (se figur \ref{fig:receiveruml}), der indkapsler
den basale receiverfunktionalitet. For bedre at kunne udnytte en
multicore processor er denne baseklasse implementeret som en
selvstændig proces ved hjælp af Pythons
multiprocessing-modul. Receiverprocessen kommunikerer ved hjælp af en
delt liste, som anvendes både til at overføre signaler og data.  For
at tilpasse de enkelte receivere implementerer disse specifikke
metoder (for eksempel til afkodning af den specifikke type data) der
kaldes på bestemte tidspunkter under det generelle receiverloop.
 
\subsubsection{Navdata-modtager}
\emph{NavdataReceiveren} modtager navdatastrøm (AR.Droens afstand til
jorden, flyvehastighed i x- og y-retning m.m.) via UDP fra
AR.Dronen og decoder denne, i forhold til de structs der findes
beskrevet i headerfilen \uri{navdata\_common.h}, \cite{ardroneorg:navdatacommonh} fra Parrots åbne API.

\subsubsection{Video-modtager og billedafkoding via Psyco}
\emph{VideoReceiveren} modtager og afkoder AR.Dronens videostrøm. Afkodningen
af billederne foregår på baggrund af den process der nævnes i sektion 7.2 side 43 i
\cite{techreport:ardroneDevGuide}. 

\paragraph{JIT-kompileret afkodning af billeder}
%%% % % %
Da Python er et fortolket højniveau sprog er det ikke muligt at opnå
samme afviklingshastighed som ved afvikling af C eller andre
maskinnære sprog. I forbindelse med afkodningen af video 
fra AR.Dronen anvendes biblioteket Psyco~\cite{electronic:psyco}
til netop at øge afviklingshastigheden. Psyco er en form for JIT
compiler hvormed man kan afvikle umodificeret Pythonkode hurtigere. Til sammenligning vil
almindelig afviklet Python-kode til afkodning af 100 billeder fra
AR.Dronens billedstrøm tage omkring 37.569 sekunder men hvis det samme
Python-kode afvikles med hjælp fra Psyco er afviklingstiden kun 7.075
sekunder. Ved brug af Psyco kan man altså hente og afkode 5 gange så mange
billederammer fra AR.Dronen til brug på klienten. Psyco er lavet til
32 bit maskiner og virker fint der, men desværre har udvikleren af
Psyco valgt ikke at lave en 64-bit version af biblioteket. 

Videoformatet der sendes fra AR.Dronen minder meget om JPEG formatet og 
forfatteren til den algoritme vi har tilpasset, spekulerer
på forumsiden, \cite{electronic:parrotforum2}, om det ville være muligt at anvende
en decideret JPEG decoder til afkodningen. Dette kunne gøre det muligt
at anvende et optimeret C bibliotek til formålet for derved at øge
afkodningshastigheden og samtidig eliminere afhængigheden af Psyco som
kun fungerer på en 32-bit platform.


\subsubsection{WIFI-signalstyrke modtager}
Som de andre receivere modtager \emph{WIFIReceiveren} en datastrøm fra
AR.Dronen. WIFI-strømmen er dog en tilføjelse vi har lavet og formatet
der skal afkodes er blot en tekststreng bestående af en MAC-adresse og
en signalstyrke, der modtages således en pakke for hver pakke
det indlejrede WIFI-sensor-program(beskrevet i
sektion~\ref{sec:sub:sub:wifisensor}) opsnapper. WIFI-receiveren
skiller sig ud fra de 
andre receivere, ved at placere de modtagne adresse/signal-par i en
liste og holde denne liste opdateret med de nyeste signalværdier for
hver adresse, for hver adresse gemmes desuden de sidste *X* modtagne værdier,
deres gennemsnit, deres varians og deres middelværdi. Når
wifireceiverens get\_data() metode kaldes, er det disse værdier der
returneres, værdierne finder anvendelse når wifidata skal matches med hinanden.

\subsubsection{Virtuel sensor}
\label{sec:sub:virtuel}
%\cite{Dey01:ctoolkit}\cite{lenser:behaviorbasedarchitecture} #Bardram jcaf?
Den virtuelle sensor er en sen tilføjelse til platformen og er som
tidligere nævnt inspireret af sensorhierakierne beskrevet i
\cite{lenser:behaviorbasedarchitecture} og \cite{Dey01:ctoolkit}, hvor
sensorer opererer på forskellige niveauer og derfor leverer sensor
output på forskellige 
abstraktionsniveauer. I modsætning til receiverne modtager den
virtuelle sensor ikke data direkte fra AR.Dronen, men derimod netop
fra receiverne. Den virtuelle sensor bearbejder kontinuerligt eg.
billeddata for at genkende henholdsvis markører på jorden og
silhuetter umiddelbart foran AR.Dronen. Sensoren leverer således
information om AR.Dronens omgivelser på et højere abstraktionsniveau
end de rå værdier fra receiverstrømmene. 

For at kunne detektere både markører på gulvet og silhuetter er det
nødvendigt for sensoren hele tiden at skifte mellem bund- og
frontkamera. Dette giver
imidlertid problemer for Parrots styreprogram der ved konstante kameraskift løbende øger sit
hukommelsesforbrug indtil Linuxkernen lukker programmet ned. Vores
målinger viser, at AR.Dronens program kun kan køre mellem ti og tolv
minutter, såfremt vi samtidig skifter kamera med cirka 5
millisekunders interval. Selvom om dette problem ikke beskrives
direkte nogen steder, antydes det dog af en udvikler på Parrots forum,
at denne funktionalitet ikke er ment anvendt til hurtige kameraskift ved
hjælp af software og at der derfor er stor sandsynlighed for at metoden
ikke virker pålideligt, \cite{electronic:parrotforum1}.

\subsection{Python interface til fjernstyring af AR.Dronen}
\label{sec:sub:controlinterface}
\todo{Uddybes en del, med referencer til eksempler med AT-kommando
  beskeder. Scratch that, reference til det simple ekempel med Python
  fjernstyring i firkant figur~\ref{fig:pythonimpl1}, eller hvad?}
Ved hjælp af AT-kommandoerne definerer AR.Dronen et interface til
enheder der er indenfor rækkevidde af dens trådløse netværk. For at
gøre dette AT-interface mere praktisk anvendeligt har vi videreudviklet
på en Python-wrapper, \cite{electronic:venthur}, til dette interface. Vi har implementeret
interfacet som en Python tråd der løbende sender kommandoer med de
aktuelle kontrolværdier til AR.Dronen. 
Dette betyder at platformen ikke afhænger af at alle
kontrolpakker når deres destination, hvilket er praktisk da
UDP-protokollen netop ikke garanterer at en pakke når sin destination. %think I understand but humor me and elaborate...
Ved at sende al kontrolkommunikation gennem interfacet, behøves vi heller ikke at tænke på detaljer som sekvensnumre og
watchdog-beskeder, da disse håndteres transparent af systemet.
  
Python-interfacet: \code{ControllerInterface} i filen \cite{github:controllers} definerer en række metoder til at
fjernstyre AR.Dronens opførsel og bevægelser. Alt styring af AR.Dronen
fra platformens tråde går altid forbi den samme ene instans af
Python-interfacet se figur~\ref{fig:ciagregateuml}. Udover at virke i platformen kan interfacet til
fjernstyring i det simpleste tilfælde også anvendes alene direkte fra
Pythons interpreter, som beskrevet i eksemplet
sektion~\ref{sec:sub:fjernstyrpy}.

\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/controllerinterface_agregate_uml.png}
  \caption{Singeltonklassen ControllerInterface til fjernstyring af
    AR.Dronen.}
  \label{fig:ciagregateuml}
\end{framefig}

\subsection{Controllers}
\label{subsec:controller}
Selvom det er muligt, er det ikke praktisk anvendeligt at brugeren skal
afvikle scripts, der anvender kontrolinterfacet, for hver ønsket
bevægelse (som det der er beskrevet i eksemplet i
figur~\ref{fig:pythonimpl1}), i stedet er der implementeret to
controllere til at tage mod input fra brugeren, ie.
en keyboardcontroller der lytter efter input
fra tastaturet og en joystickcontroller der er tilknyttet en fysisk
Xbox360-controller (se figur~\ref{fig:xboxlayout}).

Man kan starte systemet op med begge controllere på samme tid, eller
undlade en eller begge. Under udviklingen af systemet har vi oftest
anvendt begge controllere sideløbende, da de komplimenterer hinanden.
Man kan som sagt anvende platformen helt uden controllere, og udelukkende anvende
taskmanageren til at kontrollere AR.Dronen, men skulle der i det
tilfælde ske noget uforudset under udførslen af en opgave, vil der
ikke være nogen mulighed for at bringe AR.Dronen manuelt (og sikkert) ned. 

\begin{framefig}
  \centering
  \includegraphics[width=0.6\textwidth]{grafik/controlleruml.png}
  \caption{Klassehieraki for klasser der nedarver fra
    Controller-klassen. Underklasser af Controller er kendetegnet ved
    at have adgang til styring af AR.Dronen gennem controlinterfacet.}
  \label{fig:controlleruml}
\end{framefig}

Begge controllerklasser nedarver fra en grundlæggende
controllerklasse: \code{Controller} (se
figur~\ref{fig:controlleruml}). Denne er implementeret som en tråd der
med et givent tidsinterval kalder en kontrolmetode (\code{process\_events()}). Den simple
opbygning betyder, at hvis man ønsker at implementere en anden type
controller, er det nok at nedarve Controller og implementere
\code{process\_events()} metoden. Da Python behandler metoder som
førsteklasses objekter, er det let at lade en controller skifte mellem
forskellige tilstande. Dette gøres ved at kalde \code{set\_control\_method} med
en ny kontrolmetode som argument. 

\subsubsection{Joystickcontroller}
I platformen anvendes Joystick istedet for en iPhone eller
iPad til at fjernstyre AR.Drone manuelt i forbindelse
med testflyvninger. Der var flere praktiske grunde til at vi valgte at
implementere en alternativ fjernstyring. Blandt andet havde vi et
behov for at kunne interagere både med vores eget system og AR.Dronen
på samme tid, mens vi til stadighed måtte have øjne på AR.Dronen. Vi
fandt desuden ikke iPhone styringen tilstrækkelig præcis eller særlig
intuitiv. Xbox360 controlleren derimod omtales som en af de bedst
designede controllere og har endda fundet anvendelse af militære
styrker i USA og storbritannien, \cite{electronic:zdnet},
\cite{electronic:joystiq}, \cite{electronic:youtube1}.
Fjernstyringsdelen er meget simpel og fungerer ved at oversætte
værdier fra Xbox360 controlleren (som vi får givet af pygames, \cite{electronic:pygame},
joystickmodul) og så sende disse til
fjernstyrings-interfacet (ControllerInterface). Xbox360 Joystick-controlleren
bruges udover manuel fjernstyring af AR.Dronen også til at starte
forprogrammerede handlingssekvenser, ie. tasks (task beskrives nærmere
i sektion~\ref{sec:tasks})).
Således kan piloten manuelt bringe AR.Dronen i en ønsket position og så herefter starte den automatiske
afvikling af et forudbestemt bevægemønster. 
På figur~\ref{fig:xboxlayout} ses vores standardopsætning af Xbox360
controlleren.
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/xbox360layout.png}
  \caption{Standardopsætning af Xbox360 controlleren}
  \label{fig:xboxlayout}
\end{framefig}

\subsubsection{Keyboardcontroller}
Hvis man ønsker at starte platformen op uden grafisk brugerflade (og
tilhørende GTK keyboardbindinger), er det
praktisk stadig at kunne anvende tastaturet som input (evt. i kombination med Xbox360
controlleren). Vi har derfor implementeret en simpel
keyboardcontroller der læser input fra terminalen. Vi anvender ikke
keyboardet til direkte styring af AR.Dronen (da Xbox360
controlleren er mere intuitiv og nu alligevel er tilgængelig), men
istedet til ting som at få udskrevet nuværende batteriniveau ('b'),
til at skifte kamera ('z'), til at få udskrevet aktive tasks('c'), til
at få udskrevet en oversigt over tråde i programmet('t'), samt til at
starte tasks ('1', '2' og '3'). %%%%% <ins>task</ins> <del>opgaver på autocontrolleren</del>

\subsection{Taskmanager}
\label{subsec:taskmanager}
Taskmanagerklassen startede ud som en almindelig controller, men er endt
som en mere selvstændig del af platformen. Taskmanageren skiller sig
ud ved ikke, i modsætning til joystick- og keyboardcontrollerne, at
arbejde direkte med kontrolinterfacet eller med output fra
receiverne. Taskmanagerens opgave er udelukkende at starte
forudspecificerede opgaver (tasks; som gennemgåes i sektion~\ref{sec:tasks}) og stoppe dem igen, samt at
facilitere beslutningen om hvilken task der har lov til at bevæge AR.Dronen. 

\subsection{Testdevice}
\label{sec:sub:testdevice}
Dette modul muliggør gennemførelsen af simple simuleringseksperimenter i et
test miljø. Grundideen bag modulet er, at man skal kunne optage, ikke
bare film, men alle sensoroutput fra en flyvning med AR.Dronen, for så
senere at kunne afspille hele flyvningen igen. Selve afspilningen
foregår næsten transparent for resten af platformen, receiverne skal
stadig initiere kommunikationen (nu blot med testdevicet og ikke den
reelle AR.Drone), forbindelsen
skal stadig holdes i live og dataen der modtages skal stadig
afkodes. De eneste synlige forskelle der er mellem test og
live-sessions er timing og hastighed, den første fordi vi ikke har
prioriteret at afsende testdataen i de intervaller de blev modtaget i
og den sidste fordi samme computer nu står både for afsendelse,
modtagelse og afkodning.  Testdevicemodulet har ikke noget at gøre med
optagelsen af sensoroutputtet, denne foregår i receiverklasserne.


%\subsection{Praktisk anvendelse af Platformen}
%        Introduktion til styring af AR.\ $\!$Drone med Python}

%           Eksempel på anvendelse af Python interface til AR.Dronen

\section{Tasks} % Morten
\label{sec:tasks}
For at indkapsle forskellige bevægemønstre og for at kunne koordinere
en parallel afvikling af disse, har vi implementeret et hierakisk
opbygget tasksystem. Vi har valgt at kalde vores enheder for tasks
istedet for behaviors ellers bruges i ~\cite{lenser:behaviorbasedarchitecture}, grunden til dette er at flere af vores tasks er
forholdsvis kortlivede(for eksempel take-off og landtasks) og derfor
mere minder om en opgave der skal forberedes, udføres og afsluttes,
dette ses i modsætning til en behavior som kan opfattes som en
længerevarende opførsel. Vi har også implementeret højniveau bevægemønstre der
minder mere om en traditionelle behaviors(for eksempel
FollowTourTask), men vi mener også at det giver mening at kalde disse
tasks, da de også implementeres efter vores generelle taskstruktur.

Vores første udgaver af tasksystemet indbefattede tasks der indsamlede
og behandlede data fra AR.Dronens sensorer, i den endelige udgave er
disse tasks dog udskilt til den virtuelle sensor, så tasks ikke skal
behandle sensordata, men kun reagere og formidle bevægelse på baggrund
af højniveau-sensordata. 

\subsection{Task typer} 
Vi har to hovedtyper, simple og compoundtasks. Simple tasks nedarver
fra vores basetaskklasse Task. De repræsenterer oftest en simpel
bevægelse, men kan være vilkårligt komplekse, for eksempel har vi en
task der blandt andet implementerer en PID-controller. Compoundtasks
er tasks der indeholder en liste af undertasks og funktionalitet til
at starte og stoppe de underliggende tasks. Vi anvender to typer
compoundtasks, sekventielle og parallelle. Sekventielle compoundtasks
starter deres undertasks en efter en, men venter med at starte en ny
undertask før den forrige er afsluttet, en sekventiel compoundtask
stopper når den sidste undertask er afsluttet. Parallelle compoundtask
starter alle deres undertasks på en gang og stopper også når den
sidste undertask har afsluttet. 

\subsection{Task hieraki}
Ved hjælp af de ovenfor beskrevne tasktyper, og specialiseringer af
disse, kan vi danne en task træstruktur der minder om den beskrevet i
\cite{lenser:behaviorbasedarchitecture}. Selvom om de basale
træstrukturer minder om hinanden, er der dog adskillige forskelle på
algoritmen der bestemmer hvilken task/behavior der skal
aktiveres. 

Tilgangen i \cite{lenser:behaviorbasedarchitecture} er at
en aktiveret behavoir skal bestemme hvilke af sine underbehaviors der
skal aktiveres, dette er praktisk, da det med deres robot er muligt at
udføre forskellige bevægelser på samme tid og det bedste sæt af
parallelle bahaviors skal beregnes. I vores tilgang operer vi ikke med
flere aktive tasks samtidig(de bevægelser vi har til rådighed kan ikke udføres
uden indflydelse på hinanden) og generelt bliver vores træer ikke
ligeså dybe som \cite{lenser:behaviorbasedarchitecture}, da vores
interface til AR.Dronen er på et højere abstraktions niveau. Vores
tilgang indbefatter at alle tasks kører samtidig(alle tasks er
implementeret som Python tråde), men at maksimalt en
task har tilladelse til at flytte på AR.Dronen, hver gang en task
ønsker at flytte AR.Dronen spørges op  i træet efter tilladelse. Alle
tasks på samme niveau(med samme forælder) er ordnet i et fast hieraki
og en højere rangeret task kan altid kræve kontrol og dermed
undertrykke lavere rangerende tasks.

På ~\ref{fig:taskhieraky} ses et eksempel på et taskhieraki
konstrueret med vores taskcontructor, under antagelse af at hver TestTask
ønsker at udføre et antal bevægelser og ikke på noget tidspunkt ønsker
frivilligt at opgive bevægekontrollen, vil TestTask med level 1 slutte
først og TestTask med level 2 slutte sidst.
\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/taskhieraky.png}
  \caption{Eksempel på et taskhieraki.}
  \label{fig:taskhieraky}
\end{framefig}

\subsubsection{HoverTrackTask}
\label{sssec:hovertracktask}

\subsubsection{FollowTourTask}
\label{sssec:followtourtask}
Et mere realistisk og komplekst eksempel er en FollowTourTask, se
\ref{fig:followtour}, denne task nedarver fra en sekvensiel
compoundtask og indeholder udover takeoff og land-tasks også en
parallel compoundtask der kan veksle mellem bevægelse og fastholdelse
af en position(ved hjælp af en PID-controller). Selve FollowTourTask
enheden holder styr på den tour der skal følges, ved hjælp af et
Map-objekt, og sørger for at opdatere HoverTrackTasken(PID) med en ny
målposition når det er nødvendigt. 
Hvis man anskuer vores Map som en relationel graph hvor alle knuder er
forbundne, minder vores tilgang meget om Kuipers og Byuns metode der
beskrives i 
\cite{murphy2000introduction}. Specielt lægges mærke til at der i
begge systemer udføres fejlkorrigering hver gang en ny position
detekteres, i vores system sker dette ved at HoverTrackTasken bringer
robotten indenfor en specifik afstand af den præcise position før
Ar.Dronen sendes videre. Det ville desuden være let at lade vores
*DistanceMeasurer* algoritme tilføje metrisk data til kortet, hver gang
AR.Dronen har bevæget sig mellem to positioner.

\begin{framefig}
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/followtourhieraki.png}
  \caption{Opbygningen af en FollowTourTask.}
  \label{fig:followtour}
\end{framefig}

\chapter{Klientværktøjer} % Morten,
I dette kapitel gennemgåes tre værktøjer som er udviklet i
forbindelse med specialearbejdet. Der er tale om et program til at
præsentere og visualisere datastrømmene fra AR.Drone(Sensor display),
et program til at bygge de map-objekter der anvendes af vores FollowTourTask(Mapcreator)
og et program til at instantiere, kombinere og afvikle
Tasks(Taskcreator). Alle tre værktøjer anvender et drone-objekt som
vist på figur~\ref{fig:initialiseringssekvens} og kan startes i
henholdsvis normal- og testtilstand(se
figur~\ref{fig:sensordisplayrun},\ref{fig:mapcreatorrun} og
\ref{fig:taskcreatorrun}). Hvis brugeren vælger at starte et 
værktøj i testtilstand, vil drone-objektet oprette forbindelse til et
testdevice (beskrevet i sektion~\ref{sec:sub:testdevice}) istedet for AR.Dronen.

\section{Sensor display}
\begin{framefig}
  \subfigure[Navdata display]{\includegraphics[width=0.49\textwidth]{grafik/guinav.png}
    \label{fig:navdatadisplay}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Video display]{\includegraphics[width=0.49\textwidth]{grafik/guivideo.png}
    \label{fig:videodisplay}}
  
  \subfigure[WIFI display]{\includegraphics[width=0.49\textwidth]{grafik/guiwifi.png}
    \label{fig:wifidisplay}}
 ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[WIFI samples display]{\includegraphics[width=0.49\textwidth]{grafik/guisamples.png}
    \label{fig:sampledisplay}}
 
  \label{fig:sensordisplay}
  \caption{derp}
\end{framefig}

Sensordisplay-værktøjet er implementeret i
sensordisplay.py,~\cite{github:sensordisplay}. Programmet anvender et
drone-objekt til at få adgang til AR.Dronens
sensorstrømme. Brugeren vælger hvilken sensorstrøm der ønskes
præsenteret, ved at trykke på en af de fire radio buttons(Navdata,
Video, WIFI eller WIFI samples), som aktiverer hver deres
præsentationstilstand. Hvis brugeren ønsker at indsamle datapakker til
anvendelse i testtilstand, kan dette gøres ved at trykke på knappen
'Capture Sensor Data'. De rå datapakker vil så blive gemt ved
programmets afslutning og kan herefter bruges af det implementerede
testdevice, se sektion~\ref{sec:sub:testdevice}.

Når navdatastrømmen præsenteres, ser brugeren et skærmbillede som
det på figur~\ref{fig:navdatadisplay}. På dette billede ses værdier
såsom højde, hastighed, samt vinklerne $\theta$, $\phi$ og 
$\psi$. Desuden vises oplysninger om AR.Dronens umiddelbare tilstand,
såsom motortilstand, ultralydstilstand og om hvorvidt førnævnte vinkler er
indenfor de tilladte grænser.

Når videostrømmen præsenteres, vises et billede som det på
figur~\ref{fig:videodisplay}. Afhængig af hvilken videostrøm der
modtages, eller om der løbende skiftes videostrøm, vil en af
billedrammerne muligvis være sort. Hvis der løbende skiftes videostrøm,
vises det nyeste billede fra hver videostrøm i de to billedrammer.

Hvis WIFI-sensorprogrammet er startet på AR.Dronen (beskrevet i
sektion~\ref{sec:sub:sub:wifisensor}) og WIFI-strømmen præsenteres, vil brugeren blive
præsenteret for et billede som det på figur~\ref{fig:wifidisplay}. Her
ses en søjle der repræsenterer den sidst kendte signalstyrke, for hver
erkendt WIFI-kilde. I denne præsentationstilstand er der desuden mulighed for
at indsamle signalstyrkesamples og sætte en mål-signalstyrkesample. 

Disse indsamlede signalstyrkesamples kan sammenlignes, både med hinanden og
mål-signalstyrkesamplen, dette gøres i den fjerde og sidste 
præsentationstilstand. I denne tilstand, vises en søjle for hver
WIFI-kilde og i denne søjle indikeres kildens signalstyrke for hver
indsamlet sample, se figur~\ref{fig:sampledisplay}.

Sensordisplay programmet startes fra en linuxkommandolinie, som vist
på figur~\ref{fig:sensordisplayrun}
\begin{framefig}
\begin{verbatim}
$ ./sensordisplay.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./sensordisplay.py -t
\end{verbatim}
\caption{Afvikling af sensordisplay-værktøjet fra en linuxkommandolinie.}
\label{fig:sensordisplayrun}
\end{framefig}

\section{Mapcreator}
\label{sec:mapcreator}
\begin{framefig}
  \subfigure[World editor]{\includegraphics[width=0.49\textwidth]{grafik/mapworld.png}
    \label{fig:mapworld}}
  ~ 
  \subfigure[Tour editor]{\includegraphics[width=0.49\textwidth]{grafik/maptour.png}
    \label{fig:maptour}}
    \label{fig:mapcreator}
  \caption{Mapcreator-værktøjets to redigeringstilstande, world og tour.}
\end{framefig}
Mapcreator er et værktøj til at oprette og redigere map-objekter.
Map-objekter bruges af FollowTourTasks til at navigere
efter. Mapcreator har to redigeringstilstande, world og tour, som kan
vælges ved at trykke på det tilsvarende faneblad øverst i billedet. De
to tilstande ses på figur~\ref{fig:mapcreator}.

I world-redigeringstilstanden kan brugeren tilføje, flytte eller fjerne
punkter fra kortet. Meningen med punkterne er, at de skal matche en
visuel markør i den fysiske opstilling, som vist på
figur~\ref{fig:simplesetup}. Internt i map-objektet vedligeholdes en
liste med vinklerne mellem alle punkter, når punkter tilføjes, flyttes
og slettes opdateres denne liste også automatisk. Udover punkternes
fysiske position i forhold til hinanden, kan man også tilføje
WIFI-signalstyrkemålinger (WIFI-fingerprint) til punkterne ved at trykke på
'Add/update WIFI sample'-knappen og herefter trykke med musen på det
punkt der ønskes markeret. Hvis WIFI-signalstyrkemålingen
ønskes slettet, trykkes først på 'clear WIFI sample' og derefter på
punktet. World-skærmbilledet kan ses på figur~\ref{fig:mapworld}.

I tour-redigeringstilstanden kan brugeren ikke længere redigere
punkterne, til gengæld kan disse forbindes til en tour. Dette gøres
ved at holde 'ctrl' nede og så trække en linie mellem to punkter med musen. Det
er kun muligt, at forbinde til det første eller sidste punkt i en
eksisterende tour. Det er muligt, at fjerne liniesegmenter fra en tour
ved at markere segmentet med musen og herfter trykke på knappen
'Delete Segment'. Ved at markere et liniesegment og trykke enten
'Add/remove A' eller 'Add/remove B' kan man markere, at dette
liniesegment tilhører en specifik type. På sigt er det meningen, at
FollowTourTask skal kunne vælge en bevæge-algoritme på baggrund af
denne information. Tour-skærmbilledet kan ses på figur~\ref{fig:maptour}.

Når programmet sluttes, gemmes map-objektet i filen
./testdata/map.data ved hjælp af Pythons pickle-modul. I map klassens
constructor søges efter denne fil og hvis den findes, indlæses
map-objektets state fra denne.

Som sensordisplay-værktøjet, startes mapcreator-værktøjet fra en linuxkommandolinie, som vist
på figur~\ref{fig:mapcreatorrun}.
\begin{framefig}
\begin{verbatim}
$ ./mapcreator.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./mapcreator.py -t
\end{verbatim}
\caption{Afvikling af mapcreator-værktøjet fra en linuxkommandolinie.}
\label{fig:mapcreatorrun}
\end{framefig}

\section{Taskcreator}
\label{sec:taskcreator}

\begin{framefig}
  \centering
  \includegraphics[width=1.0\textwidth]{grafik/taskcreator.png}
  \caption{Taskcreator-skærmbillede. Dropdown-listen med tilgængelige Tasks og  parameterinputfelterne
  til den valgte Task, er markeret med rødt.}
  \label{fig:taskcreator}
\end{framefig}

Taskcreator-værktøjet bruges til at instantiere, kombinere og afvikle
Tasks (beskrevet i sektion~\ref{sec:tasks}. 
Brugeren kan vælge mellem de tilgængelige Tasks ved at trykke
på dropdown-listen, som ses yderst til venstre i den røde firkant på
figur~\ref{fig:taskcreator}. Herefter kan den valgte Tasks
standardparametre tilpasses i inputfelterne, bl.a. kan man sætte hver
Tasks level. En Tasks level beskriver dens prioritet i forhold til
andre Tasks med samme parent. Den valgte Task
instantieres ved at trykke 'Add task' og kan fjernes igen, ved at markere den med musen
og trykke 'Delete task'. Tasks der nedarver fra klassen
CompoundTask(ParCompoundTask, SeqCompoundTask og FollowTourTask)
kan indeholde subtasks. Subtasks tilføjes ved at holde 'ctrl' nede og
trække en linie fra CompoundTasken til den ønskede
subtask med musen. Hvis brugeren ønsker afkoble en subtask fra en CompoundTask,
gøres det ved at gentage ovennævnte
procedure. Figur~\ref{fig:taskcreator} viser et opbygget Tasktræ   
bestående af tre ParCompundTasks og et antal TestTasks. 

Når brugeren har instantieret og kombineret de ønskede Tasks, kan
disse afvikles og dermed udføres af AR.Dronen. Dette gøres ved at markere den
ønskede Task og trykke på 'Execute task'. Afviklingen af en Task eller alle
Tasks kan stoppes ved at trykke på henholdsvis 'Stop task' eller 'Stop
all tasks'. 

Taskcreatoren startes som de andre værktøjer fra en
linuxkommandolinie, se figur~\ref{fig:taskcreatorrun}
\begin{framefig}
\begin{verbatim}
$ ./taskcreator.py
  
  eller hvis programmet ønskes anvendt i testtilstand

$ ./taskcreator.py -t
\end{verbatim}
\caption{Afvikling af taskcreator-værktøjet fra en linuxkommandolinie.}
\label{fig:taskcreatorrun}
\end{framefig}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Eksperimenter - section
\chapter{Eksperimenter} % Thomas & Morten
\section{Navigation}
En vigtig forudsætning for at en robot kan navigere i et miljø er, at
den er i stand til at lokalisere sig selv i dette miljø. Vi har derfor
ekperimenteret med forskellige former for lokalisering. Mere specifikt
har vi arbejdet med lokalisering ved hjælp af WIFI-signalstyrker og
lokalisering ved hjælp af visuelle markører. Vi har i vores
eksperimenter gjort brug af et kort med beskrivelser af
lokationer(bestående af indsamlede WIFI-signalstyrker, markørbeskrivelse og koordinater) og en
rutebeskrivelse bestående af en liste med lokationer. Kortet
indeholder desuden en fortegnelse over vinkelretninger mellem alle
lokationspar. Kort og rutebeskrivelser oprettes let ved hjælp af
Mapcreator-værktøjet beskrevet i \ref{sec:mapcreator}. For at teste forskellige de lokaliseringsmetoder har vi
anvendt HoverTrackTasks(se section \ref{sssec:hovertracktask}) og
FollowTourTasks(se section \ref{sssec:followtourtask}).

\subsection{Lokalisering via Wifi-fingerprinting} % Morten (1side)
\label{ssec:wifilokalisering}
Som nævnt i sektion~\ref{sec:udvidelse} har vi udvidet AR.Dronen med
en WIFI-sensor. Denne sender løbende information om de omkringliggende
WIFI-kilders signalstyrker. I de følgende eksperimenter anvender vi en
teknik kaldet WIFI-fingerprinting. WIFI-fingerprinting fungerer i
hovedtræk ved at der opbygges et WIFI-kort
under en initiel offline-fase, hvorefter livesignalstyrkemålinger kan
sammenlignes med dette kort. WIFI-kortet opbygges ved at foretage
signalstykemålinger på et antal faste punkter. Generelt kan det siges,
at jo mere offlinedata der indsamles, jo bedre fungerer 
lokaliseringen. WIFI-fingerprintingteknikken beskrives i
\cite{quan2010wi}. 

\subsubsection{WIFI-fingerprinting-algoritmen}
\label{sssec:wifialgo}
I de følgende eksperimenter anvender vi en fingerprintingalgoritme,
der udfører en simpel nearest-neighbor søgning i et
WIFI-kort, som opbygges umiddelbart før eksperimentets
udførelse. Et overblik over offline-proceduren kan ses på
figur~\ref{fig:offline}. 
Nearest-neighbor søgningen udføres ved at sammenligne hvert
punkt i WIFI-kortet med de nuværende signalstyrker. To sæt af
signalstyrker sammenlignes ved først at finde en fællesmængde af
WIFI-kilder i de to sæt og herefter udregne en afstand mellem de to
sæt baseret på denne fællesmængde. Lokaliseringsproceduren er
illustreret på figur~\ref{fig:online} Algoritmen er implementeret i
virtualsensors.py, \cite{github:virtual}. 

\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifioffline.png}
  \caption{Diagram der viser viser fingerprintingteknikkens
    offlinefase. WIFI-sensoren sender løbende signalstyrker til
    WIFI-receiveren, der stiller disse tilrådighed for det program der
    bruges til at opbygge WIFI-kortet(mapcreator.py). Figuren er
    tilpasset fra \cite{quan2010wi}.}
  \label{fig:offline}
\end{framefig}

\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifionline.png}
  \caption{Diagram der viser viser lokaliseringsfasen. Outputtet fra
    WIFI-receiveren sammenlignes med indholdet i WIFI-kortet og
    resultatet er den mest sandsynlige position. Figuren er
    tilpasset fra \cite{quan2010wi}.}
  \label{fig:online}
\end{framefig}

\subsubsection{Første Eksperiment}
Opstillingen til dette eksperiment består af 3 markører med 3 meters
mellemrum som vist på figur~\ref{fig:wifisetup}. Efter opstillingen
udførtes en indsamling af WIFI-signalstyrker som beskrevet i
sektion~\ref{ssec:wifilokalisering}. Til dette formål anvendtes   
mapcreator-værktøjet(sektion~\ref{sec:mapcreator}). Indsamlingen blev
udført i en højde af 1 meter, med AR.Dronen placeret på et fast
stativ.
\begin{framefig}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/wifisetup.png}
  \caption{Setup til første WIFI-eksperiment, knappen "Add/update WIFI
    sample" anvendes til at associere en WIFI-signalstyrkemåling med
    en lokation.}
  \label{fig:wifisetup}
\end{framefig}

Efter den indledende fase, blev AR.Dronen skiftevis placeret over de
tre markører og WIFI-fingerprinting-algoritmen afviklet 200
gange. Dette blev gentaget 3 gange for hver position. På
tabel~\ref{fig:wifidetection1} ses detektionsraten for hver udførelse,
detektionsraten defineres som antal gentagelser(200) divideret med
antal korrekte lokaliseringer.
\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Position & 1. gentagelse & 2. gentagelse & 3. gentagelse \\
    \hline
    1        & 0.55          & 0.26          & 0.685         \\
    \hline
    2        & 0.67          & 0.81          & 0.735         \\
    \hline 
    3        & 0.925         & 0.915         & 0.93          \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for WIFI-fingerprinting, 3 positioner
    med 3.0 meters afstand}
  \label{fig:wifidetection1}
  \end{framed}
\end{table} 
Som det ses er resultaterne meget svingende. Med vores fingerprinting
algoritme og afstande mellem lokationerne på 3 meter eller derunder,
kan vi ikke garantere en konsistent lokalisering.

\subsubsection{Andet Eksperiment}
Vores andet eksperiment med WIFI-lokalisering er direkte afledt af de
dårlige resultater i det første eksperiment. Vi besluttede  at øge
afstanden mellem punkterne til 6 meter, for herefter at gentage
eksperimentet som beskrevet ovenover.
Resultatet af dette eksperiment kan ses på
tabel~\ref{fig:wifidetection2}. De opnåede detektionsrater under denne
gennemførsel er markant bedre end under det første eksperiment 
\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Position & 1. gentagelse & 2. gentagelse & 3. gentagelse \\
    \hline
    1        & 1.0           & 1.0           & 0.995         \\
    \hline
    2        & 0.725         & 0.9           & 0.855         \\
    \hline 
    3        & 1.0           & 0.97          & 0.955         \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for WIFI-fingerprinting, 3 positioner
    med 6.0 meters afstand}
  \label{fig:wifidetection2}
  \end{framed}
\end{table} 
Det faktum, at vi kun kan forvente en konsistent lokalisering hvis
vores lokationer ligger med 6 meters afstand gør, at vi ikke fører
denne eksperimentrække videre. En videreudvikling af vores algoritme
kunne muligvis fungere som støtte til en primær visuel lokaliseringsmetode. 

\subsection{Visuel lokalisering}
\begin{framefig}
  \centering
  \subfigure[Simpel markør]{
    \includegraphics[width=0.20\textwidth]{grafik/simplemarker.png}
    \label{fig:simplemarker}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[semiavanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/semiadvancedmarker.png}
    \label{fig:semiadvancedmarker}}
  ~
  \subfigure[avanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/advancedmarker.png}
    \label{fig:advancedmarker}}

  \label{fig:markers}
  \caption{Markører vi har testet i forbindelse med visuel lokalisering af AR.Dronen}
\end{framefig}
\subsubsection{Lokalisering via simple markører} % Thomas & Morten
\paragraph{Første eksperiment}
\label{p:simple1}
Vores første forsøg med visuel lokalisering anvendte en simpel rød
markør i A5 størrelse, se figur~\ref{fig:simplemarker}. Vi havde til
formålet implementeret en blobdetectionalgoritme i Python, \cite{github:blob}, som for
hver pixel vurderer om den er rød nok til at være en del af markøren. Algoritmen antager at der i
billedet kun findes et rødt objekt og er på grund af Pythons
afviklingshastighed ikke videre hurtig, men til dette simple
eksperiment var den passende.
Eksperimentet udførtes ved at markøren blev placeret på
gulvet og AR.Dronen placeret i position over markøren, hvorefter
blobdetectionalgoritmen blev startet. Vi definerer detektionsraten som
antal succesfulde detektioner delt med antal gentagelser af
algoritmen. Vi kørte algoritmen 200 gange i henholdsvis 0.5, 1.0, 1.5
og 2.0 meters højde og resultaterne kan ses i tabel
~\ref{fig:detection1}, sammen med resultaterne for de andre markører.
 
Vores første forsøg viste os, at vi i højder fra 0.5 til 2.0 meter altid var i stand
til at genkende den simple markør med AR.Dronens bundkamera.

\paragraph{Andet eksperiment} 
Vores næste skridt var at anvende ovennævnte lokaliseringsmetode i
forbindelse med afviklingen af en HoverTrackTask(sektion ~\ref{sssec:hovertracktask})
for at få en idé om metodens anvendelighed i en kontekst hvor
effektivitet er afgørende.. 
Vi anvendte til dette forsøg også kun en enkelt markør og forsøget
afvikledes ved at AR.Dronen blev fløjet i position over denne,
hvorefter HoverTrackTasken blev aktiveret. Det viste sig tydeligt
at blobdetectoren ikke var effektiv nok. Vi var ikke i stand til
at opdatere hurtigt nok og som resultat nåede AR.Dronen ofte at bevæge
sig væk fra markøren. Dette resultat fik os til at reimplementere
HoverTrackTasken således at den anvendte vores blobdetector til at
opdage markøren og herefter anvendte en af OpenCVs
opticalflow-algoritmer (cv2.calcOpticalFlowPyrLK, \cite{opencv:optflow}) til at tracke den i videostrømmen. Afviklingen af
opticalflow-algoritmen er omtrent 10 gange hurtigere end vores
blobdetector. Da vi gentog forsøget med denne tilføjelse var AR.Dronen
meget mere responsiv og efter at have fintunet HoverTrackTaskens
PID-parametre var AR.Dronen i stand til at forblive over markøren. På
fig~\ref{fig:simplehover} ses HoverTrackTraskens PID error-værdier i
forhold til markøren på gulvet, over en periode på mere end 2
minutter. Errorværdierne er omregnet fra pixels til millimeter ved
hjælp af den højdeværdi vi løbende får fra navdatastrømmen. Det bemærkes at
der er et udsving omtrent midt i forløbet. Dette er ikke usædvanligt,
da AR.Dronen er meget påvirkelig af 
turbulens og dette let forekommer, især i mindre lokaler.
\begin{framefig}
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/simpledronehover.png}
  \caption{HoverTrackTaskens errorværdier over en længere
    periode. Errorværdierne angiver hvor mange millimeter AR.Dronen
    befinder sig fra markørens centrum. Errorværdierne er indsamlet
    ved at gemme disse for hvert gennemløb af HoverTrackTaskens update-metode.}
  \label{fig:simplehover}
\end{framefig}

\paragraph{Tredje eksperiment}
\label{p:simple3}
Sidste eksperiment med de simple markører anvendte en FollowTourTask
der fulgte en rute mellem fire markører på gulvet. Eksperimenterne startede
denne gang fra jorden, da en FollowTourTask selv letter og lander.
Vores setup, både det konstruerede kort og den fysiske opstilling ses
på figur ~\ref{fig:simplesetup}.
\begin{framefig}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/simplesetup.png}
  \caption{Repræsentation af det kort FollowTourTasken anvender til at
  Navigere og den fysiske opstilling kortet afspejler.}
  \label{fig:simplesetup}
\end{framefig}

Vi forventede her at udfordringen ville være at opdage markøren tidsnok til at stoppe
AR.Dronen før den var fløjet forbi. Dette problem kan dog løses ved at
lade AR.Dronen flyve i en passende højde, vi fandt at en
højde på 1800 mm var passende til dette formål. Til gengæld
afdækkede vi to andre problemer. 
Det første problem var relateret til at
alle vores markører var ens, dette betød at der var tilfælde hvor
AR.Dronen fandt samme lokation to gange i træk, men antog at den
fundne lokation var den næste i rutebeskrivelsen. Problemet lagde
grund til tanken om at anvende mere avancerede lokationsmarkører for
at opnå en mere præcis lokalisering. Det andet problem
var at hvis vi først mistede en markør fra AR.Dronens synsfelt, så
var det umuligt at fortsætte. Dette problem løstes ved at tilføje en
recovermode til HoverTrackTasken, således at denne kunne bevæge
AR.Dronen opad indtil enten markøren igen kunne genkendes eller en højdegrænse
blev nået, hvorefter experimentet alligevel måtte afsluttes. Denne
simple tilføjelse gjorde vores FollowTourTask meget mere robust.   

\paragraph{Resultater}
De overordnede resultater af vores eksperimenter med de simple
lokationsmarkører var, at vi fik bekræftet AR.Dronens evne til at
genkende simple markører og at vi fik udviklet både HoverTrackTasken
og FollowTourTasken. Vi fandt desuden ud af at vores markørsetup var
for simpelt til at være praktisk anvendeligt og at vi måtte udvikle
mere avancerede markører for bedre at understøtte vores FollowTourTask.
\cite{electronic:youtube2}(youtube link) viser en video hvor vores AR.Drone flyver frem
og tilbage mellem to simple markører og demonstrerer at lokaliseringen,
på trods af de to nævnte problemer, oftest fungerer. 

\subsubsection{Lokalisering via avancerede markører} % Thomas & Morten
Vi valgte efter de indledende eksperimenter at designe og anvende en
mere avanceret form for markør. Vores første redesign var markøren der
ses på figur~\ref{fig:semiadvancedmarker}. 

\paragraph{Detekteringsalgoritme}
Grundideen er at man først detekterer den røde firkant, hvorefter det
indkransede område afsøges for en farvet blob der relaterer markøren
til en specifik lokation. For at sikre en hurtig detektering(og dermed
lokalisering) gør vores algoritme omfattende brug af OpenCVs
optimerede billedbehandlingsalgoritmer, \cite{opencv:main}. Vores detekterings algoritme
er implementeret i vores virtuelle sensor og afvikles således konstant
for at give et så nøjagtigt billede af det øjeblikkelige miljø som muligt. 
Første skridt i algoritmen er at udføre thresholding ved hjælp af
OpensCV inRange algoritme. Dette giver os et billede hvor alle de
oprindelige røde pixels nu er hvide og resten er sorte.
Andet skridt er at finde konturer i det binære billede. Dette gøres med
OpenCVs findContours algoritme.
Tredje skridt indbefatter at udvælge de konturer der har et stort nok
areal til at være interessante. Dette gøres med OpenCVs contourArea
algoritme.
Fjerde skridt er at undersøge områderne der dækkes af de udvalgte
konturer for at finde en farvet blob. Denne undersøgelse anvender igen
OpenCVs inRange algoritme. Hvis en sådan farvet blob findes,
kan vi sammenligne farven med beskrivelsen af de forskellige
lokationer vi finder i vores kort og dermed lokalisere AR.Dronen
præcist.

\paragraph{Første eksperiment}
\label{p:advanced1}
Vores indledende eksperiment udførtes på samme måde som
\ref{p:simple1}, blot var den simple markør skiftet 
ud med en semiavanceret markør, vist på figur
~\ref{fig:semiadvancedmarker}. Vores forventninger var fra start, at vi
ville få svært ved at identificere markøren over en hvis højde, da den
røde markering ikke er lige så koncentreret som på den simple
markør. Efter at have tilpasset vores thresholdværdier og udført
eksperimentet i højder fra 1 til 2.5 meter, kunne vi bekræfte vores
formodning. Vi kunne med med en hvis sikkerhed genkende den semiavancerede markør
op til 1.5 meters højde, men herefter faldt detektionsraten
betydeligt, resultaterne kan ses i tabel \ref{fig:detection1}. En
detektionshøjde på 1.5 ligger lidt under de 1.8 meter 
vi tidligere havde bestemt som en passende flyvehøjde. Vi valgte
derfor at udvikle en markør med en større rød overflade, se
figur~\ref{fig:advancedmarker}, for at sikre bedre detektion. En
gentagelse af eksperimentet med den avancerede markør bekræftede at
denne nu kunne detekteres i op til 2 meters højde. Den avancerede
markør er desuden designet sådan at den på sigt kan bruges til at
udlede AR.Dronens præcise retning, hvilket ville gøre os mindre
afhængige af de upræcise $\psi$-målinger fra AR.Dronens piezo-sensor(
se sektion~\ref{ssec:dronerecap}). 

\paragraph{Andet eksperiment}
Vores andet eksperiment er næsten identisk med det der blev udført for
de simple markører. Det skal dog nævnes at vores HoverTrackTask i
mellemtiden blev redigeret til at anvende den virtuelle sensors
detekteringsresultater, istedet for selv at udføre en
detekteringsalgoritme som det var tilfældet under de tidligere
eksperimenter. Under dette eksperiment var den virtuelle sensor
indstillet til ikke at skifte mellem de to kamerafeeds. 
Som forventet var AR.Dronen også i stand til at holde positionen over
den avancerede markør.

\paragraph{Tredje eksperiment}
\label{p:advanced3}
Dette eksperiment varierede fra det ovenstående ved at den virtuelle
sensor her løbende skiftede kamerafeed. Vi ønskede at undersøge om
HoverTrackTasken stadig var i stand til at stabilisere AR.Dronen over
en erkendt lokation når bundkameraets framerate blev begrænset. Forsøget viste
AR.Dronen stadig forblev stabil over lokationen. Til gengæld stoppede
den pludseligt da AR.Dronens originale styringssoftware lukkede ned efter omtrent ti
minutter. Nærmere undersøgelser viser at AR.Dronens software muligvis
har en memoryleak. Dette viser sig ved at softwarens hukommelsesforbrug
stiger kraftigt så snart vi iværksætter hurtige kameraskift ved brug
af AR-kommandoen set\_config. Denne opførsel beskrives også i sektion~\ref{sec:sub:virtuel}. 

\paragraph{Fjerde eksperiment}
Ligesom \ref{p:simple3} anvendte vi til dette formål en opsætning med
fire markører, formålet var at bekræfte at vi stadig med de nye
markører og den ændrede HoverTrackTask var i stand til navigere mellem
de fire punkter med vores FollowTourTask. Opstillingen kan ses på
figur ~\ref{fig:advancedsetup}.

Eksperimentet bekræftede, at AR.Dronen konsistent kunne lokalisere sig
selv. 
\begin{framefig}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/advancedsetup.png}
  \caption{Billede af opstilling med avancerede markører til brug for
    det fjerde eksperiment med disse markører}
  \label{fig:advancedsetup}
\end{framefig}

\subsubsection{Detektering af markører på små billeder}
\label{ssec:smallmarks}
I de ovenstående eksperimenter har vi anvendt AR.Dronens bundkamera.
Dette har en opløsning på 176x144. Der er dog andre tasks der skal
bruge data fra frontkameraet. Af den grund kan vi være nødt til kontinuerligt at zappe mellem
de to kameraer og dette fungererer som nævnt i sektion~\ref{p:advanced3} ikke særligt
godt. Vi har derfor undersøgt mulighederne for at anvende den
videostrøm hvor bundkameraets billeder er lagt oven på 
frontkameraets(se \ref{fig:2frames}). Når vi udtrækker bundkameraets billeder fra
denne strøm, får vi billeder i en opløsning på 88x72.
Eksperimenterne er fuldstændig magen til dem vi udførte i sektion~\ref{p:simple1} og
 sektion~\ref{p:advanced1}. De er ligeledes udført i højderne 0.5, 1.0, 1.5 og 2.0 og er
udført både med de simple, semiavancerede og avancerede
markører. Resultaterne ses i tabel~\ref{fig:detection2}, de viser tydeligt at
det ikke er muligt at anvende de små billeder, da de selv for de
simple markører kun giver et pålideligt resultat op til 1.5 meter,
hvorefter detektionsraten falder betydeligt. 

\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{3,0cm}| p{3,0cm}}
    Højde & Simpel & Semiavanceret & avanceret \\
    \hline
    0.5   &  1.0   & 1.0           & 1.0       \\
    \hline
    1.0   & 1.0    & 1.0           & 1.0       \\
    \hline 
    1.5   & 1.0    & 0.805         & 1.0       \\
    \hline
    2.0   & 1.0    & 0.045         & 1.0       \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for de forskellige markører i højder op til
    2.0 meter}
  \label{fig:detection1}
  \end{framed}
\end{table} 

\begin{table}[H]
  \begin{framed}
  \begin{center}
  \begin{tabular}{ l | p{2,5cm} | p{4,0cm}| p{3,0cm} }
    Højde & Simpel(88x72) & Semiavanceret(88x72)  & Avanceret(88x72)\\
    \hline
    0.5   & 1.0           & 0.615                 & 1.0 \\
    \hline
    1.0   & 1.0           & 0.065                 & 0.935 \\
    \hline 
    1.5   & 1.0           & 0.0                   & 0.015 \\
    \hline
    2.0   & 0.17          & 0.0                   & 0.015 \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Detektionsrater for de forskellige markører i højder op til
    2.0 meter når der anvendes 88x72 billeder}
  \label{fig:detection2}
  \end{framed}
\end{table} 


\subsubsection{Lokalisering via QR-koder} % Morten
\label{sec:QRdecoding}
Vi har undersøgt muligheden for at anvende QR-koder til
lokalisering. Man er begyndt at se QR-koder mange steder i det
offentlige rum, hvor de ofte bruges i forbindelse med
reklamekampagner. QR-koder bruges ofte til at indkode en webadresse,
men kan i princippet anvendes til at indkode en arbitrær
streng. QR-koder er blevet mere populære i takt med at opløsningen på
mobiltelefonkameraer er steget, således har de fleste moderne
mobiltelefonerkameraer en opløsning flere gange højere end AR.Dronens
kamera. 

Til dette eksperiment anvendtes bibliotekerne pyqrcode, 
\cite{electronic:pyqrcode}, og Zbar, 
\cite{electronic:zbar}, til henholdsvis
indkodning og afkodning af QR-koder. Zbar har pythonbindinger og anvendes også på
iPhone-platformen. Dette betød at vi kunne teste den genererede
QR-kode ved at scanne den med en iPhone.

\begin{framefig}
  \centering
  \includegraphics[width=0.35\textwidth]{grafik/woah.png}
  \caption{Teksten ``woah'' som QR-kode}
  \label{fig:qr_woah}
\end{framefig}

Formålet med eksperimentet var primært at undersøge om AR.Dronens
kameras opløsning er god nok til at vi kan afkode QR-koder. Til
formålet fremstillede vi QR-koden der ses på figur \ref{fig:qr_woah},
printede den ud på et A4 ark og sikrede os at den kunne afkodes ved hjælp af en iPhone. Herefter
skrev vi et lille testprogram der henter billeder fra AR.Dronens
frontkamera (320x240 pixels) og scanner disse for QR-koder ved hjælp af Zbar.

Afviklingen af eksperimentet foregik ved at vi placerede QR-koden på
en væg og herefter placerede AR.Dronen i skiftende afstande til
væggen. Vi forsøgte at afkode QR-koden i afstande op til en meter, men
fik på intet tidpunkt en succesfuld afkodning. IPhonen kunne samtidig
afkode QR-koden på alle afstande op til en meter. Resultatet viser med
al tydelighed at vi ikke kan anvende QR-koder med AR.Dronens kamera og
at vi derfor må anvende andre markører til lokalisering i vores system.

%\subsection{Optisk flow navigation} % tt
%\todo{fedt hvis det kan nåes at lave det...}
% birds eye view (lidt refs under speciale-mappen), head bobbing, biers afstandsbedømmelse ligger i
% optiske flow observeret fra A til B, forskel i tunnel og åbent land...

%problematik at flyve fremad; for smalle trekanter, måske bedre at flyve sidelæns eller op og ned
% evt afhjælpe med fiskeøje, så flow af featurepoints kan trackes længere langs siden af kopteren når man flyver forbi...



\section{Korridor-detektering} % Thomas
%intro
Denne sektion omhandler en metode til at genkende gangarealer og
korridorer gennem frontkameraet på AR.Dronen.

\paragraph{Målet} 
for dette eksperiment er at implementere og teste 2
ting: 1) At finde sandsynligheden for at være i en korridor, og
2) At finde ud af hvor enden af korridoren (forsvindingspunktet) er.

%mere intr, !det kan bruges til!
\paragraph{Kontekstuel viden} er hensigtsmæssig at have for at kunne
træffe funderede beslutninger om navigationen i specifikke
omgivelser. Derfor vil det være interessant at se om det er muligt at
implementere en korridor-detektions funktionalitet som senere vil kan
blive en et virtuelle sensor hieraki.
Robotten Minerva, \cite{Thrun99minerva:a}, fik lov at lave guidede
tours på the Smithsonian i 2 
uger. I eksperimentet var robotten blevet givet forskellige
opførsler at udfører under forkellige forhold. Eksempelvis skulle den
så ofte som muligt bruge analogien Coastal-planner, dvs. at følge
væggene rundt så den havde mulighed for at genkende allerede gemte
kendemærker. Når coastal-planner ikke var muligt skulle den gå over i
open-sea mode, hvor den så havde et andet bevæggrundlag.
Et andet eksempel på forskellige operationstilstande er i,
\cite{single_img_pespective_cues}, hvor en quadrokopter med
frontkamera med godt resultat kan kategoriseredens omgivelsertil at
være i en trappeopgang, et gangareal eller et lille kontor. I
mostætning til Minerva bruger denne tilgang ikke 2D kort eller
omgivelser til pathplanning. Videnen om omgivelserne bruge til at
indsnævre de optiske strukture (perspective cues) der skal søges og
orienteres efter.

\paragraph{Vanishing-point, et kendetegn for korridor}
Algortimen virker ved at lede efter et billedes
fælles-forsvindingspunkt.

Hvis et billede viser en korridor vil man i strukturen i billedet
kunne se at mange linier gå vandret og lodret. En del linier vil gå
diagonalt i billedet. Det har gennem eksperimentation vist sig, 
at der hvor de diagonale liner skærer hinanden er der ofte et
sammenfald med enden af korridoren i billedet.

%metode
\paragraph{Filtrering-processen}
Første skridt når man står med et rå billede og vil undersøge om der er
en korridor er først at filtrere det for at finde de lige
linier. Hernæst at frasortere de vandrette og lodrette, men fordi
AR.Dronen ikke altid holder lige i luften skal frasorteringen
augmenteres med AR.Dronens $\phi$-vinkel\footnote{Husk $\phi$ er den
  værdi der variere med AR.Dronens hældning sideværts.}. På den måde frasorteres de
vandrette og lodrette linier fra billedet af gangen og ikke af
billedrammen. Når man står tilbage med de diagonale linier
sammenlignes de hver især en-og-en for at finde
skæringspunkter. Punkterne for skæring tælles sammen og antallet
gemmes for hvert gitterpunkt i et 2 dimensionelt array af form $n
\times k$. %$n$ og $k$ er i eksperimentet sat til 11
Den celle i det 2 dimensionelle array med størst værdi stemmer ofte
overens med cellekoordinatet for billedets forsvindingspunkt.
Processen gennemgåes nærmere i det følgende.

%hvad er der i de følgende uddybbende sektioner herunder?
%(+metode) sub?
%+-at finde lige linier, , subsub?
%+-Augmentering med $\phi$
%+-Celle med flest linie-skæringspunkter
%+-troværdighed / trustworthyness

%+At finde kanter i et billede
%+Houghlinier

%+resultater
%+erfaringsudbytte


\subsection{At finde lige linier}
Det at finde linier foregår i to skridt via 2 forskellige
algoritmer. Til eksperimentet er open source computer vision bibliotekets, opencv,
\cite{opencv:main} implentation af et Canny-filter, \cite{Canny1986}, brugt til at finde
kanter med. Udfra kanterne er der brugt en OpenCV implementation af
Hough-transforms, \cite{duda1972houghline}, til at finde linier med polar-koordinater.

Figur~\ref{fig:houghprocess} viser de forskellige trin i
filtreringsprocessen fra rå billede til en samling af diagonale linier
på polær form. Kantdetekteringen i figuren \ref{fig:houghprocess}b er
lavet med en Sobelfunktion, men i Python-implementationen er der brugt
et Canny-filter.
\begin{framefig}
  \centering
  \subfigure[Et billede af en gang påført gauss-sløring]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_a.png}
    \label{fig:vanishopa}}
  %~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Billede a efter kantdetektering (her et sobel-filter)]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_b.png}
    \label{fig:vanishopb}}
  %~ 
  \subfigure[Hough-linier, udregnes udfra kantpunkterne i billede b]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_c.png}
    \label{fig:vanishopc}}
  %~ 
  \subfigure[Vandrette og lodrette hough-linier sorteres fra billede c]{
    \includegraphics[width=0.45\textwidth]{grafik/vanish_operation_d.png}
    \label{fig:vanishopd}}
  %~ 
  \label{fig:houghprocess}
  \caption{Detektering af Hough-linier.}
\end{framefig}

\subsection{Augmentering med $\phi$}
Hver af de fundne linier har en hældning som herefter sammenlignes med
grænseværdier opsat omkring variabelværdien $\phi$. Frasorteringen
gennem grænseværdierne gør at problemet der søges løst kan forenkles
til kun at omhandle det at finde området med flest
linie-skæringspunkter blandt de diagonale linier i
billedet. På figur~\ref{fig:phiaugmentet} kan man se $\phi$-vinklen
augmenteret på billedet. Den tykke blå-farvede linie indikere det
vandrette plan i billedet og er en funktione af $\phi$.

\begin{framefig}
  \centering
  \subfigure[Gangareal Ada-bygningen, Katrinebjerg]{
    \includegraphics[width=0.32\textwidth]{grafik/out2.png}
    \label{fig:out2}}
  %~
  \subfigure[Billedet er opdelt i $11 \times 11$ celler. I hver celle
    gemmes antallet af linier som skærer hinanden. I dette tilfælde er
    cellen med flest skærringer godt overensstemmende med korridorens
    forsvindingspunkt i billedet. Biledet er augmenteret med
    AR.Dronens hældning $\phi$]{
    \includegraphics[width=0.6\textwidth]{grafik/out_put_2jpg.png}
    \label{fig:phiaugmentet}
  }
  \label{fig:linieceller}
  \caption{Visualisering af vanishinpoint detektor.}
\end{framefig}

\subsection{Celle med flest linie-skæringspunkter}
Ved at finde det område af billedet med flest linier der skære
hinanden kan man under de rigtige forhold få et godt gæt på hvor
billedets forsvindingspunkt kan findes. I figur~\ref{fig:phiaugmentet}
ses ved celleindgang (5,5) der er i det område er 1307
linie-skæringer. En god indikator på et vanishingpoint.

\subsection{Troværdighed af et fundet vanishpunkt}
Observationer under eksperimenterne har vist at cellen med flest
skærringspunkter skal indeholde et hvis antal for at ikke at få sande
positiver, samt at der bør være skæringspunkter jævnt fordelt over det
meste af billedet. 

\subsection{Forhold og anvendelses område}
Figur~\ref{fig:phiaugmentet} er et eksempel på et billede med optimale
forhold for at finde forsvindingspunkter. 

En videreudvikling kunne være at udbygge detektoren til at huske mere
end fra en billedramme til den næste, evt. at parre korridorens
endepunkt med en $\psi$-retning fra AR.Dronen, som hele tiden
opdateres også når AR.Dronen drejer rundt og står på tværs af
korridoren, således at endepunktet senere kan genfindes.
 
Implementationen er en simplificeret løsning der virker i de lette
tilfælde. Det er en anden og knap så resoucekrævende tilgang end den
af, \cite{single_img_pespective_cues}, hvor der blandt andet bruges en
Viterby-algoritme til at huske hvor korridorenden sidst blev set og om
det så er rimeligt at tro at den er der hvor detektoren siger den er
nu.

\subsection{At finde kanter i et billede}
Canny blev valgt over Sobel, fordi Canny blev kendt først, men også
fordi det ser ud til at kantmarkeringerne af et Sobelfilter er bredere end
med Canny. Hvilet senere vil have indvirkning på Houghtransformens
udførsel.

Herunder gives en definition på hvad kanter er, samt deres egenskaber.
Forklaring er simplificeret og tager udgangspunkt i et eksempel med
kantdetektering i kun 1 dimension. 
\begin{framefig}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/kant_detektering1a.png}
  \caption{Graf over rød-, grøn-, blå-farveværdier for en linie i
    billedet med pixel indeks $j=67,\ i=\{0,1,\ldots,320-1 \}$. Samt
    pixel intensitet, her givet ved summen af de tre farver for $i$ og
    $j$. Der hvor grafen, eg. for intensiteten, har stejlest hældning
    er der stor sandsynlighed for en kant i billedet.}
  \label{fig:kanter1}
\end{framefig}

Kanter i et billede defineres som abrupt skift i farveværdi eller
lysintensitet lokalt\footnote{Man kan også lede efter kanter globalt,
  se f.eks., \cite{electronic:wiki_stepdetectglobal}, men det er en
  anden tilgang. I dette afsnit beskrives en metode der søger 
lokalt.} mellem naboer af pixels. Kanter er ofte en indikator for
eksistensen af bl.a. ændringer i dybden af et billede og for
overfladeændringer m.fl., se figur~\ref{fig:kanter1} 
%illustrerer hvordan ændringerne i intensitet
%imellem tilstødende billedpunkter (pixel) kan ses som en graf af
%højder. De steder hvor der er stor højdemæssig afstand, altså hvor
%hældningen er stejl, er der som oftest en kant at finde.
For at forbedre resultatet af kant-filteret, kan indput-billedet
forinden flyttes ned i dimension ved at bruge et båndpasfilter til at
lave en ``udglatning'' af billedet. Udglatningsoperationen udføres med
et gaussisk udglatningsfilter, \cite{electronic:wiki_gaussklokke},
(fra billedbehandlingsprogrammer, eg. GIMP\cite{gimp}, er funktionen
kendt som Gaussian-blur).

Til at finde kanter (edges) bruges cannyfilteret,
\cite{Canny1986}. Det tager et billede og returnere et binært billede
(sort/hvid) som det i figur~\ref{fig:vanishopb}. Hvide billedpunkter
indikere at der er en kant i det oprindellige billede, mens sort
betyder at der ikke er ændringer. Tager man alle pixelpositioner hvor
den første afledte er over threshold kan man ende op med mange tykke
kanter, for at præcisere placeringen kan man udtynde dem, ved at søge
i retningen af den lokale gradient og istedet finde
optimum. \todo{skal det med?: Videreudbygninger af Cannyfilter-implementationen efter 1986
bruger bl.a. endnu en retningsbestemt afledt af gradienten og finder
maximum ved den anden afledtes skærring med 0. Lindenberg88 2. 3. afledt...}

\subsection{Liniedetektering via Houghtransform}
Generaliseret houghtransformation, \cite{duda1972houghline}, er en
metode til at finde perfekte linier hvor der ikke andet end
billedpunkter. Dvs. metoden finder steder i et billeder hvor der ligger mange
billedpunkter på en lige række, og kalder rækken for en linie.

For hvert billedpunkt $(x_0, y_0)$ i billedet kan man skrive de linier
der går gennem punktet som
\begin{equation}
  r_{\theta} = x_0 * \cos{ \theta } + y_0 * \sin{ \theta }
\end{equation}
Hvor hvert par ($\theta_0$, $\rho_0$) vil repræsentere en linie igennem
det punkt (se figur~\ref{fig:houghting}).

\begin{framefig}
  %\centering
  \subfigure[En linie beskrevet ved det velkendte almindelige
    cartesisk koordinatsystem som: $y=ax+b$, samme linie beskrevet i
    polar koordinatsystem ved en vinkel $\theta$ og en afstand
    $\rho$. Forholdet imellem de 2 repræsentationeer er $y = 
\left( 
  - \frac{ \cos \theta }{ \sin \theta} 
\right)x 
+
 \left(
   \frac{ \rho }{ \sin \theta } 
\right) $]{
    \includegraphics[width=0.32\textwidth]{grafik/polar2cartesian.png}
    \label{fig:polar2cartesian}
}
  %~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Houghtransform, linie med flest stemmer, de 3 blå kryds
    er billedpunkter]{
    \includegraphics[width=0.66\textwidth]{grafik/houghting.png}
    \label{fig:houghting}}
  \label{fig:houghprocess2}
  %\caption{Markører vi har testet i forbindelse med visuel lokalisering af AR.Dronen}
\end{framefig}
De linie-par som er løsning til flest linier-igennem-punket ligninger
vil rangere højest. Man kan opsætte et kriterie for havd der gør en
god linie, ie. linien skal gennemløbe mindst $X$ billedpunkter, for
komme i betragtning til at være detekteret som en linie.
Resultatet af at køre det cannyfiltrerede billed igennem en
Houghtransform er en liste med linier, repræsenteret ved ($\theta_i$,
$\rho_i$) par.

OpenCV har to implementationer af Houghtransforms: en standard som
ovenfor beskrevet, og en probabilistisk Hough line transform. Den
probalististiske returnere en liste af linier, repræsenteret ved
liniernes endepunkter $(x_1, y_1), (x_2, x_2)$ istedet for $(\theta_i$,
$\rho_i)$ par.

\subsection{Resultater}
De steder hvor billedets forsvindingspunkt ligger i midten af
billedrammen af et billede taget ned ad Ada 100-gangen fra midten,
finder detektoren området for forsvindingspunktet hver gang. Der er
nogle tilfælde hvor det ikke altid virker: hvis billedet er taget i
den ene side af gangen, hvor Canny-filteret har detekteret mange
næsten lodrette linier, men ikke lodret nok til at blive frasorteret
via $\phi$-grænsen. Der vil der blive et stort antal linieskæringer i
siden som vil føre til fejl-placering af forsvindingspunktet.

Testmålinger forsøg viser at den nuværende implementation har en
virkningsgrad på ...
lavet en opstilling hvor resultatet gav at den fandt xxx ud af yyy tilfældende.


%konklusion af korridor detektor
\subsection{Erfaringsudbytte}


\paragraph{virtuelle sensore} kan, som nævnt i beskrivelsen i
sektion~\ref{sec:sub:virtuel}, bruges til at udlede højere niveau
kontekstuel information fra rå sensordata. Et hieraki af virtuelle- og
rå sensore er godt til at præsentere data for mennesker med
forskellige kontekstuelle fortolkninger ``\textit{Jo mere abstrakt
information AR.Dronen har adgang til des mere hensigtsmæssigt vil den
kunne den kunne træffe sine valg om navigering i sine omgivelser}'',
vil man sige. 

men i ``Minerva'' og ``opticalflow corridor vanishing point'' har robotterne fået
forskellige handleprocedure afhængigt af hvilket miljø de befinder sig i.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % % %

\section{Distance-measure} % Thomas
\label{sec:distance-mesu}
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

jo mindre quadrokopter des mindre inerti kan være indeholdt, og jo
mere agile agil/adræt vil den være, måske en pointe i futureworks...

Indledende øvelse til at kunne måle afstande frontalt via triangulering <- optisk flow

Algoritme integration over afstand

Test, hvor virker det godt hvor er det problematisk
reference til Parrots egen artikle herom; hastighed udledning bundkamera hvis muligt ellers udledning af accelerometer.

\section{Erkendelse af miljø/mode...}
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

Korridor (Hopper) vs. Åbent område (Zuse)
Er der noget at hente fra koridor vanishing point? der vil være noget hvis vi kan nå at måle afstande foran i optisk flow...


\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Konklusion - section

\chapter{Konklusion}
Evaluering: at det har været svært at finde videnskabelige datalogiske
kilder af høj troværdighed omkring AR.Dronen ; in part privat
virksomhed properitær firmware / arkitektur, meget information fra
hacker, tinker, DIY fora.  Til gengæld nogle videnskabelige artikler
omkring hvordan man implementere en algoritme ovenpå en allerede
antaget platform som AR.Dronen...

at det har været en learning experinece - vi valgte bl.a. at lege med
robotter med en forventning og håb om om at den fysiske verden ville
genere os i vores modelrepræsentationer - den svigtede os ikke :) vi
har som vi ønskede fået mere erfaring i at arbejde med og tage
forbehold for computaiton in a physical world.

\section{Perspektivering}
til prisen er ar.droen ok, men virker måske ikke så godt til indendørs / præcision / turbulens ift lokationskrav, men så et andet brugsscenarie, noget turis-happening som flyv en tur på stævnsklint for 100 kroner, se andres optagede videoer *billede*,
reference til fårehyrde-robot?

eksempel argumentation på hvorfor AR.Dronen ikke virker som videnskabelig platform, for stor og klumpet, ny forskning i quadrokopter viser det også, eg. swarms med mikrokopter \cite{turpin:swarm}.

\section{Fremtidig arbejde}

billeddekodning i C, Psyco kun virke til 32 bit

\pagebreak


\appendix

\bibliographystyle{abbrv}
\bibliography{referencer}

\chapter{AT kommandoer}
\label{sec:at}
Kort fortalt så må en AT-kommando ikke deles i flere pakker, men en UDP-pakke kan
indeholde flere AT-kommandoer, kommandoerne skal blot være adskilt med
en "linefeed" karakter og den samlede længde må ikke overskride 1024
karakterer. Såfremt en kommando overskrider længdebegrænsningen eller
på anden måde ikke overholder syntaksen, vil AR.Dronen ignorere
kommandoen.   

Tabel \ref{tab:kommandoer} viser hvilke kommandoer der
jf., \cite{techreport:ardroneDevGuide} kan bruges til at
kontrollere AR.Dronen.

\begin{table}
  \begin{center}
  \begin{tabular}{ l | p{3,5cm} | p{5,5cm} }
    Navn & Argumenter & beskrivelse\\
    \hline
    REF & input & Kommando til takeoff, land og nødstop\\
    \hline
    PCMD & flag, roll, pitch, gas, yaw & Flytter AR.Dronen\\
    \hline 
    FTRIM &  & Sætter den horisontale referenceværdi \\
    \hline
    CONFIG & key, value & Ændrer en konfigurationsparameter\\
    \hline
    LED & animation, frequency, duration & Viser en LED animation\\
    \hline
    ANIM & animation, duration & Afspiller en flyvesekvens\\
    \hline
    COMWDG & & Resetter komunikationstimeren
  \end{tabular}
  \end{center}
  \caption{Gyldige AT-kommandoer} 
  \label{tab:kommandoer}
\end{table} 

\chapter{Medfølgende CD / internet}
\todo{Readme} Hvad skal læseren gøre, hvad er tilgængeligt


\end{document}

%\begin{center}
%  \includegraphics[scale=0.35]{pic/selfserv.jpg}
%\newline
%\textbf{Figur 1}
%\end{center}
%\lstinputlisting[language=Python]{../drone.py}





% \bm{J} = \bigtriangledown I (\bm{x} ) = \left(\frac{ \partial I }{ \partial x }, \frac{\partial I }{ \partial y }\right)(\bm{x})
%  G &= \sqrt{ G_x^2 + G_y^2 }\\ \theta &= \arctan \left( \frac{G_y}{G_x} \right)\\ \text{ hvor } G_x & = \frac{ \partial I }{ \partial x } \text{ og } G_y \frac{\partial I }{ \partial y }

% \bm{J}_{\sigma} = \bigtriangledown [ G_{\sigma} (\bm{x} ) \times I (\bm{x} ) ] = [ \bigtriangledown G_{\sigma} ] (\bm{x} ) \times I (\bm{x} )

% \bm{S}_{\sigma}(\bm{x} )=\bigtriangledown \dot{} \bm{J}_{\sigma}(\bm{x} )=[ \bigtriangledown ^2 G_{\sigma} (\bm{x} ) \times * \bm{I} (\bm{x} ) ]
% \bigtriangledown G_{\sigma}(\bm{x} )=(\frac{\partial G_{\sigma}}{\partial x},\frac{\partial G_{\sigma}}{\partial y})(\bm{x})=[-x -y]\frac{1}{\sigma^3} \exp \left( - \frac{x^2 + y^2}{2 \sigma ^2} \right)

%#################3

%% Hvis man observere på ibrugtagning af robotter gennem tiden, ses robotteknologien anvendt
%% mange steder i industrien ofte med hensigten om 
%% at aflaste mennesker for monotone og farlige arbejdsprocesser (Unimate1950\cite{electronic:wiki_unimate}), eller med
%% hensigten om at effektivisere en produktionsvirksomhed ved indførelsen
%% af faste maskiner til at afvikle forprogrammerede
%% rutiner. Introduktionen af ny teknologi har altid en konsekvens;
%% ændringer betyder, at noget udskiftes med noget andet.
%% Den store Erhvervsredegørelse fra 1997 konkluderede også at
%% \begin{quotation}
%%   ``globalisering og ny teknologi overflødiggør den kortuddannede arbejdskraft.''
%% \end{quotation}%http://ibog.danmarkshistorien.systime.dk/index.php?id=224
%% %google søg: ``industrialisering efterspørgsel på arbejdskraft''
%% Hvis ny teknologi skal indføres bør det ikke kun afvejes iforhold til
%% økonomiske nytteværdi og omkostninger, men også med omtanke for det
%% enkelte menneske og under hensyntagen til langtsigtende konsekvenser for samfundet.

%% Fra de faste stationere robotter i produktionen kan man gå til de
%% mobile, hvor Elmer og Elsie er eksempler. Allerede fra 1948 lavede
%% Dr. W. Grey Walter 2 selvkørende
%% robotter\cite{Walter:1950:elmer:elsie} med analog styring ombord.
%% Senere ifølge Wikipedia\cite{electronic:wikirobot:comparison} er
%% der i

%1997 lavet en rengøringsrobot Robosanitan\cite{electronic:robosanitan1997}.


%%%%%%
%% \section{Udvidelse af AR.Dronen}
%% En robotplatform der kan udvides med nye sensorer efter behov, er
%% klart mere anvendelig end en helt lukket platform. Vi har derfor
%% undersøgt mulighederne for at anvende AR.Dronens OTG-USB* port til
%% andet end at udføre softwareopdateringer. Dette har blandt indbefattet at
%% kompilere kernemoduler(*) til AR.Dronens Linuxkerne(*ref til blog*),%som beskrevet i indledende sektion~\ref{sec:blog}
%% samt at omgå AR.Dronens software for ikke at miste strømmen til
%% porten. Vores proof-of-concept var at mounte en almindelig USB memorystick
%% og at skrive og læse fra denne(*ref til blog*), dette var forholdsvis
%% simpelt, da Linuxdriverne til dette formål er meget generiske.

%% \begin{figure*}[htp]
%%   \centering
%%   \subfigure[Hjemmelavet USB-kabel, en USB memorystick og en WIFI USB-dongle til tilslutning på AR.Dronen.]{
%%     \includegraphics[width=0.47\textwidth]{grafik/usbdevices.png}
%%     \label{fig:cableandusbdevices}}
%%   ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
%%   \subfigure[AR.Dronen med USB-memorystick tilsluttet.]{\includegraphics[width=0.47\textwidth]{grafik/usbattached.png}
%%     \label{fig:usbattached}}
%%   \label{fig:usbdevices}
%% \end{figure*}

%% \subsection{Wifisensor}
%% Efter vores indledende erfaringer med AR.Dronens USB-port, valgte vi at
%% implementere en wifi-sensor for på sigt at kunne anvende denne som
%% lokaliseringsredskab. Vores wifisensor består af en USB-wifi-adapter
%% og et snifferprogram til at aflæse og videresende signalstyrker fra de
%% omkringliggende accesspoints og andre wifi-kilder. Snifferprogrammet
%% antager at det trådløse interface der anvendes er sat op i monitor og
%% promiscous mode så det modtager alle pakker der sendes fra de
%% omkringliggende kilder, dette opnåes ved at kalde vores eget
%% startup-script(*ref til annoteret startup*) til sidst i dronens
%% oprindelige bootup-sekvens, i dette script loades også alle de
%% nødvendige kernemoduler.

%% \subsubsection{WIFI-adapter}  
%% Vi har valgt at udstyre AR.Dronen med en Dlink DWL-G122~\cite{electronic:dwlg122}
%% wifi-adapter. Denne specifikke adapter bygger på et Ralink~\cite{electronic:wikiralink} chipset og
%% understøtter monitor mode. Ralink udgiver desuden linuxdrivere til
%% deres chipsets. Efter længere tids søgen og eksperimenteren, fandt vi
%% den korrekte driver og fik denne kompileret til ARM-arkitekturen og
%% kompileret til at være kompatibel med linuxkernen anvendt på
%% AR.Dronen. Efter at have loadet driveren, genkendes vores adapter og
%% interfacet ra0 oprettes i \code{/sys/class/net/}. 

%% \subsubsection{Sniffer}
%% Vores første eksperimenter med hensyn til at kompilere og afvikle kode
%% på AR.Dronen kan ses på *link til blog*, dette var simple 'hello
%% world' eksempler, men gav os indsigt i proceduren omkring
%% crosscompiling til AR.Drone arkitekturen og den anvendte toolchain. 

%% For at skrive et program der kan modtage pakker fra et
%% netværksinterface anvender man ofte linux' pcap bibliotek, dette er
%% dog ikke er installeret på AR.Dronen som standard og skulle først
%% kompileres til ARM-arkitekturen før vi kunne linke til det og senere
%% anvende programmet på AR.Dronen, se *blog ref*.

%% Sniffer programmet har mulighed for at tage et interfacenavn som input
%% når det startes, men som standard anvendes 'ra0' interfacet. Man kan desuden
%% vælge at angive en kanal, at angive om man ønsker at anvende
%% channelhopping til at modtage pakker fra forskellige kanaler og angive
%% om man ønsker at få output til terminalen. Når programmet startes vil
%% det vente på en UDP initieringspakke fra en klient før det begynder at
%% modtage og behandle pakker. Sniffer programmet sender signalstyrker og
%% mac-adresser videre til klienten ved hjælp af UDP kommunikation på
%% port 5551, hvilket gør at det er transparent for klienten om hvorvidt
%% den modtager data fra den originale AR.Drone software eller fra vores
%% udvidelse. 
