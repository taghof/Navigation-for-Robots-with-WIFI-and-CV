%\documentclass[12pt,a4paper,twoside,openright,fleqn]{memoir}
%\documentclass[10pt]{article}
\documentclass[12pt]{report}
%\documentclass[12pt,twoside,openright]{report}
%\documentclass[10pt,twoside]{report}

\usepackage[danish]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{float}
\usepackage{listings}

\usepackage{natbib}
\usepackage{titleref}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tocbibind}

%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{hyperref}

%\newenvironment{nb}
%   {\begin{framed}\begin{quotation}}
%   {\end{framed}\end{quotation}}
%\newcommand{\oversaet}[1]{(\textit{#1})}


\newcommand{\uri}[1]{ #1 }
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textbf{todo:}(#1)}
\newcommand{\dronen}[1]{#1}
\newcommand{\drone}[0]{AR.Drone} %AR.\ $\!$Drone

\begin{document}
\title{Master Thesis}
\author{Thomas Thyregod 20051688\\ Morten Daugaard 20051715\\ \texttt{tysse, taghof}@cs.au.dk\\\\ Department of Computer Science, University of Aarhus\\ Aabogade 34, 8200 Aarhus N, Denmark\\\\
\hline\\
\vspace{4cm}\\
\makeatletter
\texttt{Semi Autonomous Indoor}\\ %Indoor? - nogle udendørs robotter heller ikke bruge gps, for dyrt i præcision...
\texttt{Navigation for Arial Robots}\\
\vspace{4cm}\\
\hline\\
}
\date{\today}
\maketitle
%\newpage

\pagestyle{headings}

\section*{Abstract}
In english...

\section*{Resume}
%Indeholder og gennemgår i det følgende (på dansk).
%-Kort intro hvad vil vi gøre...
Dette speciale handler om at klarlægge muligheder for at gøre en
quadrotor%\cite{electronic:wiki_quadrotor}
 delvis selvflyvende i et
indendørsmiljø. Mere specifikt handler det i første omgang om at
kontruere en base i software som der kan bygges videre på senere ved at
udnytte en AR.Drones%\cite{electronic:ardrone}
 indbyggede sensorapparat
og kontrolinterface. I anden omgang handler det om at undersøge
hvorledes sensorapparatet ombord på AR.Dronen kan udnyttes til at
udlede mere abstrakt information ud af som igen kan anvendes til at
abtrahere en indforstået model som AR.Dronen kan navigere i til støtte
for en menneskelig bruger.
\todo{Kort uddybning af, hvad har vi gjort, hvad var resultatet.}

\pagebreak

\tableofcontents

\pagebreak

\chapter{Introduktion} %Thomas
% på videnskabeligt grundlag identificere, afgrænse og formulere en faglig problemstilling, 
% definere og opstille testbare hypoteser inden for fagets emneområde, 
% selvstændigt planlægge og under anvendelse af fagets videnskabelige metode gennemføre et større fagligt projekt, 
% analysere, kritisk diskutere og perspektivere en faglig problemstilling, 
% vurdere, kritisk analysere og sammenfatte den videnskabelige litteratur inden for et afgrænset emneområde, 
% formidle videnskabelige resultater objektivt og koncist til et videnskabeligt forum
\label{sec:introduktion}
%\subsection{området, teknologi}
%emne generelt, related work, hvad har andre lavet?
%problem specifik
%definition af nøglebegreber
Med 1970'ernes øgede tilgængelighed af halvlederteknologi og de faldende
priser på mikroprocessore %http://en.wikipedia.org/wiki/Intel_4004
er der de senere år (gen)opstået en
interesse blandt innovatøre og forbrugere for mobile robotter, specielt
indenfor ``gadget''-genren. Kendte eksempler herpå er robotstøvsugeren
Roompa\cite{electronic:irobotroomba} af 2002 fra iRobot og den selvkørende
plæneklipper; Husqvarna's Solar Mower\cite{techreport:solarmower} af 1995.

\paragraph{Fremmende teknologier}
Sammenfaldet ligger som sagt i at teknologien nu er moden
til, med billige komponenter, at konstruere maskiner der selvstændigt
kan indsamle elektronisk sensordata og træffe simple beslutninger
baseret på den indsamlede digitale data. Computeren har således været
en nøgleteknologi for udviklingen af det nogle kalder kunstig
intelligens (AI) eller det at en maskine er \emph{autonom}.

\paragraph{Andre anvendelser}
Man ser en stor del af robotudviklingen idag bliver båret af løftet om at
støtte menneskers arbejde og dagligdag gennem
robotteknologi. Sammenlignet med tidligere (og til stadighed) hvor
mekaniseringen ofte har haft konsekvenser for samfundet i form af øget
arbejdsløshed gennem fokus på reduktion af omkostninger i produktionen,
ser man nu også robotter blive taget i brug i
støttefunktioner, altså som medhjælp istedet for afløsning. Eksempler
herpå er udviklingen af en redningsdrone til at afhjælpe
kystvagter. I artiklen\cite{electronic:redningsdronedk} refereres til
en næste %kinect 
version hvor redningsdronen selvstændigt flyver ud til den
nødstedte og aflevere en redningsvest. Et andet anvendelsesscenarie er
brugen af de flyvende maskiner (på engelsk; UAV (Unmanned Arial
Vehicle)) i inspektionen af blandt andet højspændingsledninger og
broer\cite{uavbridgeinspection2007}. Robotter bruges ikke for at
overtage menneskernes job, men som støtte. Menneske og maskine
samarbejder, fordi mennesker er bedre til at træffe beslutninger i
uforudsete situationer og dermed guide robotten, mens robotten er en
perfekt stedfortræder i farlige, kedelige, eller belastende
situationer. 

\paragraph{Omkring AR.Dronen} %problem domæne,
Grundmotivationen bag specialet har langt af vejen været at ``der skal være et underholdende
indhold''. Kigger man på hjemmesiden til promovering af Parrots
AR.Drone\cite{electronic:ardrone} skinner underholdningfaktoren også
tydeligt igennem (se figur~\ref{fig:devisen}).
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.50\textwidth]{grafik/flyingvideogame.png}
  \caption{AR.Dronens logo med slagordene \emph{the Flying Video Game} april 2011 http://web.archive.org/web/20110715044741/http://ardrone.parrot.com/parrot-ar-drone/en/.}
  \label{fig:devisen}
\end{figure}
AR.Drone blev introduceret i 2010 som ``the~Flying~Video~Game'' og
promoveres stadig af producenten som et stykke legetøj, der fjernstyres fra
brugerens iPhone eller iPad. Siden har AR.Dronen dog fundet stor
anvendelse, ikke blot som et avanceret stykke legetøj; forskere,
studerende og hobbyister har taget den fjernstyrede quadrokopter til
sig som en færdig platform og selv udviklet videre ovenpå den.%cite
%\ ... {Related works}          %hobby selvbyg (ombygge fysisk
                                %LED/GPS/WIFI) / flashe ny
                                %styreprogram
                                %ombord \paragraph{Lignende arbejde -
                                %forskning} %\cite evt. flere som har
                                %brugt AR.Dronen i deres forsøg
                                %%\cite{electronic:ludep}\cite{single_img_pespective_cues}
%%%forskning, bl.a. LUDEP. --- UD_DYB hvad det er referencerne har
%%%lavet 
%
%Sousveillance vs. surveillance, noget multimedieæstetik, occupy
%movement thing
%http://www.alternet.org/occupywallst/153542/ows\_fights\_back\_against\_police\_surveillance\_by\_launching\_\%22occucopter\%22\_citizen\_drone/ 

\paragraph{At quadrokoptere er mulige} % enabling technologies
At ``billige'' quadrokopter UAV idag flyver er kun gjort muligt
gennem forbrugselektronikkens udvikling og brug af Micro
Electro-Mechanical Systems (MEMS) accelerometer og gyroskober, samt
evigt faldende priser på mikroprocessorer. AR.Dronen bruger, udover
hovedprocessoren, hurtige accelerometre og dedikeret elektronik til
motorstyring for at stabilisere flyvningen. Uden dem ville det være
umuligt for en utrænet pilot ikke at styrte konstant. 

%flyvende energieffektivt... overblik ift. kørende
%specialet/flyvende udsprunget fra den verden vi befinder os i
%flyvende robotter naturligt valg, flow, 
%bruge i område/case som før kørende robot entity, hvad giver forskellen - hvad
%skal man være bedre til når man flyver, hvad kan man ikke, hvad opnår
%man ved flyv.
Selvom elektronikken ombord på quadrokopteren gør mange ting
lettere, er der stadig mange udfordringer i at opfatte en 3
dimensionel verden og i at styre en robot i 3 dimensioner, samtidig
med at robotten udføre andre opgaver selvstændigt.
En stor del af forskningen i quadrokoptere har hidtil gået på at
optimere mekanikken\cite{quteprints33767}, stabiliseringen og
motorstyringen. I mange eksperimenter har quadrokopteren været
fjernstyret gennem ekstern observation. Eksemplevis i forskningen af
små agile Mikro Arial Vehicle (MAV) i flok\cite{turpin:swarm} er der
fastmonteret kameraer i lokalet omkring MAV til udregning og feedback
af absolut position.

\paragraph{Nyeste forsøg} %nyeste forskning
Idag er der, igen gennem ny nøgleteknologi, lavet
forsøg med ``Simultaneous localization and mapping''
(SLAM)\cite{shen2011kinect}, hvor quadrokopteren er påmonteret en
kinect\cite{electronic:kinect} til selv at opfatte omgivelserne i 3
dimensioner og derigennem selv orientere sig.\newline

\paragraph{Specialets tilgang} % <- Tese herop?
Dette speciale anvender den fjernstyrede AR.Dronen til at opbygge en
robotplatform omkring for at undersøge hvilke krav der nødvendigvis
skal opfyldes for at konstruere en modulopbygget kontrolplatform til
styring af en quadrokopter.

\paragraph{Domæne}
AR.Dronen siges at kunne flyve både indendørs og udendørs. Dette
specialet er afgrænset til at behandle navigation indendørs. For at
skabe en god basis at arbejde videre udfra, defineres afgrænsningen
yderligere. Der er fundet inspiration i indendørsområder som de
beskrevet i arbejdet med den kørende museumsrobotguide Minerva fra
1999\cite{Thrun99minerva:a}. Af hensyn til at lave praktiske forsøg,
bliver de konkrete navigationsløsninger designet til at virke i
IT-byens kontorer og gangarealer samt Zuse-bygningen på Katrinebjerg i
Århus.

%-introduktion til andres arbejde / resultater, 
%-hvad bygger vi videre på (hvis arbejde), 
%-hvad er vores antagelser til at starte med



\section{Motivation} % Thomas & Morten, Hvorfor
\label{sec:motivation}
%Anvendelser}%anvendelser fra dette speciales emner

%\paragraph{Hvem vil vores resultater komme til gode}
forløberen har været ``fremkommelsen'' af nøgleteknologier
``enabling-teknologies'' - har gjort det muligt at udbrede teknologien
kommercielt i legetøjs-helikoptere, hvilket igen har skaffet kapital
og resourcer til udvikling af mere teknologi, - vi ønsker mere
teknologi - derfor arbejde hen imod at konstruere en base som
tilgængeliggør automatisering af flyvende robotter i højere grad for
en bredere gruppe af hobbyister og dermed udvide viden og udvikling på
området. 

\paragraph{Hvorfor er det relevant}
+Optimering -> resource besparelser...?
+Grundforskning...?
+vigtigt at klarlægge - ting, så man forstår konsekvenser hvis senere
tages ibrug...?

\paragraph{Anvendelses områder}
+For hurtigere at kunne gennemløbe cyklusen mellem ide til
implementation til aftestning til analyse til ide.
+ fordi modulopbygget - Lave gode robuste/specialdesignede
hjælpemidler til dedikeret støtte i forskellige arbejdssituationer.

\paragraph{Hvorfor er det bedre/billigere med vores løsninger}
+modulopbygget, hurtigt at udskifte genbruge komponenter
+Nemmere / hurtigere kunne implementere nye produkter.
+en robot skal ikke kunne det hele, så behøver ikke være en stor
computer monolith, eller mekanisk klumpet platform, men lille hvor
tilføje / fjerne moduler afhængig af produkt der designes til.


%mere? - Minerva museums tourguide robot\cite{Thrun99minerva:a}?

referencer til lignende arbejde, vores forskellig fordi..


\section{Teseformulering/mål} % Thomas & Morten, Hvad
\label{sec:tese}
HVAD er det præcis vi vil vise eller afvise?! Lave et

framework/platform til at afhjælpe tiden mellem kodeimplementation til
aftestning 

nærmest punktform, - punkter kan konfirmeres eller afvises i konklusionen.
er det en platform som andre kan arbejde videre på, undersøge hvad der evt. skal til for at lave en sådan
Det er noget med en [semi autonom] [navigation] [indendørs] [luftbåren]

Vi har identifiseret nogle punkter/elementer som vi mener skal med /tilstede
indeholde til modulopbygget kontrolplatform, teste om det kan
implementeres, ---
Hvor kommer de identificerede elementer fra, vores tidligere robotleg
med Lego? - andres artikler?, minerva / ROS men i luften istedet for
på jorden?

fra semiautonom og navigation er der noget, 1) bevægelse i forskellige miljøer, og 2) avoid opførsel, møde med silhuet eller lavt batteri.

at det er indendørs giver nogle begrænsninger, kan bestemme et bestemt miljø ala Hopper-gangene / Ada-gangene eller Zuse.

Luftbåren at vi ligepludselig har en 3. dimension at forholde os til

Er det en del af opgaven at se om AR.Dronen er den rigtige platform at lave dette på? - er en konklusion at den er adequate, men at det ville være bedre havde man haft tiden til at lave sin egen, finde en anden???

\section{Metode} % Thomas, Hvordan, skal den være en SUBsection eller section?
\label{sec:metode}
%% Antagelser, problemdomæne formuleret i introduktionen sektion \ref{sec:introduktion}.
%% (Lavpraktisk:)
%% - hvilke værktøjer vil vi tage i brug AR.Dronen, PC, joystick/controller, Python, => hvorfor, argumentation.
%%    tage AR.Dronen gå fra at bruge den som en fjernstyrbar enhed til at gøre den til en næsten selvstændig semi autonom enhed.

%% -Hvordan vil vi faktisk teste vores tese? - arbejde går imod at lave en testopstilling, hvad skal der til for at opstillingen virker? framework, processor-kraft båndbredde
%% Højniveau?:
%% - er der noget højt,... abstraktion?
hvordan teori



%%%% Introduction - Metode
\subsection{AR.Drone} % Thomas,
Selvstændig selvstående paragraf omkring, valg af AR.Dronen. Hvorfor har vi valgt den, hvad tilfredstiller den, hvad tilfredstiller den ikke. Hvad skal den bruges til.

generelle sensore på dronen; 2 kameraer, acceleration, rotation, hastighed, 
wifi-kommunikation

Discussion af AR.Dronen som tilfredstillende basis... ``Modelling and
control of a quad-rotor robot'', Pounds et al\cite{quteprints33767}
bygge en ny mekanisk forsimplet udgave af en quadrocopter, mere robust
=> færre reperationer... --- formentlig ofte et spørgsmål om
tilgængelighed, ``it's good enough'' ej genopfinde dybtalerken....

Definition på en robot; kan udføre en opgave - en sekvens af operationer/handlinger -- eksemplet en ``dum'' industrirobot,
anden karakteriserende pind/punkt/egenskab; at den på baggrund af sensorinput (feedback kontrol) kan træffe beslutninger / beslutningstræ...
Disse to egenskaber findes allerede i AR.Dronen, udfra det er den tilpas til at starte ud med når vi skal lege med robotter

\paragraph{2 AR.Droner} % Thomas, en med ekstern lab-forsyning, en flyvende

\paragraph{Nu med USB-driver}% Thomas & Morten
- udvide features / muligheder / operationer / operationsområder, 
- samt undersøgt muligheden for at udvide AR.Dronens Hardwareplatform med yderligere sensore eg. usb / mere flashdrive

%%%% Introduction - Metode
\subsection{Domæne/navigationsmiljø} % Thomas,
Nogle bygninger på Katrinebjerg, indendørs...
hvorfor / uddybende beskrivelse af kontor, gang, openspace
%% - hvad er miljøet, Vi begrænser anvendelses-/test-området, hvorfor argumentation er simplificering for os selv, fokusere på det relevante, hvad er det (relevante)?! - pege tilbage på tesenformuleringen sektion \ref{sec:tese}.

\paragraph{PC-linux}
afvikling kommunikation, styring hvem er chef / sensor


\paragraph{Python}
platform skrevet i python med bibliotek / wrapper til opencv (ffmpeg), pygame, numpy...

%%%% Introduction - Metode
\subsection{Platform samt hjælpeværktøjer} % Thomas & Morten
%arkitektur perspektivering analyse?
forskellige frameworks til sensordata opsamling og
udledning/abstraktioner Dey's toolkit / JCAF m.fl.
- vi forestiller os en platform med understøttelse af virtuelle
sensore 

Som hjælp til andre og til dette projekt er der konstrueret en række værktøjer. For f.eks. at minimere den tid der går fra at en ide implementeres i kode til aftestning på quadrokopteren, er det muligt at lave scripts (Task) bestående af kun få liniers kode der nemt afvikles på systemet.
%
\paragraph{drone.py} -modulet / -klassen er konstrueret for at tage sig af al kommunikationen mellem PC og AR.Dronen. Kommunikationen består i at opretholde forbindelsen, samt modtage datapakker fra quadrokopterens sensor system og at afsende styrekommandoer fra brugeren til AR.Dronen.
%
\paragraph{Tasks} er måden at afvikle og teste selvstyrende/kørende opførsel. For at muliggøre manuel støttes, styring og afhjælpning under aftestning kan brugeren anvende PC tastatur og XBox joystick.
%
\paragraph{Joystick- og Keyboard-Controller} er den del af platformen som lytter efter bruger input events til manuel styring af AR.Dronen og som input til den grafiske brugergrænseflade (GUI).
%
\paragraph{GUI til datarepræsentation}


%%%% Introduction - Metode
\paragraph{Mapdrawer / turplanlægger}


%%%% Introduction - Metode
\paragraph{Task manager plugin og tasks}


\subsection{Eksperimenter} % Thomas,

\paragraph{Lokalisering via Wifi-fingerprinting}
%\paragraph{Simpel-blob}
\paragraph{Blob-detection og PID styring} simpel blob, og udbygget nested blob
\paragraph{Korridor vanishpoint}
\paragraph{distance-measurererere}

\paragraph{avoid silhouet}
\paragraph{low battery}
\paragraph{Erkendelse af miljø...}
 Korridor (Hopper) vs. Åbent område (Zuse)

%%%% Introduction
\section{Raport struktur} % Thomas, %/læsevejledning
Introduktion til rapportens arbejde, hvilke afsnit der gennemgåes og hvad afsnit indeholder.

\section{Medfølgende CD}
Dude its a CD
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Opsætning}
\section{AR.Drone}
\label{sec:AR.Drone}
AR.Drone er en quadrokopter fremstillet og udviklet af det franske
firma Parrot \cite{electronic:parrot}\cite{electronic:wiki_parrot}.
 En quadrokopter benytter fire faste rotorer til at
generere opdrift, samt til at styre sin bevægelse i rummet.%\\

Parrots quadrokopter kommer med et sæt sensorer bestående af to kameraer; et vertikalt 
nedadrettet og et horisontalt fremadrettet, en højdemåler, et gyroskop %(til at måle hældning, krængning og drejning)<- senere detalje, her kun intro
, samt et accelerometer. 

I 2012 ventes en opdateret udgave af AR.Dronen. Den nye version tilføjer yderligere et kamera i høj opløsning (HD),
en trykmåler (digitalt barometer) og et kompas til platformen.

AR.Drone er opbygget som en letvægtskonstruktion af kulfiber, plast og
skummateriale og kommer med to forskellige skumskjold til henholdsvis
indendørs (figur~\ref{fig:indendoersskjold}) og udendørs flyvning (figur~\ref{fig:udendoersskjold}).

I det følgende gennemgåes i afsnit \emph{\titleref{ssec:hardware}} de fysiske enheder som quadrokopteren består af, i afsnit \emph{\titleref{ssec:fysik}} gives en introduktion til hvorledes quadrokopteren bruger roterne til at skabe opdrift og bevægelse i luften, i afsnit \titleref{ssec:software} og afsnit \emph{\titleref{ssec:kommunikation}} beskrives den software der ligger på AR.Dronen til at afhjælpe brugerinteraktion og kommunikation immelem AR.Dronen og en klientenhed, mens afsnittet \emph{\titleref{ssec:dronerecap}} lister de obdervationer der er gjort under specialearbejdet og som måske kan hjælpe med at træffe en beslutning om AR.Dronen er det rigtige valg til platform at bygge videre på.

\begin{figure}[ht]
\centering
  \subfigure[Indendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone.jpg}
    \label{fig:indendoersskjold}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[Udendørsskjold]{\includegraphics[width=0.45\textwidth]{grafik/parrot-ardrone-outdoor.jpg}
    \label{fig:udendoersskjold}}
  \caption{AR.Drone med indendørs- og udendørsskjold. Figurene er fra AR.Drone Developer Guide\cite{techreport:ardroneDevGuide}}
  \label{fig:ardrone}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Hardware - subsection
 
\subsection{Hardware}
\label{ssec:hardware}
Det er nærmest umuligt for et menneske at styre en helikopter ved direkte kontrol med mindre man har trænet i mange år (hvilket er det profesionelle piloter gør). Brugerens styring af AR.Dronen er således ikke direkte men støttet op af en feedback kontrol løkke hvortil der er sensorinput fra omverdenen. Sensorene, de aktuatore der gør det muligt at interagere med omverdenen, strømforsyningen og computeren ombord beskrives herunder.

%og må siges at være en af de grundlæggende egenskaber for en robot. At der så er mulighed for at udbygge en stillingstagen iforhold til 
%
%og en processorenhed. Herunder beskrives sensorene og aktuatore samt den ombordværende computer og trømforsyningen.

\subsubsection{Sensorer}
\label{sssec:sensorer}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.50\textwidth]{grafik/frontcamera.jpg}
  \caption{AR.Dronens frontkamera.}
  \label{fig:frontkamera}
\end{figure}
\paragraph{Optik}
AR.Drone er som sagt udstyret med to kameraer; et nedadrettet og et fremadrettet.
Det fremadrettede er med en synsvinkel på $93 ^{\circ}$ og består af en CMOS\footnote{Complimentary-Symmetry Metal Oxide Semiconductor, på dansk: en sensor af komplimentær metaloxid halvleder teknologi.}-billedsensor som kan levere $640 \times 480$ billeder, AR.Dronen transmittere dog kun med en opløsning på Quarter Video Graphics Array (QVGA) $320 \times 240$ med en framerate på $15 FPS$.
Det nedadrettede kamera er en CMOS sensor med $64 ^{\circ}$ vinkel, der tager billeder i Quarter Common Intermediate Format (QCIF) $176 \times 144$ opløsning, med en framerate på $60 FPS$.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/navboard.png}
  \caption{Over- og underside af Navigationsprint til undersiden af AR.Dronen. Navboardet indeholder bl.a. ultralydssensoren. Figurene er fra to internetbutikker \uri{droneparts.de} og \uri{aust-rc-tech.com.au} hvorfra der kan bestilles AR.Drone reservedele.}
  \label{fig:navboard}
\end{figure}

\paragraph{Højdemåler} Til at bestemme AR.Dronens afstand til jorden
anvendes en ultrasonisk afstandsmåler. Denne opererer ved at udsende
en ultralydspuls og herefter måle tidsintervallet der går før ekkoet
måles. Ultralydsmåleren er angivet til at virke op til 6 meter og er
rent fysisk placeret på navboardet (se figur \ref{fig:navboard}) på
AR.Dronens underside.
\paragraph{Acceleration og inerti} AR.Drone er udstyret med to
gyroskoper, et 2-akses gyroskop til at måle hældningsraten om x- og
y-aksen (pitch og roll) samt et 1-akses piezoelektrisk gyroskop til at
måle rotationen om z-aksen (yaw). Gyroskoperne kombineres med et
3-akses accelerometer i en samlet enhed til at måle inerti
(IMU)\footnote{Inertial Measurement Unit}, denne anvendes til løbende
at give estimater af Euler-vinklerne; theta, phi og psi. Hvor phi og
theta er henholdsvis pitch og roll, mens at psi er yaw. For en grafisk
repræsentation se evt figur \ref{fig:drone_movements}.

\subsubsection{Aktuatorer}
\label{sssec:aktuatorer}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/motor_gear.png}
  \caption{a) Motorreservedel, b) samlet motor og gear på kulfiberkryds og c) gear-reservedel.}
  \label{fig:motorgear}
\end{figure}
AR.Dronens aktuatorer er de fire børsteløse $15 Watt$ motorer. Motorerne
arbejder i intervallet $10350$ til $41400$ omdrejninger per minut ($RPM$) og når AR.Dronen står stille i
luften arbejder motorerne med $28000 RPM$. Dette svarer, grundet gearingen,
til $3300 RPM$ for propellerne. Motorerne har uden sammenligning det
største strømforbrug af alle AR.Dronens enheder.

\subsubsection{Den indlejrede computer}
\label{sssec:processor}
Den centrale beregnerenhed på AR.Dronen er en ARM9 $468 MHz$
processor (Parrot 6 ARM926EJ) med $128 Mbyte$ DDR RAM til 
arbejdshukommelse og $128 Mbyte$ NAND flash til persistent (vedvarende) hukommelse.

Til den embeddede computer er der et integreret trådløst netværkskort (Atheros AR6102G-BM2D) til wifi. 

På undersiden af quadrokopteren er der et 7 benet molex-stik til ekstern serialkommunikation. Porten virker bl.a. som en On-The-Go USB-port, der primært anvendes ved opdateringer.

\subsubsection{Strømforsyning}
\label{sssec:forsyning}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/batteries.jpg}
  \caption{Batterier til AR.Dronen (Figuren er fra www.techpowerup.com/\-reviews/\-Parrot/\-AR Drone/\-images/\-batteries.jpg).}
  \label{fig:batterier}
\end{figure}
Batteriet der driver AR.Dronens enheder er et 3 celle lithium-polymer batteri med en open-circuit\footnote{Open-circuit spændingen er den spænding der er over batteriet når det er uden belastning og ikke oplades.} spænding på $11,1 Volt$, med ladning på $1000$ milli-ampere-timer ($mAh$) og en afladnings hastighed på $10$ Coulomb ($C$). Opladning af batteriet kan foregå i løbet af 90 minutter. Efter sigende skulle batteriet give omking 12 minutters flyvetid\cite{electronic:ar.drone_parrot_technologies}\cite{electronic:wiki_ar.drone}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%%     AR.Drone - section
%%%%      - Hardware -
%%%%      + Fysik - subsection
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Quadrokopter fysik}
\label{ssec:fysik}
AR.Dronen fåes som nævnt med 2 skrog eller skjold
(figur~\ref{fig:indendoersskjold}), og vejer 420 gram med det
indendørs skrog monteret. Et pund er en relativ lille vægt, andre
quadrokoptere\cite{quteprints33767} har f.eks. 4-5 kg som de skal
løfte rundt på. 

De 4 propeller sidder i hver sit hjørne af et kulfiberkryds på
AR.Dronen. Når en propel på AR.Dronen rotere om sin akse flytter den
luften i et areal rundt om propellen. Lufttrykket på undersiden af
propellen bliver herved større end lufttrykket på oversiden af
propellen. Hvis AR.Dronen ligger vandret vil forskellen i lufttryk
giver en
Bernoullieffekt\cite{nla.cat-vn856146}%\cite{electronic:wiki_bernoulli} 
 som påvirker AR.Dronen med en kraft modsat tyngdekraften
 (thrust)\cite{electronic:wiki_quadrotor}. Udover thrust påvirker den
 roterende bevægelse også propellens base med et moment (torque)
 modsat propellens retning. Eksempelvis vil en helikopter uden haleror
 dreje ukontrolerbart rundt om sig selv, men fordi AR.Drone har et par
 rotore som drejer een vej (med uret), mens et andet par drejer modsat
 (imod uret)\cite{techreport:ardroneDevGuide}, bliver summen af
 momentpåvirkning nul, hvis de 4 rotere drejer med samme hastighed. 
Denne egenskab bruges til at styre AR.Dronens flyvning og bevægelser i
rummet, som kan beskrives med 3 bevægelsesakser; x, y og z (se
figur~\ref{fig:drone_movements}) som er lokale for quadrokopteren. 

AR.Dronens bevægelser styres ved at kontrollere dens vinkel i forhold
til de 3 akser. Skal AR.Dronen f.eks. flyve fremad, sættes forenden
til at være lidt under bagenden i forhold til det vandrette plan. I
praksis sker det ved at sænke vinkelhastigheden på den forreste propel
samtidig med at omdrejningsantallet øges %spørgsmålet er hvad er
moment påvirkningen af $(2*omega_H + delta_A - delta_B) - (2*omega_H)$ 
på den bageste propel. Det resulterer i at AR.Dronen hælder lidt 
fremad som på figur \ref{fig:drone_movements}c. Samme princip gør sig
gældende når AR.Dronen skal flytte sig sideværts som på figur
\ref{fig:drone_movements}b.

\begin{figure}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/drone_movements.png}
  \caption{(a) vinkelhastigheden $\Omega$ øges med $\Delta_A$ på alle 4 propeller, quadrokopteren øger dermed løftekraften (acceleration i Z-aksen), (b) Vinkelhastigheden øges på venstre og mindskes på højre propel, quadrokopteren laver et ``roll'' til højre (acceleration i Euler-vinklen theta), (c) samme som b, men i pitch (acceleration i euler-vinkel phi) (d) En større vinkelhastighed på de med-uret-drejende propeller gør at torque bliver ulige og at quadrokopteren drejer mod venstre (acceleration i yaw/psi-vinklen). Ved bevægelser frem og tilbage udledes en positiv eller negativ hastighed ($V_x$), ligesom ved bevægelser sideværts ($V_y$). figuren er fra AR.Drone SDK Development guide\cite{techreport:ardroneDevGuide}}
  \label{fig:drone_movements}
\end{figure}

Hvis AR.Dronen skal dreje om sin egen akse for at kigge længere til
venstre bruges samme egenskab som før; det moment som tilføres af de 2
propeller der drejer med uret skal overstige det moment som tilføres
af de 2 propeller der drejer modsat. Det gøres ved at øge
vinkelhastigheden på de to førstnævnte og sænke hastigheden på de
propeller der drejer modsat uret.

Når AR.Dronen går fra at ligge vandret til istedet at hælde, så
flyttes noget af thrust fra at virke lodret imod tyngdekraften til
også virke sideværts. Det betyder at propellernes samlede
vinkelhastighed skal øges for at opretholde den samme afstand til
jorden. Ifølge SDK DevGuide\cite{techreport:ardroneDevGuide} bør
hældningen på phi og theta ikke overstige $0.52 rad$ ($30 ^{\circ}$)
ellers kan flyvehøjden ikke opretholdes. Den maximale hældning kan
sættes til f.eks. 0.25 radianer via AT-kommandoen i figur
\ref{fig:ateulerangle}.
\begin{figure}
  \begin{framed}
\begin{verbatim}
 AT*CONFIG=605,"control:euler_angle_max","0.25"
\end{verbatim}
  \end{framed}
  \caption{Eksempel på indholdet af en AT-kommandobesked. Beskeden
    overføres fra klient-enheden til AR.Dronen på UDP-port 5556. Lige
    netop denne besked sætter en softwarekonstant i AR.Dronens
    firmware, der bestemmer den maksimalt tilladte hældning på $\Phi$
    og $\Theta$.}   
  \label{fig:ateulerangle}
\end{figure}

I styreprogrammet på AR.Dronens computer kan man sætte en konstant (se
figur~\ref{fig:ateulerangle}) til at styre den øvre grænse for hvor
meget quadrokopteren maximalt kan krænge. Konstanten \emph{
  Euler\_angle\_max } virker som en cut-offvinkel, dvs. under flyvning
tester AR.Dronen løbende at hverken pitch ($\Phi$)- eller roll
($\Theta$)-vinklen overskrider grænsen. Sker det lukkes systemet ned;
kraften til rotorene klippes og AR.Dronen lader sig falde til jorden. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Software - subsection

\subsection{Software på AR.Dronen}
\label{ssec:software}
AR.Dronens microcomputer kommer med en Linux 2.6.27 kerne
installeret. Det bemærkes at 2.6.27 er den første kerne med indbygget
support for UBIFS\cite{electronic:wiki_ubifs} som er det filsystem der
anvendes på AR.Dronens NAND flashhukommelse.

\subsubsection{Busybox linux platformen på AR.Dronen}
\label{sssec:busybox}
Udover Linux kernen leveres AR.Drone også med
Busybox\cite{electronic:busybox} installeret.  Busybox giver adgang
til en række Unix værktøjer (copy \code{cp}, list \code{ls} osv.)
optimeret til brug i embeddede systemer. Busybox konfigureres så kun
de ønskede værktøjer medtages og disse pakkes så til en enkelt
eksekverbar fil.  Dette betyder at størrelsen holdes på et
minimum. Busybox på AR.Drone fylder $483 kbyte$ og udstyrer AR.Dronen
med alle unix brugerkommandoer man forventer på et standard linux
system, e.g. \code{cd}, \code{ls}, \code{mv}, \code{ln} osv.  Busybox
på AR.Dronen leverer desuden en Dynamic Host Configuration
Protocol\cite{rfc2131} (DHCP) service; den DHCP-server AR.Dronen
anvender til at uddele %DHCP cite?  IP\cite{rfc791}-adresser og lave
et netværk mellem en klient og AR.Dronen selv, samt en
telnet\cite{rfc854}-server der giver adgang til AR.Dronens terminal
%telnet cite?  og en texteditor %(Vi) der gør det muligt at redigere
filer direkte på AR.Dronen.

\subsubsection{Program.elf -- AR.Dronens styreprogram}
\label{sssec:program.elf}
Den firmware der står for selve styringen, transmitteringen af data og
bearbejdning af sensordata i forbindelse med flyvning er implementeret
i \uri{/bin/program.elf}. Da der er tale om proprietært software
betragtes programmet som en lukket kasse uden hensyntagen til
implementeringen, vi kan dog udlede forskellige elementer efter at
have observeret AR.Dronen under flyvning. -- \uri{Program.elf}
indeholder blandt andet
\begin{itemize}
\item en PID controller til at stabilisere AR.Dronen når der svæves,
\item en algoritme til at udlede AR.Dronens hastighed ved hjælp af
  bundkameraet (bekræftes også i en
  artikel\cite{velocity_estimation_inside_the_drone} af Parrot
  selv.)
\item og en algoritme til at detektere tags i billeder i forbindelse
  med augmented reality spil.
\end{itemize}
\uri{Program.elf} modtager kommandoer fra brugeren gennem en User
Datagram Protocol\cite{rfc768} (UDP) port. Kommunikationsinterfacet
mellem bruger og quadro\-kopteren beskrives i
afsnit~\ref{ssec:kommunikation}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/telnetscreenshot.png}
  \caption{Forbindelse til AR.Dronens telnetserver på IP addresse $192.186.1.1$.}
  \label{fig:telnetterminal}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     AR.Drone - section
%%%%      + Klient/server kommunikation - subsection


\subsection{Kommunikation mellem AR.Dronen og klient} %mellem AR.Dronen og en klient-enhed
\label{ssec:kommunikation}
Interaktion med de programmer der kører på AR.\ $\!$Dronens linux-platform kan kategoriseres i 3 grupper:
\begin{itemize}
  \item Som klient (PC, smartphone eller lignende) via Telnet eller
    FTP\cite{rfc959} over trådløst netværk til AR.\ $\!$Dronen, hvor AR.\ $\!$Dronen fungere som server.
  \item Ved at eksekvere en applikation på en klient (PC, smartphone
    eller lignende) der sender og modtager UDP-pakker og
    AT-kommandoer\footnote{AT(tention)-kommandoer over trådløst netværk (gennemgåes nærmere i
      sektion \ref{sssec:atinterface}}) til det kørende firmware
    \uri{program.\ $\!$elf} på AR.\ $\!$Dronen.
  \item Ved at eksekvere et program der kører
    og kommunikerer internt på AR.Dronen.
    Programmet kommunikerer med
    AR.\ $\!$Dronens firmware \uri{program.\ $\!$elf} gennem de tre UDP-porte der
    også anvendes ved almindelig trådløs kommunikation.
\end{itemize}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.70\textwidth]{grafik/ar_drone_nokia900.png}
  \caption{Fjernstyring af AR.Dronen med en Smartphone, her en Nokia-telefon. Figuren er oprindeligt fra http://www.nokiawp.net/wp-content/uploads//2010/12/ar-drone-n900.jpg.}
  \label{fig:nokiastyring}
\end{figure}

\subsubsection{TCP baseret kommunikation}
\label{sssec:tcp}
Fra en klient-computer er det muligt at få adgang til AR.Dronens 
installerede linuxinstallation ved at oprette en telnetforbindelse til AR.Dronens 
faste IP-adresse; $192.168.1.1$. Efter at forbindelsen er etableret 
får brugeren her adgang til en root-terminal (se figur~\ref{fig:telnetterminal}).

Ved at oprette forbindelse gennem FTP, fra et FTP-program på en 
ekstern klientcomputer til AR.Dronens FTP-server, kan man overføre 
filer imellem AR.Dronen og klientcomputeren med FTP-protokollens 
PUT og GET kommandoer. Hvis klienten ikke angiver en sti lægges 
overførte filer som standard i \uri{~/data/video} mappen på AR.Dronen.

\subsubsection{UDP baseret kommunikation}
\label{sssec:udp}
I modsætning til FTP- og Telnet-klienterne, som er TCP baserede, anvendes
UDP-protokollen til den kommunikation der sker mellem \uri{program.elf} og
brugerens system, f.eks iPhone eller PC.

Firmwaren \uri{program.elf} på AR.Dronen sender og modtager pakker på tre UDP-porte:
\begin{itemize}
  \item På port 5556 (command port) lytter quadrokopteren efter brugerkommandoer, afsendt fra klientenheden e.g. ``takeoff'', flyv-frem, drej, land osv.
  \item på port 5555 (video port) sender AR.Dronen en videostream til klientenheden.
  \item på port 5554 (navdata port) sender AR.Drone løbende statusinformation om f.eks. hastighed, eulervinkler, afstand til jorden osv. Pakkerne fra port 5554 kaldes \emph{navdata} fordi pakkerne indeholder information om navigationstilstanden.
\end{itemize}


\subsubsection{AT kommando interface}
\label{sssec:atinterface}
En klientenheden som skal kommunikerer med AR.Dronens \uri{program.elf}, gør dette ved hjælp af
såkaldte attention-kommandoer (AT command). En AT-kommando er basalt set blot en
tekststreng encodet som 8 bits karakterer. AT-kommandoer sendes til AR.Dronens
kommando\-port (5556) som UDP-pakker. Formatet for AT-kommandoer ses på
figur \ref{fig:atsyntaks}.

\begin{figure}
  \begin{framed}   
\begin{verbatim}
 AT*[kommandonavn]=[sekvensnummer],[arg1, arg2 ... argN]<LF>
\end{verbatim}
  \end{framed}
\caption{Syntaksen for en AT-kommando; [kommandonavn] kan f.eks. være
  \code{PCMD}. [sekvensnummer]-heltallet skal være stigende for hver
  ny afsendt AT-besked. Listen med argumenter; [arg1, arg2 ... argN],
  variere i antal afhængig af konteksten. Ved afsending af en
  \code{PCMD}-kommando skal der medsendes 5 talværdier for hhv. flag,
  roll, pitch, gas og yaw.} 
\label{fig:atsyntaks}
\end{figure}

Ved at afsende en kommando som i figur~\ref{fig:ateksempel} bestemmer
man hvorledes AR.\ $\!$Dronen skal bevæge sig ved translation og rotation,
som beskrevet i \titleref{ssec:fysik} sektion \ref{ssec:fysik}. 

\begin{figure}
  \begin{framed}   
\begin{verbatim}
 AT*PCMD=21625,1,0,0,0,0<LF>
\end{verbatim}
  \end{framed}
\caption{Eksempel på en \code{PCMD} AT-kommando. Denne specifikke
  kommando bringer AR.Dronen i hovertilstand. [sekvensnummer]'et er 21625. Listen med argumenter er hhv. flag=1, roll=0, pitch=0, gas=0 og yaw=0.}
\label{fig:ateksempel}   
\end{figure}
%    [kommandonavn] er navnet på den ønskede kommando
%    [sekvensnummer] er et globalt, altid stigende sekvensnummer
%    [arg1, ... argN] er et variende antal parametre

Brugen, syntaksen og betydningen af 7 forskellige AT-kommando navne
beskrives i detaljer i afsnit 6 af AR.Drone Development
Guide\cite{techreport:ardroneDevGuide}. Det skal dog nævnes at
man i reglen skal sikre sig at man vedbliver at sende beskeder
periodisk for at AR.Dronen ikke skal tolke forbindelsen til
klientenheden som tabt. En tabel med AT-kommandoerne kan ses i tabel
\ref{tab:kommandoer} i appendiks \ref{sec:at}. 

\subsection{Vurdering af/begrænsninger i AR.Dronen som robotplatform} % Thomas,
\label{ssec:dronerecap}
Under den indledende informationssøgning til dette speciale og gennem
diverse forsøg og eksperimenter undervejs, er der indsamlet en del erfaringer med
AR.Dronen. Nogle af de mest markante observationer er
listet herunder.

\paragraph{Omkring hastighed og inerti}
Det giver mening at beskrive AR.Dronens bevægelser i 3
dimensioner for derved nemmere direkte at kunne bruge accelerometer- og gyro-målinger
fra sensorene om bord. En observeret egenskab er at man ikke kan sige noget udfra
accelerometer og gyroer (de tilstedeværende sensore) omkring hvor
langt AR.Dronen har fløjet eller med hvilken hastighed AR.Dronen
bevæger sig.
%jo mindre des mindre inerti kan være indeholdt, mere agile agil/adræt
 Dog kan der udledes estimater for hastigheder ved at integrere over
accelerationen, men hastigheden i det horisontale plan er i praksis
afhængig af miljøet og kan ikke umiddelbart bestemmes. Roll og pitch
værdierne kan være aflæst til 0 og AR.Dronen vil ligge horisontal, men
kan stadig godt bevæge sig sideværts i luften\footnote{AR.Dronen siges
  at være ``trimmet'' (med en konstant hastighed, evt. $0m/S$, men også forskellig fra $0m/S$) hvis den
  ikke påvirkes af en resulterende kraft og acceleration dermed er 0}
på grund af indebåren inerti eller f.eks. en stille konstant
vind. Bevæger AR.Dronen sig med en konstant hastighed kan det ikke
aflæses af accelerometeret. Afstanden AR.Dronen har flyttet sig kan
således heller ikke udledes af accelerometer og gyro alene.
 Parrot har derfor underbygget hastighedsestimatet med en optical-flow
algoritme på billedstrømmen fra bundkameraet i firmwaren, således at
denne er blevet noget mere troværdig\cite{velocity_estimation_inside_the_drone}.

Dette speciales egne eksperimenter med hastighedsudledning er beskrevet i
afsnit~\ref{sec:distance-mesu}. Det viser sig at under de rette 
forhold er hastighedsudledningen udmærket brugbar. Udledningen af
hastighed besværliggøres ved forhold med dårlig belysning og ved
jordoverflader uden forskelligartet tekstur, hvor det ikke er muligt for
bundkameraet at opfange tilpas mange gode features at følge.

\paragraph{Ultralydssensor}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/sonarzone2.png}
  \caption{a) Udbredelsesområde for ultralydssensorsignal, b) man ser at afstanden ikke nødvendigvis er vinkelret på sensoren idet der kan forekomme ekko fra et større område under sensoren (figuren er fra \uri{http://www.cs.brown.edu/\-people/\-tld/\-courses/\-cs148/\-02/\-sonar.html}). \todo{Skal den i referencer?}}
  \label{fig:sonicbeam2}
\end{figure}
Højdemåleren består som nævnt af en ultralyds-sender og
-modtager. Teknologien gør at højden man får ud ikke nødvendigvis er
fra det vinkelrette punkt udfor sensoren, men derimod returnerer den
mindste højde indenfor et område som det på figur
\ref{fig:sonicbeam2}. Det vil sige højden er udledt af den tid der går
fra en lydpuls afsendes, til der modtages et ekko. Ekkoet behøver ikke
komme fra en stor overflade som en væg eller et gulv, men kan være en
kasse inde i sensorområdet som figur~\ref{fig:sonicbeam2}a. Det kan
specielt være et problem hvis der flyves i et trangt lokale med
kasser, stole og borde på gulvet, hvor de mange legemer vil give en
falsk opfattelse af højden til gulvet.  Fordi teknologien netop er lyd
vil der også være indeboende problemer med ekko på skrånende og bløde
overflader.  % blæst lyd %specielt i små rum/kontorlokaler, små kasser
%/ borde + turbolens
%
De ovennævnte problematikker kan have indflydelse ved udvikling med
AR.Dronen. Specielt i de tilfælde quadrokopteren hælder meget, ie. ved
flyvning i høj hastighed kan vinklen til gulvet indføre fejl i
højdemålingen.
%
\paragraph{Realtidssystem} På forummet AR Drone
Flyers\cite{electronic:ardroneflyers80} er en bruger der har skrevet,
at operativsystem, styreprogram og de eksterne processer der kører på
AR.Dronen tilsammen bruger op mod $80\%$ af CPU-kraften.  Om det lige
akkurat er $80\%$ kan formentlig diskuteres. Under alle omstændigheder
vil et overforbrug have indvirkning, ie. hvis man vælger at afvikle
yderligere processer på AR.Dronen, eg. USB-understøttelse under
flyvning og Wifi-signalstyrke indsamling som beskrevet i
afsnit~\ref{ssec:wikilokalisering}. Pointen er at man ikke kan bruge
mere end de resterende f.eks. $20\%$ uden at det vil gå ud over
kontrollen med flyvningen.
%
\paragraph{Trådløs båndbredde} Der er ikke et stort overhead i User
datagram protokollen pga. transmissionskontrol. Det gør at der i
princippet kan sendes flere f.eks. videopakker end med en
synkroniseret kommunikationsprotokol som TCP. Oveni at der
kontinuerligt sendes friske datapakker så giver det mest mening at
anvende en asynkron protokol i realtidskommunikationen med
quadrokopteren idet det ikke kan betale sig at bruge resourcer på at
generhverve det fåtal af billedframes der går tabt pga. fejl i
transmissionen. Det er bedre at indlæse den næste datapakke
istedet. Information i den gamle datapakke vil alligevel være forældet
og irrelevant.
%
\paragraph{Billedkvalitet} AR.Drone er et produkt der er designet til
mainstream og masserne. Kravende har været, at den skulle være relativ
billig i produktion for dermed let at kunne erhverves af menigmand. I
dette valg af design har Parrot afvejet et tradeoff mellem kvalitet af
billeder versus antal af billeder der bliver afsendt per sekund. Det
betyder at opdateringshastigheden af billeder og data fra AR.Dronen er
tilstrækkelig i de fleste tilfælde, men, som det fremgår af
afsnit~\ref{sec:QRdecoding}, at til en simpel ting som afkodning af
QR-koder (hvortil der findes færdige værktøjer) er billedkvaliteten
ikke af tilstrækkelig kvalitet.
%
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.60\textwidth]{grafik/2frames.png}
  \caption{Et billede fra AR.Dronens bundkamera (med opløsing på $88\times72)$ i øverste venstre hjørne af et billede fra frontkameraet ($320 \times 240$).}
  \label{fig:2frames}
\end{figure}
%
\paragraph{Zap imellem 2 kameraer} I forbindelsen mellem klient og
AR.Dronen kan man kun modtage en billedramme ad gangen. Dvs. at enten
skal man få billeder fra frontkameraet eller fra bundkameraet. -- Skal
man bruge billeder fra begge kameraer, må man skifte imellem dem
(zappe) og skiftevis få et billede fra frontkamera hver anden gang og
bundkameraet de andre gange.  Dette er ikke praktisk idet det vil
nedsætte opdateringshastigheden yderligere i eksempelvis en
feedback-kontrol løkke. Ydermere er der et lille delay forbundet med
at skifte feed fra fra det ene til det andet kamera.  Insisterer man
alligevel på at få billeder fra begge kameraer samtidig, er det dog
muligt. Billeder fra de to kameraer lægges ovenpå hinanden til eet
billede (se figur~\ref{fig:2frames}), som så kan modtages af klienten.
Det billede der er indeholdt i det andet er af noget mindre opløsning
($88 \times 72$) end hvis det havde været sendt alene ($320 \times
240$). Denne forskel i kvalitet har tydeligt en indflydelse på
resultatet af diverse billedanalysealgoritmer.

%Derefter finde ud af at jo mindre des mindre inerti kan være indeholdt, mere agile agil/adræt
%gå fra centraliseret styring til fosøg med decentraliseret, swarm anonymitet, største proble er sanse apparatet, førhen flere ekserne kameraer til observation og feedback af position til flokken, nu forsøg med lokal dataindsamling fra bl.a. Kinect {http://www.ros.org/wiki/kinect}, {pelican quadrotor}

\todo{afsluttende bemærkning... evt. reference yderligere
  behandling/opsummering i Resultater/Konklusion...}

\pagebreak
\section{Udvidelse af AR.Dronen}
En robotplatform der kan udvides med nye sensorer efter behov, er
klart mere anvendelig end en helt lukket platform. Vi har derfor
undersøgt mulighederne for at anvende AR.Dronens OTG-USB* port til
andet end at udføre opdateringer, dette har blandt indbefattet at
kompilere kernemoduler(*) til AR.Dronens linuxkerne(*ref til blog*),
samt at omgå AR.Dronens software for ikke at miste strømmen til
porten. Vores proof-of-concept var at få loadet en almindelig usbstick
og skrive og læse fra denne(*ref til blog*), dette var forholdsvis
simpelt, da driverne til dette formål er meget generiske.

\begin{figure*}[htp]
  \centering
  \subfigure[USB-kabel og de enheder vi har haft tilslutttet AR.Dronen.]{
    \includegraphics[width=0.47\textwidth]{grafik/usbdevices.png}
    \label{fig:cableandusbdevices}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[AR.Dronen med usbstick tilsluttet.]{\includegraphics[width=0.47\textwidth]{grafik/usbattached.png}
    \label{fig:usbattached}}
  \label{fig:usbdevices}
\end{figure*}

\subsection{Wifisensor}
Efter vores indledende erfaringer med AR.Dronens USB-port, valgte vi at
implementere en wifi-sensor for på sigt at kunne anvende denne som
lokaliseringsredskab. Vores wifisensor består af en usb-wifi-adapter
og et snifferprogram til at aflæse og videresende signalstyrker fra de
omkringliggende accesspoints og andre wifi-kilder. Snifferprogrammet
antager at det trådløse interface der anvendes er sat op i monitor og
promiscous mode så det modtager alle pakker der sendes fra de
omkringliggende kilder, dette opnåes ved at kalde vores eget
startup-script(*ref til annoteret startup*) til sidst i dronens
oprindelige bootup-sekvens, i dette script loades også alle de
nødvendige kernemoduler.

\subsubsection{WIFI-adapter}  
Vi har valgt at udstyre AR.Dronen med en Dlink DWL-G122~\cite{electronic:dwlg122}
wifi-adapter. Denne specifikke adapter bygger på et Ralink~\cite{electronic:wikiralink} chipset og
understøtter monitor mode. Ralink udgiver desuden linuxdrivere til
deres chipsets. Efter længere tids søgen og eksperimenteren, fandt vi
den korrekte driver og fik denne kompileret til ARM-arkitekturen og
kompileret til at være kompatibel med linuxkernen anvendt på
AR.Dronen. Efter at have loadet driveren, genkendes vores adapter og
interfacet ra0 oprettes i \code{/sys/class/net/}. 

\subsubsection{Sniffer}
Vores første eksperimenter med hensyn til at kompilere og afvikle kode
på AR.Dronen kan ses på *link til blog*, dette var simple 'hello
world' eksempler, men gav os indsigt i proceduren omkring
crosscompiling til AR.Drone arkitekturen og den anvendte toolchain. 

For at skrive et program der kan modtage pakker fra et
netværksinterface anvender man ofte linux' pcap bibliotek, dette er
dog ikke er installeret på AR.Dronen som standard og skulle først
kompileres til ARM-arkitekturen før vi kunne linke til det og senere
anvende programmet på AR.Dronen, se *blog ref*.

Sniffer programmet har mulighed for at tage et interfacenavn som input
når det startes, men som standard anvendes 'ra0' interfacet. Man kan desuden
vælge at angive en kanal, at angive om man ønsker at anvende
channelhopping til at modtage pakker fra forskellige kanaler og angive
om man ønsker at få output til terminalen. Når programmet startes vil
det vente på en UDP initieringspakke fra en klient før det begynder at
modtage og behandle pakker. Sniffer programmet sender signalstyrker og
mac-adresser videre til klienten ved hjælp af UDP kommunikation på
port 5551, hvilket gør at det er transparent for klienten om hvorvidt
den modtager data fra den originale AR.Drone software eller fra vores
udvidelse. 
 
\section{Fasciliterende klient platform} % Morten,
Vi har udviklet den grundlæggende platform med det formål, at
understøtte vores videre arbejde med AR.Dronen. Platformen skal sikre
let og effektiv adgang til AR.Dronens sensor output, samt give
mulighed for at styre AR.Dronen, både manuelt(uden brug af iPhone,
iPad eller lignende) og ved hjælp af prædefinerede opgaver~\ref{sec:tasks}.

\subsection{Implementationsdetaljer}
Vi har af flere grunde valgt at implementere vores platform i python,
den vigtigste af disse er, at vi ønsker at stille en let anvendelig og
let udbyggelig platform til rådighed for udviklermiljøet omkring
AR.Dronen. Da Python er et fortolket højniveau sprog kan vi
naturligvis ikke opnå samme afviklingshastighed som hvis vi havde anvendt C
eller andre maskinnære sprog.  I forbindelse med afkodningen af video
fra AR.Dronen anvender vi biblioteket Psyco~\cite{electronic:psyco} til at netop øge
afviklingshastigheden. Psyco er en form for JIT compiler der lader os
afvikle umodificeret pythonkode hurtigere, ved afkodning af 100
billeder er præcis den samme kode med Psyco mere end 5 gange hurtigere
end koden uden(fra 39,569 sek. til 7.075 sek.). Udviklingen af Psyco
er desværre stoppet og der findes ingen 64-bit version af biblioteket.

\subsection{Opbygning/overblik}
Vores platform består af et antal klasser placeret i et antal
moduler. Vi har udviklet tre forskellige receiverklasser(video,
navdata og wifi i receiversmodulet), to forskellige
controllerklasser(joystick og keyboard i controllersmodulet), en taskmanager og
en kontrolinterface-klasse(controllersmodulet). Derudover har vi med
inspiration fra~\cite{lenser:behaviorbasedarchitecture} implementeret en virtuel sensor der bygger oven på videoreceiveren. 

I standard situationen instantieres, startes og tilgåes disse klasser fra
droneklassen(dronemodulet). På denne måde kan udviklere der ønsker at
anvende vores platform blot importere drone modulet og instantiere
droneklassen, hvorefter der er let adgang til diverse sensordata. Vi
har desuden udviklet et testdevice modul, som anvendes i forbindelse
med udvikling når det ikke er muligt at have en AR.Drone fysisk
tilstede. De enkelte klasser kan også anvendes hver for sig, se*.

klasse diagram eller lignende

\subsubsection{Receivers}
Vores receivers er abstraktioner over de fysiske hardwaresensorer på
AR.Dronen. Receivernes opgaver er at initialisere UDP-kommunikationen
med AR.Dronen ved at sende en startbesked og herefter modtage de
datapakker AR.Dronen sender, desuden skal datastrømmen holdes i live
ved at sende startbeskeden i faste intervaller. 
Alle vores receiverklasser nedarver fra en baseklasse der indkapsler
den basale receiverfunktionalitet. For bedre at kunne udnytte en
multicore processor er denne baseklasse er implementeret som en
selvstændig proces ved hjælp af Pythons
multiprocessing-modul. Receiverprocessen kommunikerer ved hjælp af en
delt liste, som anvendes både til at overføre signaler og data.  For
at tilpasse de enkelte receivere implementerer disse specifikke
metoder(for eksempel til afkodning af den specifikke type data) der
kaldes på bestemte tidspunkter under det generelle receiverloop.
 
\paragraph{Navdatareceiver}
Navdatareceiveren modtager navigationsdata(højde, hastighed mm.) fra
AR.Dronen og decoder denne, i forhold til de strukturer der findes
beskrevet i **filnavn** fra Parrots SDK  

\paragraph{Videoreceiver}
Videoreceiveren modtager og afkoder AR.Dronens videostrøm, afkodningen
foregår på baggrund af den process der nævnes i **AR.Drone dev
guide**. Formatet der sendes i minder meget om JPEG formatet og
forfatteren til den algoritme vi har tilpasset, spekulerer
i~\cite{electronic:parrotforum2} om det ville være muligt at anvende
en decideret JPEG decoder til afkodningen. Dette kunne gøre det muligt
at anvende et optimeret C bibliotek til formålet og dermed øge
afkodningshastigheden og desuden eliminere vores afhængighed af Psyco
Python til at speede denne afkodning op.

\paragraph{Wifireceiver}
Som de andre receivere modtager wifireceiveren en datastrøm fra
AR.Dronen, wifistrømmen er dog en tilføjelse vi har lavet og formatet
der skal afkodes er blot en tekststreng bestående af en mac-adresse og
en signalstyrke, der modtages således en pakke for hver pakke
snifferprogrammet opsnapper. Wifireceiveren skiller sig ud fra de
andre receivere, ved at placere de modtagne adresse/signal-par i en
liste og holde denne liste opdateret med de nyeste signalværdier for
hver adresse, for hver adresse gemmes desuden de sidste *X* modtagne værdier,
deres gennemsnit, deres varians og deres middelværdi. Når
wifireceiverens get\_data() metode kaldes, er det disse værdier der
returneres, værdierne finder anvendelse når wifidata skal matches med hinanden.

\paragraph{Virtuel sensor}
Den virtuelle sensor er en sen tilføjelse til vores platform og er som
tidligere nævnt inspireret af sensorhierakiet beskrevet i
\cite{lenser:behaviorbasedarchitecture}, hvor sensorer opererer på
forskellige niveauer og derfor leverer sensor output på forskellige
abstraktionsniveauer.  I modsætning til receiverne modtager den
virtuelle sensor ikke data direkte fra AR.Dronen, men derimod netop
fra receiverne. Den virtuelle sensor bearbejder kontinuerligt
billeddata for at genkende henholdsvis markører på jorden og
silhuetter umiddelbart foran AR.Dronen, sensoren leverer således
målinger på et højere abstraktionsniveau end receiverne. For
at kunne detektere både markører på gulvet og silhuetter er det
nødvendigt for sensoren hele tiden at skifte mellem bund- og
frontkamera, dette giver imidlertid problemer for AR.Dronens originale
program der ved konstante kameraskift løbende øger sit
hukommelsesforbrug indtil linuxkernen lukker programmet ned. Vores
målinger viser, at AR.Dronens program kun kan køre mellem ti og tolv
minutter, såfremt vi samtidig skifter kamera med circa 5
millisekunders interval. Selvom om dette problem ikke beskrives
direkte nogen steder, antydes det dog af en udvikler på Parrots forum,
at denne funktionalitet ikke er ment anvendt til hurtige kameraskift ved
hjælp af software og at der derfor er stor sandsynlighed for at metoden
ikke virker pålideligt\cite{electronic:parrotforum1}.

\subsubsection{kontrolinterface, AT-kommando Python interface}
Ved hjælp af AT-kommandoerne definerer AR.Dronen et interface til
enheder der er indenfor rækkevidde af dens trådløse netværk. For at
gøre dette AT-interface mere praktisk anvendeligt har vi videreudviklet
på en python-wrapper \cite{electronic:venthur} til dette interface. Vi har implementeret
interfacet som en Python tråd der løbende sender kommandoer med de
aktuelle kontrolværdier til
AR.Dronen. Dette betyder at vores system ikke afhænger af at alle
kontrolpakker når deres destination, hvilket er praktisk da
UDP-protokollen netop ikke garanterer at en pakke når sin destination. %think I understand but humor me and elaborate...
Ved at sende al kontrolkommunikation gennem interfacet, behøves vi
ikke heller ikke at tænke på detaljer som sekvensnumre og
watchdog-beskeder, da disse håndteres transparent af systemet.
  
Vores interface(\todo{REF TIL EGEN KODE}) definerer en række metoder til at
kontrollere dronens opførsel og bevægelser. Interfacet kan i det
simpleste tilfælde anvendes direkte fra Pythons interpreter, men
fungerer også som en del af vores grundlæggende platform.

\subsubsection{Controllers}
Selvom det er muligt, er det ikke praktisk anvendeligt at brugeren skal
afvikle scripts, der anvender kontrolinterfacet, for hver ønsket
bevægelse(som det ses i figur~\ref{fig:pythonimpl1}), i stedet har vi
implementeret to controllere til at tage mod input fra brugeren. Vi
en keyboardcontroller der lytter efter input
fra tastaturet og en joystickcontroller der er tilknyttet en fysisk
Xbox360-controller (se figur~\ref{fig:xboxlayout}).

Vi har mulighed for at starte systemet op med begge
controllere på samme tid, eller undlade en eller flere. Under
udviklingen af systemet har vi oftest anvendt begge controllere
sideløbende, da de komplimenterer hinanden. Det er også muligt at
anvende platformen helt uden controllere, og udelukkende anvende
taskmanageren til at kontrollere AR.Dronen, men skulle der i det
tilfælde ske noget uforudset under udførslen af en opgave, vil der
ikke være nogen mulighed for at bringe AR.Dronen manuelt ned. 

Begge controllerklasser nedarver fra en grundlæggende
controllerklasse (Controller), denne er implementeret som en tråd der med et givent
tidsinterval kalder en kontrolmetode(\code{process\_events()}). Den simple
opbygning betyder, at hvis man ønsker at implementere en anden type
controller, er det nok at nedarve Controller og implementere
\code{process\_events()} metoden. Da python behandler metoder som
førsteklasses objekter, er det let at lade en controller skifte mellem
forskellige tilstande, dette gøres ved at kalde \code{set\_control\_method} med
en ny kontrolmetode som argument. 

\paragraph{Joystickcontroller}
Joystickcontrolleren står i vores system istedet for en iPhone eller
iPad og anvendes til at fjernstyre AR.Drone manuelt i forbindelse
med testflyvninger. Der var flere praktiske grunde til at vi valgte at
implementere en alternativ fjernstyring, blandt andet havde vi et
behov for at kunne interagere både med vores eget system og AR.Dronen
på samme tid, mens vi til stadighed måtte have øjne på AR.Dronen. Vi
fandt desuden ikke iphonestyringen tilstrækkelig præcis eller særlig
intuitiv, Xbox360 controlleren derimod omtales som en af de bedst
designede controllere og har endda fundet anvendelse af militære
styrker i USA og
storbritannien\cite{electronic:zdnet}\cite{electronic:joystiq}\cite{electronic:youtube1}.
Fjernstyringsdelen er meget simpel og fungerer ved at oversætte
værdier fra Xbox360 controlleren(som vi får givet af pygames
joystickmodul\cite{electronic:pygame}) og så sende disse til
kontrolinterfacet.  Joystickcontrolleren kan udover simpel
fjernstyring også anvendes til at starte prædefinerede opgaver(REF TIL
TASK SECTION) ved hjælp af taskmanageren, således kan piloten manuelt bringe
AR.Dronen i en ønsket position og så herefter at starte den automatiske
afvikling af et forudbestemt bevægemønster. På
figur~\ref{fig:xboxlayout} ses vores standardopsætning af Xbox360
controlleren.
\begin{figure}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/xbox360layout.png}
  \caption{Standardopsætning af Xbox360 controlleren}
  \label{fig:xboxlayout}
\end{figure}

\paragraph{Keyboardcontroller}
Hvis vi ønsker at starte vores system uden grafisk brugerflade(og
tilhørende GTK keyboardbindinger), er det
praktisk stadig at kunne anvende tastaturet som input(evt i kombination med Xbox360
controlleren). Vi har derfor implementeret en simpel
keyboardcontroller der læser input fra terminalen. Vi anvender ikke
keyboardcontrolleren til direkte styring af AR.Dronen(da det ikke er
praktisk i forhold til Xbox360 controlleren), men til at få udskrevet
nuværende batteriniveau('b'), til at skifte kamera('z'), til at få
udskrevet aktive opgaver på autocontrolleren('c'), til at få udskrevet
en oversigt over tråde i programmet('t'), samt til at starte opgaver
på autocontrolleren('1', '2' og '3'). 

\subsubsection{Taskmanager}
Taskmanagerklassen startede ud som en almindelig controller, men er endt
som en mere selvstændig del af platformen. Taskmanageren skiller sig
ud ved ikke, i modsætning til joystick- og keyboardcontrollerne, at
arbejde direkte med kontrolinterfacet eller med output fra
receiverne. Taskmanagerens opgave er udelukkende at starte
forudspecificerede opgaver(tasks) og stoppe dem igen, samt at
facilitere beslutningen om hvilken task der har lov til at bevæge AR.Dronen. 

\subsubsection{Testdevice}
Dette modul muliggør gennemførelsen af simple eksperimenter i et
test miljø. Grundideen bag modulet er, at man skal kunne optage, ikke
bare film, men alle sensoroutput fra en flyvning med AR.Dronen, for så
senere at kunne afspille hele flyvningen igen. Selve afspilningen
foregår næsten transparent for resten af platformen, receiverne skal
stadig initiere kommunikationen(nu blot med testdevicet), forbindelsen
skal stadig holdes i live og dataen der modtages skal stadig
afkodes. De eneste synlige forskelle der er mellem test og
live-sessions er timing og hastighed, den første fordi vi ikke har
prioriteret at afsende testdataen i de intervaller de blev modtaget i
og den sidste fordi samme computer nu står både for afsendelse,
modtagelse og afkodning.  Testdevicemodulet har ikke noget at gøre med
optagelsen af sensoroutputtet, denne foregår i receiverklasserne.


\subsection{Praktisk anvendelse af Platformen}
%        Introduktion til styring af AR.\ $\!$Drone med Python}

%           Eksempel på anvendelse af Python interface til AR.Dronen
\subsubsection{Python-eksempel, nem fjenstyring af AR.Dronen}
Et simpelt eksempel på anvendelse af vores kontrolinterface gives ved at
starte pythons fortolker, importere examples-modulet og herefter
afvikle en metode indeholdende kommandoer til
AR.Dronen. Fremgangsmåden ses på figur~\ref{fig:pythoninterpex1}.

\begin{figure}[ht]
  \begin{framed}   
    \begin{verbatim}
>>> import examples as e
>>> e.square()
    \end{verbatim}
  \end{framed}
\caption{Import og afvikling af simpelt kontrolinterface-eksempel i pythonfortolkeren}
\label{fig:pythoninterpex1}
\end{figure}

Metoden \code{square()} der kaldes i eksemplet i figur~\ref{fig:pythoninterpex1},
lader AR.Dronen lette, flyve rundt i en firkant og for herefter at
lande igen(under antagelse af at klientcomputeren er forbundet med AR.Dronens
trådløse netværk). Bevægelsesmønstret opnåes ved at der kaldes en
metode(\code{take\_off}, \code{move} eller \code{land}) på kontrolinterfacet, hvorefter der er en pause
før den næste metode kaldes. Metodekaldende oversættes af interfacet til
AT-kommandoer og sendes så som UDP-pakker til AR.Dronens
kommandoport. Den konkrete implementation af \code{square()} ses i
figur~\ref{fig:pythonimpl1}. Det bemærkes at kontrolinterfacetråden
skal startes (kald af \code{c.start()}) før kommandoerne har nogen effekt.

\begin{figure}[H]
  \lstset{language=Python, frame=single}
  \begin{lstlisting}
def square():
    import controllers
    import time

    c = controllers.ControllerInterface()
    c.start()

    print 'taking off'
    c.take_off()
    time.sleep(5.0)
    print 'moving Forward'
    c.move(0.0, 0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving right'
    c.move(0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving back'
    c.move(0.0, -0.2, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'moving left'
    c.move(-0.2, 0.0, 0.0, 0.0, True)
    time.sleep(2.0)
    print 'landing'
    c.land()

    c.stop()
\end{lstlisting}
\caption{Python implementation der importerer og anvender vores kontrolinterface til en
  simpel flyvning}
\label{fig:pythonimpl1}
\end{figure}

\subsection{Modtagelse af ``sensor''-data fra AR.Dronen} % Introduktion til modtagelse af video, navigationsog wifidata fra AR. Dronen
I det følgende gennemgåes mulighederne for at modtage datapakker med video, navigation- og wifi-data fra AR.Dronen.
For at kunne anvende AR.Dronen som en robot er det nødvendigt at kunne
indsamle data fra de ombordværende sensorer, samt at kunne stille
disse data til rådighed for andre dele af systemet. Til dette formål
har vi designet og implementeret en receiver-struktur bestående af tre
receiver-moduler, disse moduler eksekveres i hver deres process og
sørger selv for at initiere forbindelsen med dronen. Efter
initialiseringen lytter hver receiver efter enten video-, navigations-
eller wifi-pakker og når en sådan modtages udføres en passende
afkodning af pakken og den afkodede data deles med processen der
startede receiveren. De tre receivere er ikke
afhængige af hinanden og kan afvikles hver for sig, ligeledes kan vi
vælge at afvikle vores samlede system med en eller flere receivere
tilkoblet. Sektion~\ref{subsection:receiver} giver et eksempel
på hvordan man lettest anvender videoreceiveren, til at modtage og vise
et enkelt billede fra AR.Dronens videostrøm. Anvendelsen af de to
andre receivere foregår på fuldstændig samme måde som videoreceiveren,
den eneste forskel er typen af data der returneres. 

\subsubsection{Eksempel på anvendelse af videoreceiveren}
\label{subsection:receiver}
Eftersom vores videoreceiver kan anvendes for sig selv, vises her et
eksempel på hvor få linier kode man behøver for at modtage og vise et
billede fra AR.Dronens kamera. Eksemplet antager som tidligere, at der
allerede er oprettet en virkende forbindelse med AR.Dronens trådløse
netværk. Eksemplet startes fra Pythons kommandopromt.  
\begin{figure}[H]
  \begin{framed}   
    \begin{verbatim}
>>> import examples as e
>>> e.receive_and_show_picture()
    \end{verbatim}
  \end{framed}
\caption{Import og afvikling af simpelt videoreceiver-eksempel i pythonfortolkeren}
\label{fig:pythoninterpex2}
\end{figure}

Som det ses på figur~\ref{fig:pythonimpl2} kræver det tre liniers reel
kode for at modtage et billede(og endnu fire for at vise det), vi
mener at dette fint illustrerer, hvordan vi med dette abstraktionslag kan
tilbyde et let anvendeligt interface til AR.Dronens forskellige
sensoroutput.   
\begin{figure}[H]
  \lstset{language=Python, frame=single}
  \begin{lstlisting}
def receive_and_show_picture():
    import receivers, settings, time
    import cv2.cv as cv
   
    video_sensor = receivers.VideoReceiver(VIDEO_PORT)
    video_sensor.start()
    time.sleep(1)
    pic = video_sensor.get_data()
      
    cv.StartWindowThread()
    win = cv.NamedWindow('win')
    cv.ShowImage('win', cv.fromarray(pic))
    cv.WaitKey()
    cv.DestroyWindow('win')

    video_sensor.stop()

  \end{lstlisting}
  \caption{Python implementation der importerer og anvender
    videoreceiver til at modtage og vise et billede}
\label{fig:pythonimpl2}
\end{figure}

\pagebreak

\section{Hjælpeværktøjer} % Morten,

\subsection{GUI til datarepræsentation}


\subsection{Mapdrawer / turplanlægger}

\subsection{Wifi-signalstyrkesample presenter}

\pagebreak

\section{Tasks} % Morten
\label{sec:tasks}
For at indkapsle forskellige bevægemønstre og for at kunne koordinere
en parallel afvikling af disse, har vi implementeret et hierakisk
opbygget tasksystem. Vi har valgt at anvende task-terminologien
istedet for behaviors~\cite{lenser:behaviorbasedarchitecture}, grunden til dette er at flere af vores tasks er
forholdsvis kortlivede(for eksempel take-off og landtasks) og derfor
mere minder om en opgave der skal forberedes, udføres og afsluttes,
dette ses i modsætning til en behavior som kan opfattes som en
længerevarende opførsel. Vi har også implementeret højniveau bevægemønstre der
minder mere om en traditionelle behaviors(for eksempel
FollowTourTask), men vi mener også at det giver mening at kalde disse
tasks, da de også implementeres efter vores generelle taskstruktur.

Vores første udgaver af tasksystemet indbefattede tasks der indsamlede
og behandlede data fra AR.Dronens sensorer, i den endelige udgave er
disse tasks dog udskilt til den virtuelle sensor, så tasks ikke skal
behandle sensordata, men kun reagere og formidle bevægelse på baggrund
af højniveau-sensordata. 

\subsection{Task typer} 
Vi har to hovedtyper, simple og compoundtasks. Simple tasks nedarver
fra vores basetaskklasse Task. De repræsenterer oftest en simpel
bevægelse, men kan være vilkårligt komplekse, for eksempel har vi en
task der blandt andet implementerer en PID-controller. Compoundtasks
er tasks der indeholder en liste af undertasks og funktionalitet til
at starte og stoppe de underliggende tasks. Vi anvender to typer
compoundtasks, sekventielle og parallelle. Sekventielle compoundtasks
starter deres undertasks en efter en, men venter med at starte en ny
undertask før den forrige er afsluttet, en sekventiel compoundtask
stopper når den sidste undertask er afsluttet. Parallelle compoundtask
starter alle deres undertasks på en gang og stopper også når den
sidste undertask har afsluttet. 

\subsection{Task hieraki}
Ved hjælp af de ovenfor beskrevne tasktyper, og specialiseringer af
disse, kan vi danne en task træstruktur der minder om den beskrevet i
\cite{lenser:behaviorbasedarchitecture}. Selvom om de basale
træstrukturer minder om hinanden, er der dog adskillige forskelle på
algoritmen der bestemmer hvilken task/behavior der skal
aktiveres. 

Tilgangen i \cite{lenser:behaviorbasedarchitecture} er at
en aktiveret behavoir skal bestemme hvilke af sine underbehaviors der
skal aktiveres, dette er praktisk, da det med deres robot er muligt at
udføre forskellige bevægelser på samme tid og det bedste sæt af
parallelle bahaviors skal beregnes. I vores tilgang operer vi ikke med
flere aktive tasks samtidig(de bevægelser vi har til rådighed kan ikke udføres
uden indflydelse på hinanden) og generelt bliver vores træer ikke
ligeså dybe som \cite{lenser:behaviorbasedarchitecture}, da vores
interface til AR.Dronen er på et højere abstraktions niveau. Vores
tilgang indbefatter at alle tasks kører samtidig(alle tasks er
implementeret som Python tråde), men at maksimalt en
task har tilladelse til at flytte på AR.Dronen, hver gang en task
ønsker at flytte AR.Dronen spørges op  i træet efter tilladelse. Alle
tasks på samme niveau(med samme forælder) er ordnet i et fast hieraki
og en højere rangeret task kan altid kræve kontrol og dermed
undertrykke lavere rangerende tasks.

På ~\ref{fig:taskhieraky} ses et eksempel på et taskhieraki
konstrueret med vores taskcontructor, under antagelse at hver TestTask
ønsker at udføre et antal bevægelser og ikke på noget tidspunkt ønsker
frivilligt at opgive bevægekontrollen, vil TestTask med level 1 slutte
først og TestTask med level 2 slutte sidst.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/taskhieraky.png}
  \caption{Eksempel på et taskhieraki.}
  \label{fig:taskhieraky}
\end{figure}

\subsubsection{HoverTrackTask}
\label{sssec:hovertrack}

\subsubsection{FollowTourTask}
\label{sssec:followtourtask}
Et mere realistisk og komplekst eksempel er en FollowTourTask, se
\ref{fig:followtour}, denne task nedarver fra en sekvensiel
compoundtask og indeholder udover takeoff og land-tasks også en
parallel compoundtask der kan veksle mellem bevægelse og fastholdelse
af en position(ved hjælp af en PID-controller). Selve FollowTourTask
enheden holder styr på den tour der skal følges, ved hjælp af et
Map-objekt, og sørger for at opdatere HoverTrackTasken(PID) med en ny
målposition når det er nødvendigt. 
Hvis man anskuer vores Map som en relationel graph hvor alle knuder er
forbundne, minder vores tilgang meget om Kuipers og Byuns metode der beskrives i
\cite{murphy2000introduction}. Specielt lægges mærke til at der i
begge systemer udføres fejlkorrigering hver gang en ny position
detekteres, i vores system sker dette ved at HoverTrackTasken bringer
robotten indenfor en specifik afstand af den præcise position før
Ar.Dronen sendes videre. Det ville desuden være let at lade vores
*DistanceMeasurer* algoritme tilføje metrisk data til kortet, hver gang
AR.Dronen har bevæget sig mellem to positioner.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.80\textwidth]{grafik/followtourhieraki.png}
  \caption{Opbygningen af en FollowTourTask.}
  \label{fig:followtour}
\end{figure}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Eksperimenter - section
\chapter{Eksperimenter} % Thomas & Morten
\section{Navigation}
En vigtig forudsætning for at en robot kan navigere i et miljø er, at
den er i stand til at lokalisere sig selv i dette miljø. Vi har derfor
ekperimenteret med forskellige former for lokalisering, mere specifikt
har vi arbejdet med lokalisering ved hjælp af WIFI-signalstyrker og
lokalisering ved hjælp af visuelle markører. Vi har i vores
eksperimenter gjort brug af et kort med beskrivelser af
lokationer(WIFI, markørbeskrivelse og koordinater) og en
rutebeskrivelse bestående af en liste med lokationer, kortet
indeholder desuden en fortegnelse over vinkelretninger mellem alle
lokationspar. Kort og rutebeskrivelser oprettes let ved hjælp af*ref
taskcreator*. For at teste forskellige lokaliseringsmetoder har vi
anvendt HoverTrackTasks og FollowTourTasks\ref{sssec:followtourtask}.

\subsection{Lokalisering via Wifi-fingerprinting} % Morten (1side)
\label{ssec:wikilokalisering}
%,skal disse have hver deres egen hovedsektion (istedet for SUBsection)?
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

Take Sample, indsamling, sortering, alder, sammenlign/ sammenhold
\subsection{Visuel lokalisering}

\begin{figure}[H]
  \centering
  \subfigure[Simpel markør]{
    \includegraphics[width=0.20\textwidth]{grafik/simplemarker.png}
    \label{fig:simplemarker}}
  ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc. (or a blank line to force the subfig onto a new line)
  \subfigure[semiavanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/semiadvancedmarker.png}
    \label{fig:semiadvancedmarker}}
  ~
  \subfigure[avanceret markør]{\includegraphics[width=0.35\textwidth]{grafik/advancedmarker.png}
    \label{fig:advancedmarker}}

  \label{fig:markers}
\end{figure}
\subsubsection{Lokalisering via simple markører} % Thomas & Morten
\paragraph{Første eksperiment}
Vores første forsøg med visuel lokalisering anvendte en simpel rød
markør i A5 størrelse, se figur~\ref{fig:markers}, vi havde til
formålet implementeret en blobdetectionalgoritme i Python, som for
hver pixel vurderer om den er rød nok til at være en del af markøren. Algoritmen antager at der i
billedet kun findes et rødt objekt og er på grund af Pythons
afviklingshastighed ikke videre hurtig, men til dette simple
eksperiment var den passende.
Eksperimentet udførtes ved at markøren blev placeret på
gulvet og AR.Dronen fløjet i position over markøren, hvorefter
blobdetectionalgoritmen blev startet. 
Vores første forsøg viste os, at vi i højder fra 0.5 til 2.5 meter altid var i stand
til at genkende markøren med AR.Dronens bundkamera.

\paragraph{Andet eksperiment} 
Vores næste skridt var at anvende ovennævnte lokaliseringsmetode i
forbindelse med afviklingen af en HoverTrackTask~\ref{sssec:hovertrack}
for at få en idé om metodens anvendelighed i en
realtidskontekst. 
Vi anvendte til dette forsøg også kun en enkelt markør og forsøget
afvikledes ved at AR.Dronen blev fløjet i position over denne,
hvorefter HoverTrackTasken blev aktiveret. Her viste det sig tydeligt
at pythonblobdetectoren ikke var effektiv nok, vi var ikke i stand til
at opdatere hurtigt nok og som resultat nåede AR.Dronen ofte at bevæge
sig væk fra markøren. Dette resultat fik os til at reimplementere
HoverTrackTasken således at den anvendte vores blobdetector til at
opdage markøren og herefter anvendte OpenCVs
opticalflow-algoritme (optflowpyrlk) til at tracke den i videostrømmen. Afviklingen af
opticalflow-algoritmen er omtrent 10 gange hurtigere end vores
blobdetector. Da vi gentog forsøget med denne tilføjelse var AR.Dronen
meget mere responsiv og efter at have fintunet HoverTrackTaskens
PID-parametre var AR.Dronen i stand til at forblive over markøren. På
fig~\ref{fig:simplehover} ses error-værdierne i x og y-retningen over
en periode på mere end 2 minutter hvor AR.Dronen befinder sig over en
markør, det bemærkes at der er et udsving omtrent midt i forløbet,
dette er ikke usædvanligt, da AR.Dronen er meget påvirkelig af
turbulens og dette let forekommer i, især, mindre lokaler.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{grafik/simpledronehover.png}
  \caption{HoverTrackTask error-værdier over en længere periode}
  \label{fig:simplehover}
\end{figure}

\paragraph{Tredje eksperiment}
Sidste eksperiment med de simple markører anvendte en FollowTourTask
der fulgte en rute mellem fire markører på gulvet. Eksperimenterne startede
denne gang fra jorden, da en FollowTourTask selv letter og lander.

Vi forventede her at udfordringen ville være at opdage markøren tidsnok til at stoppe
AR.Dronen før den var fløjet forbi. Dette problem kan dog løses ved at
lade AR.Dronen flyve i en passende højde, vi fandt at en
højdesetting på 1800 mm var passende til dette formål. Til gengæld
afdækkede vi to andre problemer. 
Det første problem var relateret til at
alle vores markører var ens, dette betød at der var tilfælde hvor
AR.Dronen fandt samme lokation to gange i træk, men antog at den
fundne lokation var den næste i rutebeskrivelsen, problemet lagde
grund til tanken om at anvende mere avancerede lokationsmarkører for
at opnå en mere præcis lokalisering. Det andet problem
var at hvis vi først mistede en markør fra AR.Dronens synsfelt, så
var det umuligt at fortsætte, dette problem løstes ved at tilføje en
recovermode til HoverTrackTasken, således at denne kunne bevæge
AR.Dronen opad indtil enten markøren igen kunne genkendes eller en højdegrænse
blev nået, hvorefter experimentet alligevel måtte afsluttes. Denne
simple tilføjelse gjorde vores FollowTourTask meget mere robust.   

\paragraph{Resultater}
De overordnede resultater af vores eksperimenter med de simple
lokationsmarkører var, at vi fik bekræftet AR.Dronens evne til at
genkende simple markører og at vi fik udviklet både HoverTrackTasken
og FollowTourTasken. Vi fandt desuden ud af at vores markørsetup var
for simpelt til at være praktisk anvendeligt og at vi måtte udvikle
mere avancerede markører for bedre at understøtte vores FollowTourTask.

\subsubsection{Lokalisering via avancerede markører} % Thomas & Morten
Vi valgte efter de indledende eksperimenter at designe og anvende en
mere avanceret form for markører. Vores første design var markøren der
ses på figur~\ref{fig:semiadvancedmarker}. 

\paragraph{detekteringsalgoritme}
Grundideen er at man først detekterer den røde firkant, hvorefter det
indkransede område afsøges for en farvet blob der relaterer markøren
til en specifik lokation. For at sikre en hurtig detektering(og dermed
lokalisering) anvender gør vores algoritme omfattende brug af OpenCVs
optimerede billedbehandlingsalgoritmer. Vores detekterings algoritme
er implementeret i vores virtuelle sensor og afvikles således konstant
for at give et så nøjagtigt billede af miljøet som muligt. 
Første skridt i algoritmen er at udføre thresholding ved hjælp af
Opencvs inRange algoritme, dette giver os et billede hvor alle de
oprindelige røde pixels nu er hvide og resten er sorte.
Andet skridt er at finde konturer i det binære billede dette gøres med
OpenCVs findContours algoritme.
Tredje skridt indbefatter at udvælge de konturer der har et stort nok
areal til at være interessante, dette gøres med OpenCVs contourArea
algoritme.
Fjerde skridt er at undersøge områderne der dækkes af de udvalgte
konturer for at finde en farvet blob, denne undersøgelse anvender igen
OpenCVs inRange algoritmre. Hvis en sådan farvet blob findes
kan vi sammenligne farven med beskrivelsen af de forskellige
lokationer vi finder i vores kort og dermed lokalisere AR.Dronen
præcist.

\paragraph{Første eksperiment}
Vores indledende eksperiment udførtes på samme måde som det første
eksperiment med de simple markører, blot var den simple markør skiftet
ud med en semiavanceret markør, vist på figur
~\ref{fig:semiadvancedmarker}. Vores forventninger var fra start, at vi
ville få svært ved at identificere markøren over en hvis højde, da den
røde markering ikke er lige så koncentreret som på den simple
markør. Efter at have tilpasset vores thresholdværdier og udført
eksperimentet i højder fra 1 til 2.5 meter, kunne vi bekræfte vores
formodning. Vi kunne med sikkerhed genkende den semiadvancerede markør
op til 1.5 meters højde, men herefter faldt detektionsraten
betydeligt. En detektionshøjde på 1.5 ligger lidt under de 1.8 meter
vi tidligere havde bestemt som en passende flyvehøjde. Vi valgte
derfor at udvikle en markør med en større rød overflade, se
figur~\ref{fig:advancedmarker}, for at sikre bedre detektion. En
gentagelse af eksperimentet med den avancerede markør bekræftede at
denne nu kunne detekteres i op til 2 meters højde. Den avancerede
markør er desuden designet sådan at den på sigt kan bruges til at
udlede AR.Dronens præcise retning, hvilket ville gøre os mindre
afhængige af de upræcise psi-målinger fra Ar.Dronens piezo-sensor. 

\paragraph{Andet eksperiment}
Vores andet eksperiment er næsten identisk med det der blev udført for
de simple markører, Det skal dog nævnes at vores HoverTrackTask i
mellemtiden blev redigeret til at anvende den virtuelle sensors
detekteringsresultater, istedet for selv at udføre en
detekteringsalgoritme som det var tilfældet under de tidligere
eksperimenter. Under dette eksperiment var den virtuelle sensor
indstillet til ikke at skifte mellem de to kamerafeeds. 
Som forventet var AR.Dronen også i stand til at holde positionen over
den avancerede markør.

\paragraph{Tredje eksperiment}
Dette eksperiment varierede fra det ovenstående ved at den virtuelle
sensor her løbende skiftede kamerafeed, vi ønskede at se om
HoverTrackTasken stadig var i stand til at operere med 
\subsubsection{Lokalisering via QR-koder} % Morten
\label{sec:QRdecoding}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\textwidth]{grafik/woah.png}
  \caption{Teksten ``woah'' som QR-kode}
  \label{fig:qr_woah}
\end{figure}
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...


%\subsection{Optisk flow navigation} % tt
%\todo{fedt hvis det kan nåes at lave det...}
% birds eye view, head bobbing, biers afstandsbedømmelse ligger i
% optiske flow observeret fra A til B, forskel i tunnel og åbent land...

%problematik at flyve fremad; for smalle trekanter, måske bedre at flyve sidelæns eller op og ned
% evt afhjælpe med fiskeøje, så flow af featurepoints kan trackes længere langs siden af kopteren når man flyver forbi...


%\subsection{Korridor vanishpoint}
\section{Korridors forsvindingspunkt} % Thomas
%Hvad vil vi vise, afvise teste?
%Hvad kan/skal resultatet bruges til?
%Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
%Hvad var resultatet?
%perspektiver, hvad skal / kan vi bruge den nye viden til?
%Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % % %

En tilgang er at bruge bundkameraet og få quadrokopteren til at finde et punkt på gulvet og
svæve over det. En anden tilgang er at teste om quadrokopteren kan orientere sig i forhold
frontkameraet.

I ``Minerva'' og ``opticalflow corridor vanishing point'' har robotterne fået
forskellige handleprocedure afhængigt af hvilket miljø de befinder sig i.

Målet for dette eksperiment er at : 2 ting
1) finde ss. for at man er i en korridor, og
2) hvor er enden af korridoren (forsvindingspunktet)

Lidt omkring hvordan man kan detekte at man er i en lang koridor. Plus nogle referencer til andre hvorfra der er kommet inspiration.
finde edges -> finde houghlines -> sortere linier (diagonale)
-> finde skæringspunkter blandt diagonale linier, tælle område med
flest skæringspunkter
+augmenter med phi

\subsection{At finde kanter i et billede}

\begin{figure}
  \centering
  \includegraphics[width=0.99\textwidth]{grafik/kant_detektering1a.png}
  \caption{Graf over rød-, grøn-, blå-farveværdier for en linie i
    billedet med pixel indeks $j=67,\ i=\{0,1,\ldots,320-1 \}$. Samt
    pixel intensitet, her givet ved summen af de tre farver for $i$ og
    $j$. Der hvor grafens hældning er stejl er der stor sandsynlighed for en
    kant i billedet.}
  \label{fig:kanter1}
\end{figure}

Kanter i et billede defineres som abrupt skift i farveværdi eller
lysintensitet lokalt\footnote{Man kan også lede efter kanter globalt
(se f.eks.\newline
\uri{http://en.wikipedia.org/wiki/Step\_detection\#Global}), men det
er en anden tilgang. I dette afsnit beskrives en metode der søger
lokalt.} mellem naboer af pixels. Kanter er ofte en indikator for
eksistensen af bl.a. ændringer i dybden af et billede og for
overfladeændringer m.fl.

Figur~\ref{fig:kanter1} illustrerer hvordan ændringerne i intensitet
imellem tilstødende billedpunkter (pixel) kan ses som en graf af
højder. De steder hvor der er stor højdemæssig afstand, altså hvor
hældningen er stejl, er der som oftest en kant at finde.

Til at finde kanter (edges) kan cannyfilteret\cite{Canny1986}
anvendes. Det tager et billede og returnere et binært billede
(sort/hvid). Hvide billedpunkter indikere at der er en kant i det
oprindellige billede, mens sort betyder at der ikke er ændringer.

Fordi AR.Dronens kamera er med lille opløsning og det billeder af så
tilpas ringe kvalitet vil det forbedre slutresultatet at bruge et
båndpasfilter til at lave en ``udglatning'' af
billedet. Udglatningsoperationen udføres med et gaussisk
filter\footnote{Fra billedbehandlingsprogrammer som Photoshop og GIMP
hedder det ``Gaussian blur'' filter.} en
klokkegraf\cite{electronic:wiki_gaussklokke}.

Tager man alle pixelpositioner hvor den første afledte er over
threshold kan man ende op med mange tykke kanter, for at præcisere
placeringen kan man udtynde dem, ved at søge i retningen af den lokale
gradient og istedet finde maximum/minimum. Videreudbygninger af
Cannyfilteret efter 1986 bruger bl.a. endnu en retningsbestemt afledt
af gradienten og finder maximum ved dens skærring med 0.

\begin{figure}
  \centering
  \includegraphics[width=0.90\textwidth]{grafik/derivatives.png}
  \caption{Se $f(x)$ som lysintensitet per pixellokation $x$. $f'(x)$ og $f''(x)$ er den hhv. første-- og anden-- afledte. Når $f''(x)$ skærre med nul har $f(x)$ den største hældning.
Figuren er fundet via google hos \uri{www.cs.utah.edu/\~jmk/simian/img/derivatives.gif}}
  \label{fig:derivatives}
\end{figure}

% \bigtriangledown^2G_{\sigma}(\bm{x})=\frac{1}{\sigma^3}\left(2-\frac{x^2+y^2}{2\sigma^2}\right)\exp\left(-\frac{x^2+y^2}{2\sigma^2}\right) 

% Lindeberg98 edges found by second and third derivatives
% second derivative = laplacian


\subsection{Houghlinier}

\begin{figure}
  \centering
  \includegraphics[width=0.40\textwidth]{grafik/polar2cartesian.png}
  \caption{En linie beskrevet ved det velkendte almindelige cartesisk koordinatsystem som: $y=ax+b$, samme linie beskrevet i polar koordinatsystem ved en vinkel $\Theta$ og en afstand $\rho$.}
  \label{fig:polar2cartesian}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % % %



\section{Distance-measurererererererere} % Thomas
\label{sec:distance-mesu}
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

jo mindre quadrokopter des mindre inerti kan være indeholdt, og jo
mere agile agil/adræt vil den være, måske en pointe i futureworks...

Indledende øvelse til at kunne måle afstande frontalt via triangulering <- optisk flow

Algoritme integration over afstand

Test, hvor virker det godt hvor er det problematisk
reference til Parrots egen artikle herom; hastighed udledning bundkamera hvis muligt ellers udledning af accelerometer.

\section{Tasks/Inhibition/subsumtion} % Morten
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

\todo{avoid silhouet}, \todo{low battery}
Task, silhuet, batteri, at skifte kamera forfra til underside - problematisk.

\section{Erkendelse af miljø/mode...}
Hvad vil vi vise, afvise teste?
Hvad kan/skal resultatet bruges til?
Hvordan løste vi det, undersøgte det, hvordan udførte vi eksperimentet?
Hvad var resultatet?
perspektiver, hvad skal / kan vi bruge den nye viden til?
Det er muligt at nogle af disse spørgsmål/pinde/punkter er sagt/ skal siges allerede i introen...

Korridor (Hopper) vs. Åbent område (Zuse)
Er der noget at hente fra koridor vanishing point? der vil være noget hvis vi kan nå at måle afstande foran i optisk flow...


\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%     Konklusion - section

\chapter{Konklusion}
Evaluering: at det har været svært at finde videnskabelige datalogiske kilder af høj troværdighed omkring AR.Dronen ; in part privat virksomhed properitær firmware / arkitektur, meget information fra hacker, tinker, DIY fora.
Tilgængel nogle videnskabelige artikler omkring hvordan man implementere en algoritme ovenpå en allerede antaget platform som AR.Dronen...

\section{Perspektivering}

\section{Fremtidig arbejde}
\pagebreak


\appendix

\bibliographystyle{abbrv}
\bibliography{referencer}

\chapter{AT kommandoer}
\label{sec:at}
Kort fortalt så må en AT-kommando ikke deles i flere pakker, men en UDP-pakke kan
indeholde flere AT-kommandoer, kommandoerne skal blot være adskilt med
en "linefeed" karakter og den samlede længde må ikke overskride 1024
karakterer. Såfremt en kommando overskrider længdebegrænsningen eller
på anden måde ikke overholder syntaksen, vil AR.Dronen ignorere
kommandoen.   

Tabel \ref{tab:kommandoer} viser hvilke kommandoer der
jf. \cite{techreport:ardroneDevGuide} kan bruges til at
kontrollere AR.Dronen.

\begin{table}[ht]
  \begin{center}
  \begin{tabular}{ l | p{3,5cm} | p{5,5cm} }
    Navn & Argumenter & beskrivelse\\
    \hline
    REF & input & Kommando til takeoff, land og nødstop\\
    \hline
    PCMD & flag, roll, pitch, gas, yaw & Flytter AR.Dronen\\
    \hline 
    FTRIM &  & Sætter den horisontale referenceværdi \\
    \hline
    CONFIG & key, value & Ændrer en konfigurationsparameter\\
    \hline
    LED & animation, frequency, duration & Viser en LED animation\\
    \hline
    ANIM & animation, duration & Afspiller en flyvesekvens\\
    \hline
    COMWDG & & Resetter komunikationstimeren
  \end{tabular}
  \end{center}
  \caption{Gyldige AT-kommandoer} 
  \label{tab:kommandoer}
\end{table} 

\chapter{Medfølgende CD / internet}
\todo{Readme} Hvad skal læseren gøre, hvad er tilgængeligt


\end{document}

%\begin{center}
%  \includegraphics[scale=0.35]{pic/selfserv.jpg}
%\newline
%\textbf{Figur 1}
%\end{center}
%\lstinputlisting[language=Python]{../drone.py}





% \bm{J} = \bigtriangledown I (\bm{x} ) = \left(\frac{ \partial I }{ \partial x }, \frac{\partial I }{ \partial y }\right)(\bm{x})
%  G &= \sqrt{ G_x^2 + G_y^2 }\\ \theta &= \arctan \left( \frac{G_y}{G_x} \right)\\ \text{ hvor } G_x & = \frac{ \partial I }{ \partial x } \text{ og } G_y \frac{\partial I }{ \partial y }

% \bm{J}_{\sigma} = \bigtriangledown [ G_{\sigma} (\bm{x} ) \times I (\bm{x} ) ] = [ \bigtriangledown G_{\sigma} ] (\bm{x} ) \times I (\bm{x} )

% \bm{S}_{\sigma}(\bm{x} )=\bigtriangledown \dot{} \bm{J}_{\sigma}(\bm{x} )=[ \bigtriangledown ^2 G_{\sigma} (\bm{x} ) \times * \bm{I} (\bm{x} ) ]
% \bigtriangledown G_{\sigma}(\bm{x} )=(\frac{\partial G_{\sigma}}{\partial x},\frac{\partial G_{\sigma}}{\partial y})(\bm{x})=[-x -y]\frac{1}{\sigma^3} \exp \left( - \frac{x^2 + y^2}{2 \sigma ^2} \right)

%#################3

%% Hvis man observere på ibrugtagning af robotter gennem tiden, ses robotteknologien anvendt
%% mange steder i industrien ofte med hensigten om 
%% at aflaste mennesker for monotone og farlige arbejdsprocesser (Unimate1950\cite{electronic:wiki_unimate}), eller med
%% hensigten om at effektivisere en produktionsvirksomhed ved indførelsen
%% af faste maskiner til at afvikle forprogrammerede
%% rutiner. Introduktionen af ny teknologi har altid en konsekvens;
%% ændringer betyder, at noget udskiftes med noget andet.
%% Den store Erhvervsredegørelse fra 1997 konkluderede også at
%% \begin{quotation}
%%   ``globalisering og ny teknologi overflødiggør den kortuddannede arbejdskraft.''
%% \end{quotation}%http://ibog.danmarkshistorien.systime.dk/index.php?id=224
%% %google søg: ``industrialisering efterspørgsel på arbejdskraft''
%% Hvis ny teknologi skal indføres bør det ikke kun afvejes iforhold til
%% økonomiske nytteværdi og omkostninger, men også med omtanke for det
%% enkelte menneske og under hensyntagen til langtsigtende konsekvenser for samfundet.

%% Fra de faste stationere robotter i produktionen kan man gå til de
%% mobile, hvor Elmer og Elsie er eksempler. Allerede fra 1948 lavede
%% Dr. W. Grey Walter 2 selvkørende
%% robotter\cite{Walter:1950:elmer:elsie} med analog styring ombord.
%% Senere ifølge Wikipedia\cite{electronic:wikirobot:comparison} er
%% der i

%1997 lavet en rengøringsrobot Robosanitan\cite{electronic:robosanitan1997}.
